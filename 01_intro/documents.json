[
  {
    "course": "data-engineering-zoomcamp",
    "documents": [
      {
        "text": "Data Engineering Zoomcamp FAQ\nData Engineering Zoomcamp FAQ\nThe purpose of this document is to capture Frequently asked technical questions\nEditing guidelines:\nWhen adding a new FAQ entry, make sure the question is \u201cHeading 2\u201d\nFeel free to improve if you see something is off\nDon\u2019t change the formatting in the Data document or add any visual \u201cimprovements\u201d (make a copy for yourself first if you need to do it for whatever reason)\nDon\u2019t change the pages format (it should be \u201cpageless\u201d)\nAdd name and date for reference, if possible\nThe next cohort starts January 13th 2025. More info at DTC.\nRegister before the course starts using this link.\nJoint the course Telegram channel with announcements.\nDon\u2019t forget to register in DataTalks.Club's Slack and join the channel.",
        "section": "General course-related questions",
        "question": "Course - When does the course start?"
      },
      {
        "text": "See DE zoomcamp 2025 pre-course Q&A\nTo get the most out of this course, you should have:\nBasic coding experience\nFamiliarity with SQL\nExperience with Python (helpful but not required)\nNo prior data engineering experience is necessary. See Readme on GitHub",
        "section": "General course-related questions",
        "question": "Course - What are the prerequisites for this course?"
      },
      {
        "text": "Yes, even if you don't register, you're still eligible to submit the homework.\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.",
        "section": "General course-related questions",
        "question": "Course - Can I still join the course after the start date?"
      },
      {
        "text": "You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.",
        "section": "General course-related questions",
        "question": "Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?"
      },
      {
        "text": "Start by installing and setting up all the dependencies and requirements:\nGoogle cloud account\nGoogle Cloud SDK\nPython 3 (installed with Anaconda)\nTerraform\nGit\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.",
        "section": "General course-related questions",
        "question": "Course - What can I do before the course starts?"
      },
      {
        "text": "There are multiple Zoomcamps in a year, as of 2025. More info at DTC Article.\nHowever, they are five separate courses, estimated to be during these months:\nData-Engineering (Jan - Apr)\nStock Market Analytics (Apr - May)\nMLOps (May - Aug)\nLLM (June - Sep)\nMachine Learning (Sep - Jan)\nThere's only one Data-Engineering Zoomcamp \u201clive\u201d cohort per year, for the certification. Same as for the other Zoomcamps.\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you\u2019re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any \u201clive\u201d cohort.",
        "section": "General course-related questions",
        "question": "Course - how many Zoomcamps in a year?"
      },
      {
        "text": "For the 2025 edition we are using Kestra (see Demo) instead of MageAI (Module 2). Lookout for new videos. See Playlist\nFor the 2024 edition we used Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow. See Playlists on YouTube and cohorts folder in Github repo.",
        "section": "General course-related questions",
        "question": "Course - Is the current cohort going to be different from the previous cohort?"
      },
      {
        "text": "Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.",
        "section": "General course-related questions",
        "question": "Course - Can I follow the course after it finishes?"
      },
      {
        "text": "Yes, the slack channel remains open and you can ask questions there. But always search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don\u2019t rely on its answers 100%, it is pretty good though.",
        "section": "General course-related questions",
        "question": "Course - Can I get support if I take the course in the self-paced mode?"
      },
      {
        "text": "All the main videos are stored in the Main \u201cDATA ENGINEERING ZOOMCAMP\u201d playlist (no year specified). The Github repository has also been updated (if not create a pull request) to show each video with a thumbnail, that would bring you directly to the same playlist below.\nBelow is the MAIN PLAYLIST\u2019. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\nData Engineering Zoomcamp\nData Engineering Zoomcamp 2022\nData Engineering Zoomcamp 2023\nData Engineering Bootcamp 2024\nData Engineering Bootcamp 2025\nDE Zoomcamp 2025 (Modul 2 Kestra)",
        "section": "General course-related questions",
        "question": "Course - Which playlist on YouTube should I refer to?"
      },
      {
        "text": "It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\nYou can also calculate it yourself using this data and then update this answer.",
        "section": "General course-related questions",
        "question": "Course - \u200b\u200bHow many hours per week am I expected to spend on this  course?"
      },
      {
        "text": "The zoom link is only published to instructors/presenters/TAs.\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack and is in google calendar before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\nDon\u2019t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.",
        "section": "General course-related questions",
        "question": "Office Hours - What is the video/zoom link to the stream for the \u201cOffice Hour\u201d or workshop sessions?"
      },
      {
        "text": "Yes! Every \u201cOffice Hours\u201d will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Office Hours - I can\u2019t attend the \u201cOffice hours\u201d / workshop, will it be recorded?"
      },
      {
        "text": "The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Edit Course Profile."
      },
      {
        "text": "No, as long as you do the peer-reviewed capstone projects in time then you can get the certificate. You do not need to do the homeworks if you join late for example.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Certificate - Do I need to do the homeworks to get the certificate?"
      },
      {
        "text": "No, you can only get a certificate if you finish the course with a \u201clive\u201d cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Certificate - Can I follow the course in a self-paced mode and get a certificate?"
      },
      {
        "text": "2025 deadlines will be announced on https://courses.datatalks.club/de-zoomcamp-2025/ and in Google Calendar\nYou can find the 2024 deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Homework - What are homework and project deadlines?"
      },
      {
        "text": "No, late submissions are not allowed. But if the form is still not closed and it\u2019s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page. Make sure you are logged in.\nOlder news:[source1] [source2]",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Homework - Are late submissions of homework allowed?"
      },
      {
        "text": "Answer: In short, it\u2019s your repository on github, gitlab, bitbucket, etc\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises. Think of it like a portfolio you could present to an employer.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Homework - What is the homework URL in the homework link?"
      },
      {
        "text": "After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you\u2019ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear,(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379? others work as follows:\nYou get maximum 1 point for the FAQ Contribution in the respective week\nFor each learning in a public link you get one point, so you can get maximum 7 points.\nCheck this Video: https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Homework and Leaderboard - what is the system for points in the course management platform?"
      },
      {
        "text": "When you set up your account you are automatically assigned a random name such as \u201cLucid Elbakyan\u201d for example. If you want to see what your Display name is.\nGo to your profile:  \u2192\n2025: https://courses.datatalks.club/de-zoomcamp-2025/enrollment\n2024: https://courses.datatalks.club/de-zoomcamp-2024/enrollment \n\nLog in -> your display name is here, you can also change it should you wish. Make sure your Certificate name is correct, this name will later be printed on your certificate!!!",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?"
      },
      {
        "text": "Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\nBut Python 3.10 and 3.11 should work fine.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Is Python 3.9 still the recommended version to use in 2024?"
      },
      {
        "text": "You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\nYou might face some challenges, especially for Windows users.\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\nHowever, if you prefer to set up a virtual machine, you may start with these first:\nUsing GitHub Codespaces\nSetting up the environment on a cloud VM codespace\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?"
      },
      {
        "text": "GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\nYou can also open any GitHub repository in a GitHub Codespace.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?"
      },
      {
        "text": "It's up to you which platform and environment you use for the course.\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed."
      },
      {
        "text": "Choose the approach that aligns the most with your idea for the end project\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Do I need both GitHub Codespaces and GCP?"
      },
      {
        "text": "This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor\n1. To open Run command window, you can either:\n(1-1) Use the shortcut keys: 'Windows + R', or\n(1-2) Right Click \"Start\", and click \"Run\" to open.\n2. Registry Values Located in Registry Editor, to open it: Type 'regedit' in the Run command window, and then press Enter.' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor\" from \"if exists\" to a blank.\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\Users\\<your_user_name>\\.ssh\\known_host",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Could not establish connection to \"MyServerName\": Got bad result from install script"
      },
      {
        "text": "For uniformity.\nYou can  use other cloud platforms, since you get every service that\u2019s been provided by GCP in Azure and AWS,  you\u2019re not restricted to GCP, you can use other cloud platforms like AWS if you\u2019re comfortable with AWS or others.\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\nNote that to sign up for a free GCP account, you must have a valid credit card.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Why are we using GCP and not other cloud providers?"
      },
      {
        "text": "No, if you use and take advantage of their free trial.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Should I pay for cloud services?"
      },
      {
        "text": "You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won\u2019t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\nFor everything in the course, there\u2019s a local alternative. You could even do the whole course locally. HW3 needed BigQuery.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?"
      },
      {
        "text": "Google Cloud Platform (GCP) provides two free trial options: the Free Trial and the Sandbox. Note that users can switch from Sandbox to Free Trial anytime by adding billing details. The reverse is true at anytime as well. You can switch from the GCP Free Trial to the Sandbox option. To do this, you'll need to disable billing on your project. Once billing is disabled, your project will revert to the Sandbox mode, allowing you to use the limited free resources without a billing account.\n\nHowever, completing the course using the GCP Sandbox option is not possible because the Sandbox has limited features compared to the full Free Trial with $300 credit. The course will involve using services that are not available in the Sandbox environment. The FAQ indicates that while the course may start locally, it will eventually transition to using VMs, GCS Bucket and other paid services on on GCP, which would require the full capabilities provided by the $300 credit option. Additionally, the course emphasizes the use of BigQuery, which is a key component of GCP, and the Sandbox may not support all necessary functionalities for working with it effectively. Therefore, it's recommended to utilize the full Free Trial with billing details to ensure access to all required features for the course.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Can DE Zoomcamp course be completed using only the GCP Sandbox option, or is the Free Trial required at any point?"
      },
      {
        "text": "Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\nThe problem would be when you need help. You\u2019d need to rely on fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\nSee the de-course-aws channel on slack\nAlso see Is it possible to use x tool instead of the one tool you use?",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - I want to use AWS. May I do that?"
      },
      {
        "text": "We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.\nSee Google Calendar",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Besides the \u201cOffice Hour\u201d which are the live zoom calls?"
      },
      {
        "text": "No, but we moved the 2022 stuff to the cohort 2022 folder on github (here)",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Is the 2022 repo deleted?"
      },
      {
        "text": "Yes, you can use any tool you want for your project.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Can I use Airflow instead for my final project?"
      },
      {
        "text": "Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\nShould you consider it instead of the one tool you use? That we can\u2019t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Is it possible to use tool \u201cX\u201d instead of the one tool you use in the course?"
      },
      {
        "text": "Star the repo! Share it with friends if you find it useful \u2763\ufe0f\nCreate a PR if you see you can improve the text or the structure of the repository.\nUpdate this FAQ.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "How can we contribute to the course?"
      },
      {
        "text": "Yes! Linux is ideal but technically it should not matter. Students in the 2024 cohort used all 3 OSes successfully.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Is the course [Windows/macOS/Linux/...] friendly?"
      },
      {
        "text": "Later modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Environment - Roadblock for Windows users in modules with *.sh (shell scripts)."
      },
      {
        "text": "Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Any books or additional resources you recommend?"
      },
      {
        "text": "You will have two attempts for a project. If the first project deadline is over and you\u2019re late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Project - What is Project Attempt #1 and Project Attempt #2 exactly?"
      },
      {
        "text": "The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\nRestart app or server/pc. In\nGoogle it, use ChatGPT, Bing AI etc.\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\nThere are often different solutions for the same problem due to variation in environments.\nCheck the tech\u2019s documentation. Use its search if available or use the browsers search function.\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\nhttps://stackoverflow.com/help/how-to-ask\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\nAsk in Slack\nBefore asking a question,\nCheck Pins \ud83d\udccc in channel (where the shortcut to the repo and this FAQ is located)\nUse the slack app\u2019s search function\ncheck the FAQ (this document), use search [ctrl+f]\nUse the bot @ZoomcampQABot to do the search for you\nWhen asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the \u201coffending\u201d code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\nDO NOT use screenshots, especially don\u2019t take pictures from a phone.\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it\u2019s long, just post it in a reply to your thread.\nUse ``` for formatting your code.\nUse the same thread for the conversation (that means reply to your own thread).\nDO NOT create multiple posts to discuss the issue.\nYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\nProvide additional information in the same thread of the steps you have taken for resolution.\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\nRemember technology issues in real life sometimes take days or even weeks to resolve.\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "How to troubleshoot issues"
      },
      {
        "text": "When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\nWhat are you coding on? What OS?\nWhat command did you run, which video did you follow? Etc etc\nWhat error did you get? Does it have a line number to the \u201coffending\u201d code and have you check it for typos?\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "How to ask questions"
      },
      {
        "text": "After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\nHaving this local repository on your computer will make it easy for you to access the instructors\u2019 code and make pull requests (if you want to add your own notes or make changes to the course content).\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: How to Create a Git Repository | Atlassian Git Tutorial\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: .gitignore file - ignoring files in Git | Atlassian Git Tutorial\nNEVER stores passwords or keys in a git repo (even if that repo is set to private). Put files containing sensitive information (.env, secret.json etc.) in your .gitignore.\nThis is also a great resource: Dangit, Git!?!",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "How do I use Git / GitHub for this course?"
      },
      {
        "text": "Error: Makefile:2: *** missing separator.  Stop.\nSolution: Tabs in documents should be converted to Tab instead of spaces. Follow this stack.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "VS Code: Tab using spaces"
      },
      {
        "text": "If you\u2019re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\nwslview index.html\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Opening an HTML file with a Windows browser from Linux running on WSL"
      },
      {
        "text": "This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.",
        "section": "Course Management Platform for Homeworks, Project and Certificate",
        "question": "Set up Chrome Remote Desktop for Linux on Compute Engine"
      },
      {
        "text": "Q: When will it be sent out / released?\nQ: How do I get my certificate after project(s) have been reviewed and graded?\nA: There\u2019ll be an announcement in Telegram and the course channel for\n(1) checking that your proper full name is how you want displayed on the Certificate (see Editing course profile on the Course Management webpage), and\n(2)  when the grading is completed.\nAfter the second announcement, please follow instructions in https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md on how to generate the Certificate document yourself.\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\nWhen you try to download the 2021 data from TLC website, you get this error:\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-datar\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\nNote: Make sure to unzip the \u201cgz\u201d file (no, the \u201cunzip\u201d command won\u2019t work for this.)",
        "section": "Module 1: Docker and Terraform",
        "question": "Certificate - generating, receiving after projects graded"
      },
      {
        "text": "In this video, we store the data file as \u201coutput.csv\u201d. The data file won\u2019t store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = \u201coutput.cs -v\u201d with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = \u201coutput.csv\u201d with\ncsv_name = url.split(\u201c/\u201d)[-1] . Then when we use csv_name to using pd.read_csv, there won\u2019t be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.",
        "section": "Module 1: Docker and Terraform",
        "question": "Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?"
      },
      {
        "text": "Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\nGreen Trips: Data Dictionary - LPEP Trip Records May 1, 2018",
        "section": "Module 1: Docker and Terraform",
        "question": "Taxi Data - Data Dictionary for NY Taxi data?"
      },
      {
        "text": "You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\n\u2018\u2019\u2019gunzip green_tripdata_2019-09.csv.gz\u2019\u2019\u2019\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\nIn the def main(params) add this line\nparquet_name= 'output.parquet'\nThen edit the code which downloads the files\nos.system(f\"wget {url} -O {parquet_name}\")\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\ndf = pd.read_parquet(parquet_name)\ndf.to_csv(csv_name, index=False)",
        "section": "Module 1: Docker and Terraform",
        "question": "Taxi Data - Unzip Parquet file"
      },
      {
        "text": "\u201cwget is not recognized as an internal or external command\u201d, you need to install it.\n\u201c\u200b\u200bNo such file or directory: 'output.csv.gz'\u201d, may also caused by wget not recognized\n. \nOn Ubuntu, run:\n$ sudo apt-get install wget\nOn MacOS, the easiest way to install wget is to use Brew:\n$ brew install wget\nOn Windows, the easiest way to install wget is to use Chocolatey:\n$ choco install wget\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\nAlso, you can following this step to install Wget on MS Windows\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\n* Move wget.exe to your `Git\\mingw64\\bin\\`.\nAlternatively, you can use a Python wget library, but instead of simply using \u201cwget\u201d you\u2019ll need to use \npython -m wget\nYou need to install it with pip first:\npip install wget\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. You\u2019ll want to move the resulting file into your working directory.\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests",
        "section": "Module 1: Docker and Terraform",
        "question": "wget is not recognized as an internal or external command"
      },
      {
        "text": "Firstly, make sure that you add \u201c!\u201d before wget if you\u2019re running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\nUsing the Python library wget you installed with pip, try python -m wget <url>\nWrite the usual command and add --no-check-certificate at the end. So it should be:\n!wget <website_url> --no-check-certificate",
        "section": "Module 1: Docker and Terraform",
        "question": "wget - ERROR: cannot verify <website> certificate  (MacOS)"
      },
      {
        "text": "For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\ (no need to include in .bashrc)",
        "section": "Module 1: Docker and Terraform",
        "question": "Git Bash - Backslash as an escape character in Git Bash for Windows"
      },
      {
        "text": "Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs",
        "section": "Module 1: Docker and Terraform",
        "question": "GitHub Codespaces - How to store secrets"
      },
      {
        "text": "With the default instructions and running pgadmin in docker you may receive a blank screen after logging in to the pgadmin console. To resolve this, add the following two environment variables to your pgadmin config to allow it to work with codespace\u2019s reverse proxy:\nPGADMIN_CONFIG_PROXY_X_HOST_COUNT: 1\nPGADMIN_CONFIG_PROXY_X_PREFIX_COUNT: 1",
        "section": "Module 1: Docker and Terraform",
        "question": "Github Codespaces - Running pgadmin in docker"
      },
      {
        "text": "Make sure you're able to start the Docker daemon, and check the issue immediately down below:\nAnd don\u2019t forget to update the wsl in powershell the  command is wsl \u2013update",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
      },
      {
        "text": "As the official Docker for Windows documentation says, the Docker engine can either use the\nHyper-V or WSL2 as its backend. However, a few constraints might apply\nWindows 10 Pro / 11 Pro Users: \nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nWindows 10 Home / 11 Home Users: \nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \n\nhttps://github.com/microsoft/WSL/issues/5393",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified"
      },
      {
        "text": "Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage). \nIF the repository is public, the fetch and download happens without any issue whatsoever.\nFor instance:\ndocker pull postgres:13\ndocker pull dpage/pgadmin4\nBE ADVISED:\n\nThe Docker Images we'll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\n\nMeaning: you are NOT required to perform a docker login to fetch them. \n\nSo if you get the message above saying \"docker login': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\n\nFor instance:\n$ docker pull dbpage/pgadmin4\nWill throw that exception telling you \"repository does not exist or may require 'docker login'\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \nmay require 'docker login': denied: requested access to the resource is denied\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\nHow to fix it:\n$ docker pull dpage/pgadmin4\nEXTRA NOTES:\nIn the real world, occasionally, when you're working for a company or closed organisation, the Docker image you're trying to fetch might be under a private repo that your DockerHub Username was granted access to.\nFor which cases, you must first execute:\n$ docker login\nFill in the details of your username and password.\nAnd only then perform the `docker pull` against that private repository",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - docker pull dbpage"
      },
      {
        "text": "Issue Description:\nWhen attempting to run a Docker command similar to the one below:\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\mount\npostgres:13\nYou encounter the error message:\ndocker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\nSolution:\n1- Stop Rancher Desktop:\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n2- Install Docker Desktop:\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\n2-Retry Docker Command:\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - \"permission denied\" error when creating a PostgreSQL Docker with a mounted volume on macOS M1"
      },
      {
        "text": "When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\nsudo rm -r -f docker_test/\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - can\u2019t delete local folder that mounted to docker volume"
      },
      {
        "text": "First off, make sure you're running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn't work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Docker won't start or is stuck in settings (Windows 10 / 11)"
      },
      {
        "text": "If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.",
        "section": "Module 1: Docker and Terraform",
        "question": "Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?"
      },
      {
        "text": "You may have this error:\n$ docker run -it ubuntu bash\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\nerror:\nSolution:\nUse winpty before docker command (source)\n$ winpty docker run -it ubuntu bash\nYou also can make an alias:\necho \"alias docker='winpty docker'\" >> ~/.bashrc\nOR\necho \"alias docker='winpty docker'\" >> ~/.bash_profile",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - The input device is not a TTY (Docker run for Windows)"
      },
      {
        "text": "You may have this error:\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\n/simple/pandas/\nPossible solution might be:\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Cannot pip install on Docker container (Windows)"
      },
      {
        "text": "Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\nwinpty docker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"C:\\Users\\abhin\\dataengg\\DE_Project_git_connected\\DE_OLD\\week1_set_up\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\n-p 5432:5432 \\\npostgres:13\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video.\nNote: Check he example for the direction of the / \\\n**Another possible solution for windows, make sure to finish the folder path with a forward slash / :\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v /\"$(pwd)\"/ny_taxi_postgres_data/:/var/lib/postgresql/data/\\\n-p 5432:5432 \\\npostgres:13",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - ny_taxi_postgres_data is empty"
      },
      {
        "text": "Check this article for details - Setting up docker in macOS\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Haven\u2019t had an issue with that method.\nbrew install conflict with docker desktop and command line tools. You need to install docker desktop first and then the command line tools. [Issue](https://github.com/Homebrew/brew/issues/16309)\nbrew install \u2013cask docker\nbrew install docker docker-compose",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Setting up Docker on Mac"
      },
      {
        "text": "$ docker run -it\\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"admin\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\n-p 5432:5432 \\\npostgres:13\nCCW\nThe files belonging to this database system will be owned by user \"postgres\".\nThis use The database cluster will be initialized with locale \"en_US.utf8\".\nThe default database encoding has accordingly been set to \"UTF8\".\nxt search configuration will be set to \"english\".\nData page checksums are disabled.\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\nThe input dtc_postgres_volume_local must match in both commands below\n$ docker volume create --name dtc_postgres_volume_local -d local\n$ docker run -it\\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\n-p 5432:5432\\\npostgres:13\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\nAn alternate error could be:\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\nIf you want to create a new database system, either remove or empthe directory \"/var/lib/postgresql/data\" or run initdb\nwitls",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted"
      },
      {
        "text": "Mapping volumes on Windows could be tricky. The way it was done in the course video doesn\u2019t work for everyone.\nFirst, if you move your data to some folder without spaces. E.g. if your code is in \u201cC:/Users/Alexey Grigorev/git/\u2026\u201d, move it to \u201cC:/git/\u2026\u201d\nTry replacing the \u201c-v\u201d part with one of the following options:\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\nTry adding winpty before the whole command:\nwinpty docker run -it\n-e POSTGRES_USER=\"root\"\n-e POSTGRES_PASSWORD=\"root\"\n-e POSTGRES_DB=\"ny_taxi\"\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n-p 5432:5432\npostgres:1\nTry adding quotes:\n-v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n-v \u201c/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n-v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n-v \"c:\\some\\path\\ny_taxi_postgres_data\":/var/lib/postgresql/data\nNote:  (Window) if it automatically creates a folder called \u201cny_taxi_postgres_data;C\u201d suggests you have problems with volume mapping, try deleting both folders and replacing \u201c-v\u201d part with other options. For me \u201c//c/\u201d works instead of \u201c/c/\u201d. And it will work by automatically creating a correct folder called \u201cny_taxi_postgres_data\u201d.\nA possible solution to this error would be to use /\u201d$(pwd)\u201d/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes\u2019 position varying as in the above list).\nYes for windows use the command it works perfectly fine\n-v /\u201d$(pwd)\u201d/ny_taxi_postgres_data:/var/lib/postgresql/data\nImportant: note how the quotes are placed.\nIf none of these options work, you can use a volume name instead of the path:\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\nPostgres:13\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)"
      },
      {
        "text": "Change the mounting path. Replace it with one of following:\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\ (leading slash in front of c:)",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Error response from daemon: invalid mode: \\Program Files\\Git\\var\\lib\\postgresql\\data."
      },
      {
        "text": "When you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v <your path>:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\nWhen you run this command second time\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-p 5432:5432 \\\npostgres:13",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists"
      },
      {
        "text": "This error appeared when running the command: docker build -t taxi_ingest:v001 .\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn\u2019t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\nsudo chown -R $USER dir_path\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \n\n\t\t\t\t\t\t\t\t\t\t\tAdded by\n\t\t\t\t\t\t\t\t\t\t\tKenan Arslanbay",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''."
      },
      {
        "text": "You might have installed docker via snap. Run \u201csudo snap status docker\u201d to verify.\nIf you have \u201cerror: unknown command \"status\", see 'snap help'.\u201d as a response than deinstall docker and install via the official website\nBind for 0.0.0.0:5432 failed: port is a",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - ERRO[0000] error waiting for container: context canceled"
      },
      {
        "text": "Found the issue in the PopOS linux. It happened because our user didn\u2019t have authorization rights to the host folder ( which also caused folder seems empty, but it didn\u2019t!).\n\u2705Solution:\nJust add permission for everyone to the corresponding folder\nsudo chmod -R 777 <path_to_folder>\nExample:\nsudo chmod -R 777 ny_taxi_postgres_data/",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - build error checking context: can\u2019t stat \u2018/home/fhrzn/Projects/\u2026./ny_taxi_postgres_data\u2019"
      },
      {
        "text": "This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\n$ docker build -t taxi_ingest:v001 .\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\n$ sudo chmod -R 755 ny_taxi_postgres_data\nOr use 777 if you still see problems. 755 grants write access to only the owner.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied."
      },
      {
        "text": "Get the network name via: $ docker network ls.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Docker network name"
      },
      {
        "text": "Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\n>>> If the container is running state, use docker stop <container_name>\n>>> then, docker rm pg-database\nOr use docker start instead of docker run in order to restart the docker image without removing it.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container \u201cxxx\u201d.  You have to remove (or rename) that container to be able to reuse that name."
      },
      {
        "text": "Typical error:n.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\nE.g.:\npg-network becomes 2docker_default",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - ingestion when using docker-compose could not translate host name"
      },
      {
        "text": "terraformRun this command before starting your VM:\nOn Intel CPU:\nmodprobe -r kvm_intel\nmodprobe kvm_intel nested=1\nOn AMD CPU:\nmodprobe -r kvm_amd\nmodprobe kvm_amd nested=1",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization)."
      },
      {
        "text": "It\u2019s very easy to manage your docker container, images, network and compose projects from VS Code.\nJust install the official extension and launch it from the left side icon.\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - Connecting from VS Code"
      },
      {
        "text": "Use the following command:\n$ docker stop <container_id>",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - How to stop a container?"
      },
      {
        "text": "When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\nconnection failed: server closed the connection unexpectedly\nThis probably means the server terminated abnormally before or while processing the request.\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.\nSolution 2:\nIf your data is critical, you may be able to reset the write-ahead lock from within the docker container (see here)\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\n--network pg-network \\\npostgres:13 \\\n/bin/bash -c 'gosu postgres pg_resetwal /var/lib/postgresql/data'",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down"
      },
      {
        "text": "On some versions of Ubuntu, snap command can be used to install Docker.\nsudo snap install docker",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker not installable on Ubuntu"
      },
      {
        "text": "error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\nvolumes:\ndtc_postgres_volume_local:  # Define the named volume here\n# services mentioned in the compose file auto become part of the same network!\nservices:\nyour remaining code here . . .\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\nIn my case, after i ran docker compose up the mounting dir created was named \u2018docker_sql_dtc_postgres_volume_local\u2019 whereas it should have used the already existing \u2018dtc_postgres_volume_local\u2019\nAll i did to fix this is that I renamed the existing \u2018dtc_postgres_volume_local\u2019 to \u2018docker_sql_dtc_postgres_volume_local\u2019 and removed the newly created one (just be careful when doing this)\nrun docker compose up again and check if the table is there or not!",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - mounting error"
      },
      {
        "text": "Couldn\u2019t translate host name to address\nMake sure postgres database is running.\n\n\u200b\u200bUse the command to start containers in detached mode: docker-compose up -d\n(data-engineering-zoomcamp) hw % docker compose up -d\n[+] Running 2/2\n\u283f Container pg-admin     Started                                                                                                                                                                      0.6s\n\u283f Container pg-database  Started\nTo view the containers use: docker ps.\n(data-engineering-zoomcamp) hw % docker ps\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\nfaf05090972e   postgres:13      \"docker-entrypoint.s\u2026\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\nhw\nTo view logs for a container: docker logs <containerid>\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\nPostgreSQL Database directory appears to contain a database; Skipping initialization\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\nprogress\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\n2022-01-25 05:59:33.726 UTC [28\n] LOG:  redo done at 0/98A3C128\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\nIf docker ps doesn\u2019t show pgdatabase running, run: docker ps -a\nThis should show all containers, either running or stopped.\nGet the container id for pgdatabase-1, and run",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Error translating host name to address"
      },
      {
        "text": "After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\nIf problems persist with pgcli, we can use HeidiSQL\nKrishna Anand",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)"
      },
      {
        "text": "It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\nTry:\ndocker ps -a to see all the stopped & running containers\nd to nuke all the containers\nTry: docker-compose up -d again ports\nOn localhost:8080 server \u2192 Unable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve\nTry: new host name, best without \u201c - \u201d e.g. pgdatabase\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\nservices:\npgdatabase:\nimage: postgres:13\nenvironment:\n- POSTGRES_USER=root\n- POSTGRES_PASSWORD=root\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\nports:\n- \"5431:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Hostname does not resolve"
      },
      {
        "text": "When you login into PgAdmin and see empty database, the solution below can help:\nWhen you run\ndocker-compose up\nand at the same time\ndocker build -t taxi_ingest:v001 .\nwith\ndocker run -it \\\n--network=pg-network \\ \u2190 <---- NETWORK NAME IS THE SAME AS THAT CREATED BY DOCKER COMPOSE\ntaxi_ingest:v001 \\\n--user=postgres \\\n--password=postgres \\\n--host=db \\\n--port=5432 \\\n--db=ny_taxi \\\n--table_name=green_tripdata \\\n--url=${URL}\nIt\u2019s important to use the same --network which states in the file docker-compose.yaml (networks, as mentioned above).  OR The file docker-compose.yaml might not specify a network, as in the example below.\nservices:\ndb:\ncontainer_name: postgres\nimage: postgres:17-alpine\nenvironment:\n\u2026\nports:\n- '5433:5432'\nvolumes:\n- \u2026\npgadmin:\ncontainer_name: pgadmin\nimage: dpage/pgadmin4:latest\nenvironment:\n\u2026\nports:\n- \"8080:80\"\nvolumes:\n- \u2026\nvolumes:\nvol-pgdata:\nname: vol-pgdata\nvol-pgadmin_data:\nname: vol-pgadmin_data\nIn this case, the network name is generated automatically: The name of the directory containing the docker-compose.yaml file in lowercase + _default.\nYou can find the network\u2019s name during running docker-compose up\npg-database Pulling\n pg-database Pulled\n Network week_1_default  Creating <-- THIS ONE\n Network week_1_default  Created",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose + PgAdmin \u2013 no database in PgAdmin"
      },
      {
        "text": "So one common issue is when you run docker-compose on GCP, postgres won\u2019t persist it\u2019s data to mentioned path for example:\nservices:\n\u2026\n\u2026\npgadmin:\n\u2026\n\u2026\nVolumes:\n\u201c./pgadmin\u201d:/var/lib/pgadmin:wr\u201d\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\nservices:\n\u2026\n\u2026.\npgadmin:\n\u2026\n\u2026\nVolumes:\npgadmin:/var/lib/pgadmin\nvolumes:\nPgadmin:",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Persist PGAdmin docker contents on GCP"
      },
      {
        "text": "The docker will keep on crashing continuously\nNot working after restart\ndocker engine stopped\nAnd failed to fetch extensions pop ups will on screen non-stop\nSolution :\nTry checking if latest version of docker is installed / Try updating the docker\nIf Problem still persist then final solution is to reinstall docker\n(Just have to fetch images again else no issues)",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker engine stopped_failed to fetch extensions"
      },
      {
        "text": "As per the lessons,\nPersisting pgAdmin configuration (i.e. server name) is done by adding a \u201cvolumes\u201d section:\nservices:\npgdatabase:\n[...]\npgadmin:\nimage: dpage/pgadmin4\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root\nvolumes:\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\nports:\n- \"8080:80\"\nIn the example above, \u201dpgAdmin_data\u201d is a folder on the host machine, and \u201c/var/lib/pgadmin/sessions\u201d is the session settings folder in the pgAdmin container.\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the \u201cpgAdmin_data\u201d folder. The container runs with a username called \u201c5050\u201d and user group \u201c5050\u201d. The bash command to give access over the mounted volume is:\nsudo chown -R 5050:5050 pgAdmin_data",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Persist PGAdmin configuration"
      },
      {
        "text": "This happens if you did not create the docker group and added your user. Follow these steps from the link:\nguides/docker-without-sudo.md at main \u00b7 sindresorhus/guides \u00b7 GitHub\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\nvolumes:\n- type: volume\nsource: pgadmin_data\ntarget: /var/lib/pgadmin\nAlso add the following to the end of the file:ls\nvolumes:\nPgadmin_data:",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied"
      },
      {
        "text": "This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - docker-compose still not available after changing .bashrc"
      },
      {
        "text": "Installing pass via \u2018sudo apt install pass\u2019 helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Error getting credentials after running docker-compose up -d"
      },
      {
        "text": "For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\ncreate a new volume on docker (either using the command line or docker desktop app)\nmake the following changes to your docker-compose.yml file (see attachment)\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\nOrder of execution:\n(1) open terminal in 2_docker_sql folder and run docker compose up\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n(3) open jupyter notebook and begin the data ingestion\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup"
      },
      {
        "text": "Locate config.json file for docker (check your home directory; Users/username/.docker).\nModify credsStore to credStore\nSave and re-run",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``"
      },
      {
        "text": "To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\nuname -s  -> return Linux most likely\nuname -m -> return \"flavor\"\nOr try this command -\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Which docker-compose binary to use for WSL?"
      },
      {
        "text": "If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\nvolumes:\ndtc_postgres_volume_local:",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Error undefined volume in Windows/WSL"
      },
      {
        "text": "This error means the docker-compose executable can\u2019t be opened in current OS. Make sure the file you download from github matches your system environment.\nAs of 2025/1/17, docker-compose (v2.32.4) docker-compose-linux-aarch64 does not work, try v2.32.3 docker-compose-linux-x86_64",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - cannot execute binary file: Exec format error"
      },
      {
        "text": "This happens due to the Postgres database not being initialized before running docker-compose up -d. There are other potential ways around it (thread) but you can simply initialize the database first and the compose will work afterward.\ndocker run -it \\\n-e POSTGRES_USER=\"root\" \\\n-e POSTGRES_PASSWORD=\"root\" \\\n-e POSTGRES_DB=\"ny_taxi\" \\\n-v $(pwd)/ny_taxi_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\n--network=pg-network \\\n--name=pg_database \\\npostgres:13",
        "section": "Module 1: Docker and Terraform",
        "question": "Docker-Compose - Postgres container fails to launch with exit code (1) when attempting to compose"
      },
      {
        "text": "Error:  initdb: error: could not change permissions of directory\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\nSolution: Use Docker volumes.\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\nBenefit: This resolves permission issues and allows for better management of volumes.\nNOTE: the \u2018user:\u2019 is not necessary if using docker volumes, but is if using local drive.\n</>  docker-compose.yaml\nservices:\npostgres:\nimage: postgres:15-alpine\ncontainer_name: postgres\nuser: \"0:0\"\nenvironment:\n- POSTGRES_USER=postgres\n- POSTGRES_PASSWORD=postgres\n- POSTGRES_DB=ny_taxi\nvolumes:\n- \"pg-data:/var/lib/postgresql/data\"\nports:\n- \"5432:5432\"\nnetworks:\n- pg-network\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin\nuser: \"${UID}:${GID}\"\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\nvolumes:\n- \"pg-admin:/var/lib/pgadmin\"\nports:\n- \"8080:80\"\nnetworks:\n- pg-network\nnetworks:\npg-network:\nname: pg-network\nvolumesta:\nname: ingest_pgdata\npg-admin:\nname: ingest_pgadmin:\npg-da",
        "section": "Module 1: Docker and Terraform",
        "question": "WSL Docker directory permissions error"
      },
      {
        "text": "Cause:\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\nSolution\nfor updating Windows terminal which worked for me:\nGo to Microsoft Store.\nGo to the library of apps installed in your system.\nSearch for Windows terminal.\nUpdate the app and restart your system to  see the changes.\nFor updating the Windows security updates:\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\nDo restart your system once the updates are downloaded and installed successfully.unexpectedly",
        "section": "Module 1: Docker and Terraform",
        "question": "WSL - Insufficient system resources exist to complete the requested service."
      },
      {
        "text": "Up restarting the same issue appears. Happens out of the blue on windows.\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\nRestart your computer and then enable it with the following\nreg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\nRestart your OS again. It should work.\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\" n\nbash: conda: command not found\nDatabase is uninitialized and superuser password is not specified.\nDatabase is uninitialized and superuser password is not specified.",
        "section": "Module 1: Docker and Terraform",
        "question": "WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1."
      },
      {
        "text": "Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn\u2019t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\nYou can try to use sudo before the command\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\nchmod 600 gpc\nIf that doesn\u2019t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\ncd ~\nmkdir .ssh\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\nYou might need to adjust the permissions of the files and folders in the .ssh directory.",
        "section": "Module 1: Docker and Terraform",
        "question": "WSL - Permissions too open at Windows"
      },
      {
        "text": "Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\ncd ~\nmkdir .ssh\nCreate a config file in this new .ssh/ folder referencing this folder:\nHostName [GPC VM external IP]\nUser [username]\nIdentityFile ~/.ssh/[private key]",
        "section": "Module 1: Docker and Terraform",
        "question": "WSL - Could not resolve host name"
      },
      {
        "text": "Change TO Socket\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused"
      },
      {
        "text": "In this section of the course, the 5432 port of pgsql is mapped to your computer\u2019s 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\nSo No, you don\u2019t need to run it inside another container. Your local system will do.",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - should we run pgcli inside another docker container?"
      },
      {
        "text": "For a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\nIf you want to debug: the following can help (on a MacOS)\nTo find out if something is blocking your port (on a MacOS):\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\nOr list the running postgres services on your local machine with launchctl\nTo unload the running service on your local machine (on a MacOS):\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nthis one to start it again\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)"
      },
      {
        "text": "I get this error\npgcli -h localhost -p 5432 -U root -d ny_taxi\nTraceback (most recent call last):\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\nsys.exit(cli())\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\nreturn self.main(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\n1053, in main\nrv = self.invoke(ctx)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\nreturn __callback(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\nos.makedirs(config_dir)\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'\nSolution 1:\nThis error indicates that your user doesn\u2019t have the necessary permissions to access or modify the specified directory or file (/some/path/.config/pgcli).\nThis can happen in the context of Docker when privileges were assigned to root and not to the user you have.\nFor example, if a process inside the container creates the file as root, your user might not have write permissions to that file on the host.\nTo resolve this:\nCheck file permissions on the directory /some/path/.config/pgcli and ensure that your user has read/write access. You can do this with the command:\nls -l /some/path/.config/pgcli\nChange ownership/permissions of the file or directory so that your user has the necessary permissions. For example, to grant your user read/write permissions, use:\nsudo chown -R user_name /Users/user_name/.config\nThe sudo stands for Super User DO\nThe chown means change owner\n-R is doing so recursively\nUser_name is the name you gave to your PC (e.g. vray)\nSolution 2:\nMake sure you install pgcli without sudo.\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'"
      },
      {
        "text": "ImportError: no pq wrapper available.\nAttempts made:\n- couldn't import \\dt\nopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\nSolution:\nFirst, make sure your Python is set to 3.9, at least.\nAnd the reason for that is we have had cases of 'psycopg2-binary' failing to install because of an old version of Python (3.7.3). \n\n0. You can check your current python version with: \n$ python -V (the V must be capital)\n1. Based on the previous output, if you've got a 3.9, skip to Step #2\n   Otherwise better off with a new environment with 3.9\n$ conda create --name de-zoomcamp python=3.9\n$ conda activate de-zoomcamp\n2. Next, you should be able to install the lib for postgres like this:\n```\n$ pip install psycopg2-binary\n$ pip install psycopg_binary\n```\n3. If above steps do not work, try:\n```\n$ pip install --upgrade pgcli\n```\n4. Finally, make sure you're also installing pgcli, but use conda for that:\n```\n$ pgcli -h localhost -U root -d ny_taxisudo\n```\nThere, you should be good to go now!\nAnother solution:\nRun this",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - no pq wrapper available."
      },
      {
        "text": "If your Bash prompt is stuck on the password command for postgres\nUse winpty:\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\nAlternatively, try using Windows terminal or terminal in VS code.",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI -  stuck on password prompt"
      },
      {
        "text": "The error above was faced continually despite inputting the correct password\nSolution\nOption 1: Stop the PostgreSQL service on Windows\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\nOption 3: Change the port of the docker container\nOption 4: NEW SOLUTION: 27/01/2024\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\nIf you\u2019ve got the error above, it\u2019s probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\n\ndocker run -it \\\n-e POSTGRES_USER=root \\\n-e POSTGRES_PASSWORD=root \\\n-e POSTGRES_DB=ny_taxi \\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n-p 5432:5432 \\\npostgres:13\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.\nOption 5: Change the Port for Docker PostgreSQL\nAfter running the command: pgcli -h localhost -p 5432 -u root -d ny_taxi User get the enter password prompt and despite using the correct one, the error persist. This is provably due to user having installed Postgres in local machine. The easiest solution to this port conflict between host and container is by Changing the Port for Docker PostgreSQL: You can configure your Docker PostgreSQL container to use a different port. This way, it won't conflict with the PostgreSQL instance running on your local machine. When running the PostgreSQL container, map it to a different port on your host machine. E.g.:\n\n docker run -it \\\\\n-e POSTGRES_USER=\"root\" \\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\n-v c:/workspace/de-zoomcamp/1_intro_to_data_engineering/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\n-p 5433:5432 \\\\\nPostgres:13\n5433 refers to the port on the host machine.\n5432 refers to the port inside the Docker Postgres container.",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI -connection failed: FATAL: password authentication failed for user \"root\""
      },
      {
        "text": "Problem: If you have already installed pgcli but bash doesn't recognize pgcli\nOn Git bash: bash: pgcli: command not found\nOn Windows Terminal: pgcli: The term 'pgcli' is not recognized\u2026\nSolution: Try adding a Python path C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts to Windows PATH\nFor details:\nGet the location: pip list -v\nCopy C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\site-packages\n3. Replace site-packages with Scripts: C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts\nIt can also be that you have Python installed elsewhere.\nFor me it was under c:\\python310\\lib\\site-packages\nSo I had to add c:\\python310\\lib\\Scripts to PATH, as shown below.\nPut the above path in \"Path\" (or \"PATH\") in System Variables\nReference: https://stackoverflow.com/a/68233660",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - pgcli: command not found"
      },
      {
        "text": "In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\nBelow the usage with values used in the videos of the course for:\nnetwork name (docker network)\npostgres related variables for pgcli\nHostname\nUsername\nPort\nDatabase name\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\nPassword for root:\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\nVersion: 4.0.1\nHome: http://pgcli.com\nroot@pg-database:ny_taxi> \\dt\n+--------+------------------+-------+-------+\n| Schema | Name             | Type  | Owner |\n|--------+------------------+-------+-------|\n| public | yellow_taxi_data | table | root  |\n+--------+------------------+-------+-------+\nSELECT 1\nTime: 0.009s\nroot@pg-database:ny_taxi>",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - running in a Docker container"
      },
      {
        "text": "PULocationID will not be recognized but \u201cPULocationID\u201d will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.",
        "section": "Module 1: Docker and Terraform",
        "question": "RRPGCLI - case sensitive use \u201cQuotations\u201d around columns with capital letters"
      },
      {
        "text": "When using the command `\\d <database name>` you get the error column `c.relhasoids does not exist`.\nResolution:\nUninstall pgcli\nReinstall pgclidatabase \"ny_taxi\" does not exist\nRestart pc",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - error column c.relhasoids does not exist"
      },
      {
        "text": "1.2.2 Postgres commandline for docker\nVarious errors when first pasting docker run command - make sure there is only 1 space before \u201c\\\u201d and only a newline after \u201c\\\u201d\nError - posgres post is already in use. This seems to happen every time i try to start the docker postgres container.\nOption 1: Figure out what service is using the port (sudo lsof -i :5432) and stop that service:  sudo service postgresql stop.\nOption 2: more long term.\nI actually eventually ended up mapping to a different port, because this happened every time I restarted my VM. So I would map <local 5433: container 5432> in the docker file or docker compose file. Since i am using a VM, I also need to make sure that port 5433 is forwarded.",
        "section": "Module 1: Docker and Terraform",
        "question": "Postgres - bind: address already in use"
      },
      {
        "text": "The error persists because the psycopg library cannot find the required libpq library. Ensure the required PostgreSQL client library is installed:\n\tsudo apt install libpq-dev\nRebuild psycopg \n\tpip uninstall psycopg psycopg_binary psycopg_c -y\n\tpip install psycopg --no-binary psycopg\nThe issue should be resolved by now. However, even after these steps you get the error:\nModuleNotFoundError: No module named 'psycopg2'\nThen run the following:\n\tpip install psycopg2-binary",
        "section": "Module 1: Docker and Terraform",
        "question": "PGCLI - After installing PGCLI and checking with pgcli -- help we get the error: ImportError: no pq wrapper available"
      },
      {
        "text": "This happens while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThe port 5432 was taken by another postgres. You could already have installed Postgres in the past at the same port, so when you are trying to connect it does not reach docker, but the old Postgres installation instead. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432. Another option is to remove the old Postgres installation if it is useless.\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres. Stopping that service will resolve the issue",
        "section": "Module 1: Docker and Terraform",
        "question": "Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\""
      },
      {
        "text": "check that the port was properly forwarded. If 5432 is being used, kill the process:\nsudo lsof -i :5432\nsudo kill -9 PID\nWindows users:\nFound that my issue was related to PostgresSQL running locally on my machine and that pgAdmin4 was using my 5432 port.\nTo stop this process:\n1. Press Win + R to open the Run dialog.\n2. Type services.msc and press Enter.\n3. In the Services window, scroll down and look for a service with a name like PostgreSQL, postgresql-x64-13, or similar (the exact name depends on your PostgreSQL version).\n4. Right-click the PostgreSQL service and select Stop.",
        "section": "Module 1: Docker and Terraform",
        "question": "Postgres - connection failed: connection to server at \"127.0.0.1\", port 5432 failed: FATAL:  password authentication failed for user \"root\""
      },
      {
        "text": "Can happen when connecting via pgcli\npgcli -h localhost -p 5432 -U root -d ny_taxi\nOr while uploading data via the connection in jupyter notebook\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\nAlso, you could change port from 5432:5432 to 5431:5432\nOther solution that worked:\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.",
        "section": "Module 1: Docker and Terraform",
        "question": "Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist"
      },
      {
        "text": "~\\anaconda3\\lib\\site-packages\\psycopg2\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\n120\n121     dsn = _ext.make_dsn(dsn, **kwargs)\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n123     if cursor_factory is not None:\n124         conn.cursor_factory = cursor_factory\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\nMake sure postgres is running. You can check that by running `docker ps`\n\u2705Solution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432",
        "section": "Module 1: Docker and Terraform",
        "question": "Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist"
      },
      {
        "text": "Issue:\ne\u2026\nSolution:\npip install psycopg2-binary\nIf you already have it, you might need to update it:\npip install psycopg2-binary --upgrade\nOther methods, if the above fails:\nif you are getting the \u201c ModuleNotFoundError: No module named 'psycopg2' \u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\nFirst uninstall the psycopg package\nThen update conda or pip\nThen install psycopg again using pip.\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql",
        "section": "Module 1: Docker and Terraform",
        "question": "Postgres - ModuleNotFoundError: No module named 'psycopg2'"
      },
      {
        "text": "In the join queries, if we mention the column name directly or enclosed in single quotes it\u2019ll throw an error says \u201ccolumn does not exist\u201d.\n\u2705Solution: But if we enclose the column names in double quotes then it will work",
        "section": "Module 1: Docker and Terraform",
        "question": "Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)"
      },
      {
        "text": "pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.",
        "section": "Module 1: Docker and Terraform",
        "question": "pgAdmin - Create server dialog does not appear"
      },
      {
        "text": "Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\nCSRFError: 400 Bad Request: The referrer does not match the host.\nSolution #1:\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\nModified \u201cdocker run\u201d command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-p \"8080:80\" \\\n--name pgadmin \\\n--network=pg-network \\\ndpage/pgadmin4:8.2\nSolution #2:\nUsing the local installed VSCode to display GitHub Codespaces.\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.",
        "section": "Module 1: Docker and Terraform",
        "question": "pgAdmin - Blank/white screen after login (browser)"
      },
      {
        "text": "I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\nSolution #1:\nModified \u201cdocker run\u201d command\ndocker run --rm -it \\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\n-e PGADMIN_LISTEN_PORT=5050 \\\n-p 5050:5050 \\\n--network=de-zoomcamp-network \\\n--name pgadmin-container \\\n--link postgres-container \\\n-t dpage/pgadmin4\nSolution #2:\nModified docker-compose.yaml configuration (via \u201cdocker compose up\u201d command)\npgadmin:\nimage: dpage/pgadmin4\ncontainer_name: pgadmin-conntainer\nenvironment:\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\n- PGADMIN_LISTEN_PORT=5050\nvolumes:\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\nports:\n- \"5050:5050\"\nnetworks:\n- de-zoomcamp-network\ndepends_on:\n- postgres-conntainer",
        "section": "Module 1: Docker and Terraform",
        "question": "pgAdmin - Can not access/open the PgAdmin address via browser"
      },
      {
        "text": "Question: How can I keep pgAdmin settings after restarting the container?\nAnswer: Create a directory, map it to /var/lib/pgadmin, and fix permissions:\nCreate the directory for pgAdmin data:\n# mkdir -p /path/to/pgadmin-data\nAssign ownership to pgAdmin's user (ID 5050):\n# sudo chown -R 5050:5050 /path/to/pgadmin-data\n# sudo chmod -R 755 /path/to/pgadmin-data",
        "section": "Module 1: Docker and Terraform",
        "question": "pgAdmin - How to Persist pgAdmin Configurations"
      },
      {
        "text": "This error occurs in connecting pgAdmin with Docker Postgres. In tutorial, in the pgAdmin server creation under Connection > Host name/address: pg-database is given and resulted in the above mentioned error when saved.\nSolution 1:\nVerify that both containers are connected to pg-network : docker network inspect pg-network\nIf Docker Postgres container is not connected, then connect it to pg-network: docker network connect pg-network postgresContainer_name\nRetry connection, and if error persist, instead of using pg-database under Connection > Host name/address: pg-database, Try using IP Address: Use the IP address of the postgresContainer_name container e.g.(172.19.0.3) in the pgAdmin configuration instead of the container name or pg-database.",
        "section": "Module 1: Docker and Terraform",
        "question": "pgAdmin - Unable to connect to server: [Errno -3] Try again"
      },
      {
        "text": "ImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named 'pysqlite2'\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\Anaconda\\Dlls\\\".\n\u2705I solved it by simply copying that .dll file from \\Anaconda3\\Library\\bin and put it under the path mentioned above. (if you are using anaconda)",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - ModuleNotFoundError: No module named 'pysqlite2'"
      },
      {
        "text": "If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same steps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\n\u2705Solution: remove the cell \u201cdf=next(df_iter)\u201d that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\n\ud83d\udcd4Note: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - Ingestion with Jupyter notebook - missing 100000 records"
      },
      {
        "text": "Pandas can interpret \u201cstring\u201d column values as \u201cdatetime\u201d directly when reading the CSV file using \u201cpd.read_csv\u201d using the parameter \u201cparse_dates\u201d, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\npandas.read_csv \u2014 pandas 2.1.4 documentation (pydata.org)\nExample from week 1\nimport pandas as pd\ndf = pd.read_csv(\n'yellow_tripdata_2021-01.csv',\nnrows=100,\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\ndf.info()\nwhich will output\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 18 columns):\n#   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n0   VendorID               100 non-null    int64\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\n3   passenger_count        100 non-null    int64\n4   trip_distance          100 non-null    float64\n5   RatecodeID             100 non-null    int64\n6   store_and_fwd_flag     100 non-null    object\n7   PULocationID           100 non-null    int64\n8   DOLocationID           100 non-null    int64\n9   payment_type           100 non-null    int64\n10  fare_amount            100 non-null    float64\n11  extra                  100 non-null    float64\n12  mta_tax                100 non-null    float64\n13  tip_amount             100 non-null    float64\n14  tolls_amount           100 non-null    float64\n15  improvement_surcharge  100 non-null    float64\n16  total_amount           100 non-null    float64\n17  congestion_surcharge   100 non-null    float64\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\nmemory usage: 14.2+ KB",
        "section": "Module 1: Docker and Terraform",
        "question": "iPython - Pandas parsing dates with \u2018read_csv\u2019"
      },
      {
        "text": "os.system(f\"curl -LO {url} -o {csv_name}\")",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - Python cant ingest data from the github link provided using curl"
      },
      {
        "text": "When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\ndf = pd.read_csv('file.csv.gz'\n, compression='gzip'\n, low_memory=False\n)",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - Pandas can read *.csv.gzip"
      },
      {
        "text": "Contrary to panda\u2019s read_csv method there\u2019s no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\nimport pyarrow.parquet as pq\noutput_name = \u201chttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\u201d\nparquet_file = pq.ParquetFile(output_name)\nparquet_size = parquet_file.metadata.num_rows\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\ntable_name=\u201dyellow_taxi_schema\u201d\n# Clear table if exists\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n# default (and max) batch size\nindex = 65536\nfor i in parquet_file.iter_batches(use_threads=True):\nt_start = time()\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\nindex += 65536\nt_end = time()\nprint(f'\\t- it took %.1f seconds' % (t_end - t_start))",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - How to iterate through and ingest parquet file"
      },
      {
        "text": "Error raised during the jupyter notebook\u2019s cell execution:\nfrom sqlalchemy import create_engine.\nSolution: Version of Python module \u201ctyping_extensions\u201d >= 4.6.0. Can be updated by Conda or pip.",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'."
      },
      {
        "text": "create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\nSolution:\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - SQLALchemy - TypeError 'module' object is not callable"
      },
      {
        "text": "Error raised during the jupyter notebook\u2019s cell execution:\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\nSolution: Need to install Python module \u201cpsycopg2\u201d. Can be installed by Conda or pip.",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'."
      },
      {
        "text": "Error raised during the jupyter notebook\u2019s cell execution:\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)\nSolution: We had a scenario of a virtualenv (created by Pycharm) being run on top of another virtual env (on conda). Solution was:\nto get rid of the .venv\ncreate a brand new virtualenv with conda conda create -n pyingest python=3.12\ninstall the required dependencies pip install pandas sqlalchemy psycopg2-binary jupyterlab\nAnd re-execute the code.\nFor psycopg2, the connection string should be:\npostgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\nReference - Kayla Tinker 1/14/25",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - SQLAlchemy - NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:postgresql.psycopg"
      },
      {
        "text": "First, check SQLAlchemy and Pandas version. Make sure they are both up-to-date. Upgrade them using pip/conda if needed.\nThen, try to wrap the query using text:\nfrom sqlalchemy import text\nquery = text(\"\"\"SELECT * FROM tbl\"\"\") df = pd.read_sql_query(query, conn)",
        "section": "Module 1: Docker and Terraform",
        "question": "Python - SQLAlchemy - read_sql_query() throws \"'OptionEngine' object has no attribute 'execute'\""
      },
      {
        "text": "I had my contig file set up from the first instance of my VM setup, but once I shut the VM down and restarted it later, the config no longer worked. This was because the IP address of my VM had changed, so my config was out of date. I didn\u2019t want to change my config file every time so I wondered if there was a solution \u2013 there is!\nYou can make a static IP address. The default is ephemeral, which changes every time you start/stop. This way, you can keep the same ip address in your config file every time you start/stop the VM.\nSet up a static IP in VPC Network > IP addresses. Make sure you attach it to your VM instance to avoid extra fees. You are only charged for a static IP if it is not assigned to a specific virtual machine. There is also pretty good documentation for this on gcp.",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - Static vs Ephemeral IP / Setting up static IP for VM"
      },
      {
        "text": "Unable to add Google Cloud SDK PATH to Windows\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\tools\\google-cloud-sdk\\bin\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\nOne way of doing that is to use conda: \u2018If you are not already using it\nDownload the Anaconda Navigator\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\nMake sure to check the following boxes while you install Gitbash\nAdd a GitBash to Windows Terminal\nUse Git and optional Unix tools from the command prompt\nNow open up git bash and type conda init bash This should modify your bash profile\nAdditionally, you might want to use Gitbash as your default terminal.\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - Unable to add Google Cloud SDK PATH to Windows"
      },
      {
        "text": "It asked me to create a project. This should be done from the cloud console. So maybe we don\u2019t need this FAQ.\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{'vtpep_pickup_datetimeary': 'Origin, X-Origin, Referer', 'content-type': 'application/json; charset=UTF-8', 'content-encoding': 'gzip', 'date': 'Mon, 24 Jan 2022 19:29:12 GMT', 'server': 'ESF', 'cache-control': 'private', 'x-xss-protection': '0', 'x-frame-options': 'SAMEORIGIN', 'x-content-type-options': 'nosniff', 'server-timing': 'gfet4t7; dur=189', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'transfer-encoding': 'chunked', 'status': 409}>, content <{\n\"error\": {\n\"code\": 409,\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\n\"status\": \"ALREADY_EXISTS\"\n}\n}\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it's not surprising it's already taken.",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - Project creation failed: HttpError accessing \u2026 Requested entity alreadytpep_pickup_datetime exists"
      },
      {
        "text": "If you receive the error: \u201cError 403: The project to be billed is associated with an absent billing account., accountDisabled\u201d It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\nAnother possibility is that you have not linked your billing account to your current project",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - The project to be billed is associated with an absent billing account"
      },
      {
        "text": "GCP Account Suspension Inquiry\nIf Google refuses your credit/debit card, try another - I\u2019ve got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\nUnfortunately, there\u2019s small hope that support will help.\nIt seems that Pyypl web-card should work too.\nny-rides.json",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - OR-CBAT-15 ERROR Google cloud free trial account"
      },
      {
        "text": "The ny-rides.json is your private file in Google Cloud Platform (GCP). \n\nAnd here\u2019s the way to find it:\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the \u201cKEYS\u201d tab where you can add key as a JSON as its key type",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - Where can I find the \u201cny-rides.json\u201d file?"
      },
      {
        "text": "You likely didn\u2019t enable the Compute Engine API.",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - \u201cFailed to load\u201d when accessing Compute Engine\u2019s metadata section (e.g., to add a SSH key)"
      },
      {
        "text": "In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - Do I need to delete my instance in Google Cloud?"
      },
      {
        "text": "Initially, I could not ssh into my VM from my windows laptop. I thought at first it was because I did not follow along exactly with the tutorial. Instead of generating ssh key using the MINGW/git bash with the linux style command, I did it in command-prompt using the windows style command. I kept getting a public key error.\nPermanent solution:\nIt turns out it wasn\u2019t an issue with the keygen at all! It was silly, as with most \u201cbugs.\u201d I had given my ssh key a different username than what showed in my VM (my google account username). So I had been trying to log in with googleacctuser@[ipaddr] instead of mySSHuser@[ipaddr]. I figured this out by retracing my steps to double check that I had set up an ssh key in GCP console, where it showed the user and ssh key. I quickly changed the username to the correct one (googleacctuser) in my config file and it works!\nNow, the catch is that I\u2019ve created two users! I made all the installations, permissions granting, etc. on googleacctuser and it\u2019s not accessible from liv. So there\u2019s a couple avenues I could take, but since I set up googleacctuser and I don\u2019t need mySSHuser, I\u2019m just going to change the username at the end of the ssh key to mySSHuser from mySSHuser on local (open up public gcp ssh file in texteditor), and re-paste that into the GCP console. Then update the config file and use mySSHuser to log in.\nThen delete mySSHuser account in the VM terminal just to keep things clean. (i skipped this because i am now a bit attached :) )\nTemporary solution: Before i figured out my issue, I took a shortcut by ssh\u2019ing into the VM in the browser (see screenshot), which actually worked nicely for a while. But eventually I wanted to use VScode.",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - ssh public key error - multiple users / usernames"
      },
      {
        "text": "If you are progressing through the course and find that your VM is starting to become slow you can run the following commands to inspect and detect areas where you can improve this.\nNB: What size VM should I start with? I started with 30GB but this wasn\u2019t enough, I had to restart the project with a 60GB machine so I\u2019d recommend choosing the 60GB version.\nCommands to inspect the health of your VM:\nSystem Resource Usage:\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\nfree -h: Displays information about system memory usage and availability.\ndf -h: Shows disk space usage of file systems.\ndu -h <directory>: Displays disk usage of a specific directory.\nRunning Processes:\nps aux: Lists all running processes along with detailed information.\nNetwork:\nifconfig or ip addr show: Shows network interface configuration.\nnetstat -tuln: Displays active network connections and listening ports.\nHardware Information:\nlscpu: Displays CPU information.\nlsblk: Lists block devices (disks and partitions).\nlshw: Lists hardware configuration.\nUser and Permissions:\nwho: Shows who is logged on and their activities.\nw: Displays information about currently logged-in users and their processes.\nPackage Management:\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP Virtual Machine (VM) Size, Slow, Clean Up"
      },
      {
        "text": "if you\u2019ve got the error\n\u2502 Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\nbut you\u2019ve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!",
        "section": "Module 1: Docker and Terraform",
        "question": "Billing account has not been enabled for this project. But you\u2019ve done it indeed!"
      },
      {
        "text": "for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\nWARNING:\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\nFor me:\nI reinstalled the sdk using unzip file \u201cinstall.bat\u201d,\nafter successfully checking gcloud version,\nrun gcloud init to set up project before\nyou run gcloud auth application-default login\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP - Windows Google Cloud SDK install issue:gcp"
      },
      {
        "text": "Click on your VM\nCreate an image of your VM\nOn the page of the image, tell GCP to create a new VM instance via the image\nOn the settings page, change the location",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP VM - I cannot get my Virtual Machine to start because GCP has no resources."
      },
      {
        "text": "The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP VM - Is it necessary to use a GCP VM? When is it useful?"
      },
      {
        "text": "I am trying to create a directory but it won't let me do it\nUser1@DESKTOP-PD6UM8A MINGW64 /\n$ mkdir .ssh\nmkdir: cannot create directory \u2018.ssh\u2019: Permission denied\nYou should do it in your home directory. Should be your home (~)\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\nLink to Video 1.4.1",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP VM - mkdir: cannot create directory \u2018.ssh\u2019: Permission denied"
      },
      {
        "text": "Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\nssh\nsudo chown -R <user> <path to your directory>",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP VM - Error while saving the file in VM via VS Code"
      },
      {
        "text": "Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\n\u2705Answer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\ncd ~/.ssh\ncode config \u2190 this opens the config file in VSCode",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP VM - VM connection request timeout"
      },
      {
        "text": "(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\nGo to section Automation\nAdd Startup script\n```\n#!/bin/bash\nsudo ufw allow ssh\n```\nStop and Start VM.",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP VM -  connect to host port 22 no route to host"
      },
      {
        "text": "You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\nExecute the same command but with ports 8080 and 8888.\nNow you can access pgAdmin on local machine in browser typing localhost:8080\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP VM - Port forwarding from GCP without using VS Code"
      },
      {
        "text": "If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\nSolution : you should instead hover on the long link, and ctrl + click the long link\n\nClick configure Trusted Domains here\n\nPopup will appear, pick first or second entry\nNext time you gcloud auth, the login page should popup via default browser without issues",
        "section": "Module 1: Docker and Terraform",
        "question": "GCP gcloud + MS VS Code - gcloud auth hangs"
      },
      {
        "text": "It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Error: Failed to query available provider packages \u2502 Could not retrieve the list of available versions for provider hashicorp/google: could not query \u2502 provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, \u2502 please try again later"
      },
      {
        "text": "The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout"
      },
      {
        "text": "https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Install for WSL"
      },
      {
        "text": "https://github.com/hashicorp/terraform/issues/14513",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Error acquiring the state lock"
      },
      {
        "text": "When running\nterraform apply\non wsl2 I've got this error:\n\u2502 Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n\u2502 Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\nIt happens because there may be time desync on your machine which affects computing JWT\nTo fix this, run the command\nsudo hwclock -s\nwhich fixes your system time.",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL."
      },
      {
        "text": "\u2502 Error: googleapi: Error 403: Access denied., forbidden\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Error 403 : Access denied"
      },
      {
        "text": "One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?"
      },
      {
        "text": "Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?"
      },
      {
        "text": "You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and then run the command.",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g"
      },
      {
        "text": "The error:\nError: googleapi: Error 403: Access denied., forbidden\n\u2502\nand\n\u2502 Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\nFor this solution make sure to run:\necho $GOOGLE_APPLICATION_CREDENTIALS\necho $?\nSolution:\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes"
      },
      {
        "text": "The error:\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\nThe solution:\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.",
        "section": "Module 1: Docker and Terraform",
        "question": "stoTerraform - Error creating Bucket: googleapi: Error 403: Permission denied to access \u2018storage.buckets.create\u2019"
      },
      {
        "text": "To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.\nprovider \"google\" {\nproject     = var.projectId\ncredentials = file(\"${var.gcpkey}\")\n#region      = var.region\nzone = var.zone\n}",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform google provider requires credentials."
      },
      {
        "text": "When running `terraform destroy`, the following error can occur:\n```\nDo you really want to destroy all resources?\nTerraform will destroy all your managed infrastructure, as shown above.\nThere is no undo. Only 'yes' will be accepted to confirm.\nEnter a value: yes\ngoogle_bigquery_dataset.homework_dataset: Destroying... [id=projects/terraform-demo-449214/datasets/homework_dataset]\n\u2577\n\u2502 Error: Error when reading or editing Dataset: googleapi: Error 400: Dataset terraform-demo-449214:homework_dataset is still in use, resourceInUse\n```\nThis is because the dataset is still in use by a table. To delete the dataset, we need to set the `delete_contents_on_destroy` property to `true` in the `main.tf` file.",
        "section": "Module 1: Docker and Terraform",
        "question": "Terraform Teardown of BigQuery Dataset"
      },
      {
        "text": "For the HW1 I encountered this issue. The solution is\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria Zone';\nI think columns which start with uppercase need to go between \u201cColumn\u201d. I ran into a lot of issues like this and \u201c \u201d made it work out.\nAddition to the above point, for me, there is no \u2018Astoria Zone\u2019, only \u2018Astoria\u2019 is existing in the dataset.\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria\u2019;",
        "section": "Module 1: Docker and Terraform",
        "question": "SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist"
      },
      {
        "text": "It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\ndf = pd.read_csv(\u2018taxi+_zone_lookup.csv\u2019)\nAdd the row:\ndf.columns = df.columns.str.lower()",
        "section": "Module 1: Docker and Terraform",
        "question": "SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist"
      },
      {
        "text": "Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")",
        "section": "Module 1: Docker and Terraform",
        "question": "CURL - curl: (6) Could not resolve host: output.csv"
      },
      {
        "text": "To resolve this, ensure that your config file is in C/User/Username/.ssh/config",
        "section": "Module 1: Docker and Terraform",
        "question": "SSH Error: ssh: Could not resolve hostname linux: Name or service not known"
      },
      {
        "text": "If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anaconda\u2019s Python is not on the PATH.\nAdding it to the PATH is different for each operation system.\nFor Linux and MacOS:\nOpen a terminal.\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\nLocate your Anaconda installation. The default path is usually `C:\\Users\\[YourUsername]\\Anaconda3`.\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\Users\\[YourUsername]\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\nRefresh your environment with the command: `source ~/.bashrc`.\nFor Windows (without Git Bash):\nRight-click on 'This PC' or 'My Computer' and select 'Properties'.\nClick on 'Advanced system settings'.\nIn the System Properties window, click on 'Environment Variables'.\nIn the Environment Variables window, select the 'Path' variable in the 'System variables' section and click 'Edit'.\nIn the Edit Environment Variable window, click 'New' and add the path to your Anaconda installation (typically `C:\\Users\\[YourUsername]\\Anaconda3` and C:\\Users\\[YourUsername]\\Anaconda3\\Scripts`).\nClick 'OK' in all windows to apply the changes.\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.",
        "section": "Module 1: Docker and Terraform",
        "question": "'pip' is not recognized as an internal or external command, operable program or batch file."
      },
      {
        "text": "Resolution: You need to stop the services which is using the port.\nRun the following:\n```\nsudo kill -9 `sudo lsof -t -i:<port>`\n```\n<port> being 8080 in this case. This will free up the port for use.\n~ Abhijit Chakraborty\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\nResolution: In my case, I had to stop docker and restart the service to get it running properly\nUse the following command:\n```\nsudo systemctl restart docker.socket docker.service\n```\n~ Abhijit Chakraborty\nError: docker build Error checking context: 'can't stat '<path-to-file>'\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\n2. If the above does not work, then put the dockerfile and corresponding script, `\t1.py` in our case to a subfolder. and run `docker build ...`\nfrom inside the new folder.\n~ Abhijit Chakraborty\nDocker-Compose - it is illegal to have any blank spaces between the environment argument in docker-compose.yml\n\nThe following ways of configuring it will not work:\n- PGADMIN_DEFAULT_EMAIL = admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD = root\n\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\n- PGADMIN_DEFAULT_PASSWORD=root",
        "section": "Module 1: Docker and Terraform",
        "question": "Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use"
      },
      {
        "text": "To get a pip-friendly requirements.txt file file from Anaconda use\nconda install pip then `pip list \u2013format=freeze > requirements.txt`.\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.",
        "section": "Module 1: Docker and Terraform",
        "question": "Anaconda to PIP"
      },
      {
        "text": "Install and open Jupyter Notebook\npip install jupyter\npython3 -m notebook\nNotebook convert\npip install nbconvert --upgrade\nPython3 -m jupyter nbconvert --to=script upload-data.ipynb",
        "section": "Module 1: Docker and Terraform",
        "question": "Jupyter - Install, open Jupyter and convert Jupyter notebook to Python script"
      },
      {
        "text": "If you keep getting errors with nbconvert after: jupyter nbconvert --to script <your_notebook.ipynb>\nyou could try to convert your Jupyter notebook via another tool called jupytext\nJupytext is another excellent tool for converting Jupyter Notebooks to Python scripts, which works very similar to nbconvert\nInstall jupytext\npip install jupytext\nConvert your Notebook to a Python script\njupytext --to py <your_notebook.ipynb>",
        "section": "Module 1: Docker and Terraform",
        "question": "Alternative way to convert Jupyter notebook to Python script  (via jupytext)"
      },
      {
        "text": "If you are using Windows, try copying the .ssh folder from the Linux file path to Windows. In the config file, use\nIdentityFile C:\\Users\\<username>\\.ssh\\gcp\nInstead of IdentityFile ~/.ssh/gcp\nAnother reason: The private key in its file at the local path C:\\Users\\<username>\\.ssh\\gcp needs an extra line in the end:",
        "section": "Module 2: Workflow Orchestration",
        "question": "SSH error in VS Code - \u201cCould not establish connection to \"de-zoomcamp\": Permission denied (publickey).\u201d"
      },
      {
        "text": "Prefect Airflow Mage",
        "section": "Module 2: Workflow Orchestration",
        "question": "Where are the FAQ questions from the previous cohorts for the orchestration module?"
      },
      {
        "text": "Start docker in linux with docker run --pull=always --rm -it -p 8080:8080 --user=root \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /tmp:/tmp kestra/kestra:latest server local\nOnce run you can login to dashboard at localhost:8080\nFor windows instructions see the Kestra github here https://github.com/kestra-io/kestra\nHere sample docker-compose for kestra\n\nservices:\nkestra:\nbuild: .\nimage: kestra/kestra:latest\ncontainer_name: kestra\nuser: \"0:0\"\nenvironment:\nDOCKER_HOST: tcp://host.docker.internal:2375  # for Windows\nKESTRA_CONFIGURATION: |\nkestra:\nrepository:\ntype: h2\nqueue:\ntype: memory\nstorage:\ntype: local\nlocal:\nbasePath: /app/storage\ntasks:\ntmp-dir:\npath: /app/tmp\nplugins:\nrepositories:\n- id: central\ntype: maven\nurl: https://repo.maven.apache.org/maven2\ndefinitions:\n- io.kestra.plugin.core:core:latest\n- io.kestra.plugin.scripts:python:1.3.4\n- io.kestra.plugin.http:http:latest\nKESTRA_TASKS_TMP_DIR_PATH: /app/tmp\nports:\n- \"8080:8080\"\nvolumes:\n- //var/run/docker.sock:/var/run/docker.sock  # Windows path\n- /yourpath/.dbt:/app/.dbt\n- /yourpath/kestra/plugins:/app/plugins\n- /yourpath/kestra/workflows:/app/workflows\n- /yourpath/kestra/storage:/app/storage\n- /yourpath//kestra/tmp:/app/tmp\n- /yourpath//dbt_prj:/app/workflows/dbt_project\n- /yourpath//my-creds.json:/app/.dbt/my-creds.json\ncommand: server standalone",
        "section": "Module 2: Workflow Orchestration",
        "question": "How do I launch Kestra?"
      },
      {
        "text": "Description:\nRunning the command below in Bash with Docker running and WSL2 installed. Even running Bash as admin won\u2019t work\n```:\n$ docker run --pull=always --rm -it -p 8080:8080 --user=root -v\n/var/run/docker.sock:/var/run/docker.sock -v /tmp:/tmp kestra/kestra:latest server local\nlatest: Pulling from kestra/kestra\nDigest: sha256:af02a309ccbb52c23ad1f1551a1a6db8cf0523cf7aac7c7eb878d7925bc85a62\nStatus: Image is up to date for kestra/kestra:latest\ndocker: Error response from daemon: mkdir C:\\\\Program Files\\\\Git\\\\var: Access is denied.\nSee 'docker run --help'.\n```\nThe error mentioned above will appear and localhost wont shows the Kestra UI, the solution is to run Command Prompt as admin with the following command:\n```\ndocker run --pull=always --rm -it -p 8080:8080 --user=root ^\n-v \"/var/run/docker.sock:/var/run/docker.sock\" ^\n-v \"C:/Temp:/tmp\" kestra/kestra:latest server local\n```\nThis works flawlessly and localhost shows Kestra UI as usual.",
        "section": "Module 2: Workflow Orchestration",
        "question": "docker: Error response from daemon: mkdir C:\\Program Files\\Git\\var: Access is denied."
      },
      {
        "text": "Error: org.postgresql.util.psqlexception the connection attempt failed due to this config on kestra flow -> jdbc:postgresql://host.docker.internal:5432/postgres-zoomcamp\nSolution: Just replace host.docker.internal for the name of the service for postgres in docker compose.\n\u2014---\nI also encountered a similar error as above, slightly different error message:\norg.postgresql.util.PSQLException: The connection attempt failed. 2025-01-29 22:52:22.281 green_create_table The connection attempt failed. host.docker.internal\nI could download my dataset by executing my flow, but when i wanted to ingest it to the pg database, the connection to pg failed.\nThe main issue was that the pg database url is different for linux than the url in the tutorial. Namely, instead of host.docker.internal, linux users will use the service or container name for postgres, which for me was just postgres.\nurl: jdbc:postgresql://postgres:5432/kestra\nVoila. Also, make sure to double check your pg database name. Mine was kestra in the docker compose file, whereas in the tutorial they had named it postgres-zoomcamp.",
        "section": "Module 2: Workflow Orchestration",
        "question": "Error when running Kestra flow connecting to postgres."
      },
      {
        "text": "I encountered an error where the localhost url for pgadmin would just hang up (i chose localhost:8080 for my pgadmin, and made kestra localhost:8090, personal preference).\nThe associated error was:\nAnd the resolution involved changing the ownership of my local directory to the user \u201c5050\u201d which is pgadmin. Unlike postgres, pgadmin requires you to give it permission. Apparently the postgres user inside the docker container creates the postgres volume/dir, so it has permission`s already.\nThis is a good source: https://stackoverflow.com/questions/64781245/permission-denied-var-lib-pgadmin-sessions-in-dockerG",
        "section": "Module 2: Workflow Orchestration",
        "question": "Adding a pgadmin service with volume mounting to the docker-compose:"
      },
      {
        "text": "Running out of storage while trying to backfill. I realized my GCP VM only has 30GB of storage and I was eating it up! Couple things I did/would suggest:\nClean up your GCP VM drive. You can use this command to see what is taking up the most space:  $ sudo du -sh *\n(~1gb) For me, Anaconda installer was taking up lots of space - you can delete that immediately because I already installed anaconda. I don\u2019t need the installer anymore.\nRm -rf  <anacondainstaller_fpath>\n(~3gb) Anaconda also takes up lots of space. You can\u2019t delete it all if you want to run python, but you can clean it up significantly. I don\u2019t care much about libs, etc. because I can build them in a docker container! Command is $ conda clean --all -y\nYou can clean up your kestra files with a purge flow. Here is the generic one: https://kestra.io/docs/administrator-guide/purge\nI personally wanted to do it immediately, not at end of month, so I made end date just now and got rid of the trigger block. You can also specify if you want to removed FAILED state executions, but I chose not to: endDate: \"{{ now() }}\"\nYou can clean up your pg database by manually deleting tables in pgadmin. Or possibly set up a workflow for it in kestra, but it was easy enough to manually delete.",
        "section": "Module 2: Workflow Orchestration",
        "question": "Running out of storage when using kestra with postgres on GCP VM"
      },
      {
        "text": "Do not directly add the content of service account credential json in Kestra script, especially if we are pushing to Github. Follow the instruction to add the service account as a secret Configure Google Service Account.\nWhen we need to use it in Kestra, we can pull it through {{ secret('GCP_SERVICE_ACCOUNT') }}\nIn the pluginDefaults.",
        "section": "Module 2: Workflow Orchestration",
        "question": "How can Kestra access service account credential?"
      },
      {
        "text": "When following the youtube lesson and then running the gcp_setup flow, I get the following error:\nI tried manually creating the bucket in the GCP console, but this showed me that the bucket already existed. So I came up with another name for the bucket and it worked.\nThe GCP bucket name has to be unique globally across all buckets, even if those are not your buckets, because the bucket will be accessible by URL.",
        "section": "Module 2: Workflow Orchestration",
        "question": "Storage Bucket Permission Denied Error when running the gcp_setup flow"
      },
      {
        "text": "When following the youtube lesson and then running the gcp_setup flow,  it works until the create_bq_dataset task, where I got the following error:\nWhile not very apparent from the error message, we are not suppose to use a dash in the dataset name, so I changed the dataset name to \u201cde_zoomcamp\u201d and it worked.",
        "section": "Module 2: Workflow Orchestration",
        "question": "Invalid dataset ID Error Error when running the gcp_setup flow"
      },
      {
        "text": "Several authentication methods are available;\nThese are some of the most straightforward approaches.\nMethod 1:\nUpdate your docker-compose.yml file as follows:\nMethod 2:\nStep 1: Store the Service Account as a Secret\nRun this command, specifying the correct path to your service-account.json file and .env_encoded:\nModify docker-compose.yml to include the encoded secrets:\nStep 2: Configure Kestra Plugin Defaults\nThis ensures all GCP tasks use the secret automatically:\nStep 3: Verify it\u2019s working in a testing GCP workflow\nAdditional - QA\nQuestion: How do I update the Service Account key?\nAnswer: Generate a new key, re-run the Base64 command, and restart Kestra.\nQuestion: Why use secrets instead of embedding the JSON key in the task?\nAnswer: Secrets prevent credential exposure and make workflows easier to manage.\nQuestion: Can I apply this method to other GCP tasks?\nAnswer: Yes, all GCP plugins will automatically inherit the secret.",
        "section": "Module 2: Workflow Orchestration",
        "question": "How do I properly authenticate a Google Cloud Service Account in Kestra?"
      },
      {
        "text": "\u26a0\ufe0f Yes, you should definitely include the .env_encoded file in your .gitignore file. Here's why:\nSecurity: The .env_encoded file contains sensitive information, namely the base64 encoded version of your GCP Service Account key. Even though it's encoded, it's not secure to share this in a public repository as anyone can decode it back to the original JSON.\nBest Practices: It's a common practice to not commit environment files or any files containing secrets to version control systems like Git. This prevents accidental exposure of sensitive data.\n\u26a0\ufe0f How to do it:\n# Add this line to your .gitignore file\n.env_encoded\n\u26a0\ufe0f More on Security:\nBase64 encoding is easily reversible. Base64 is an encoding scheme, not an encryption method. It's designed to encode binary data into ASCII characters that can be safely transmitted over systems that are designed to deal with text. Here's why it's not secure for protecting sensitive information:\nReversibility: Base64 encoding simply translates binary data into a text string using a specific set of 64 characters. Decoding it back to the original data is straightforward and doesn't require any secret key or password.\nPublic Availability of Tools: Numerous online tools, software libraries, and command-line utilities exist that can decode base64 with just a few clicks or commands.\nNo Security: Since base64 encoding does not change or hide the actual content of the data, anyone with access to the encoded string can decode it back to the original data.",
        "section": "Module 2: Workflow Orchestration",
        "question": "Should I include my .env_encoded file in my .gitignore?"
      },
      {
        "text": "If you're using Linux, you might encounter Connection Refused errors when connecting to the Postgres DB from within Kestra. This is because host.docker.internal works differently on Linux.\nUsing the modified Docker Compose file in 02-workflow-orchestration readme troubleshooting tips Docker Compose Example, you can run both Kestra and its dedicated Postgres DB, as well as the Postgres DB for the exercises all together. You can access it within Kestra by referring to the container name postgres_zoomcamp instead of host.docker.internal in pluginDefaults.\nThe pluginDefaults exist in both 2_postgres_taxi_scheduled.yaml, 02_postgres_taxi.yaml, please modify as shown below.",
        "section": "Module 2: Workflow Orchestration",
        "question": "taskid: yellow_create_table The connection attempt failed. Host.docker.internal"
      },
      {
        "text": "This update corrects the Docker Compose configuration to resolve the error when using the alias `host.docker.internal` on Linux systems. Since this alias does not resolve natively on Linux, the following entry was added to the affected container:\nkestra:\nimage: kestra/kestra:latest\npull_policy: always\nuser: \"root\"\ncommand: server standalone\nvolumes:...\nenvironment:...\nports:...\ndepends_on:...\nextra_hosts:\n- \"host.docker.internal:host-gateway\"\nextra_hosts:\n- \"host.docker.internal:host-gateway\"\nWith this change, containers that need to access host services via `host.docker.internal` will be able to do so correctly. For inter-container communication within the same network, it is recommended to use the service name directly.",
        "section": "Module 2: Workflow Orchestration",
        "question": "Fix: Add extra_hosts for host.docker.internal on Linux"
      },
      {
        "text": "Adds the extraHosts configuration to the taskRunner in the dbt-build task to resolve the issue with host.docker.internal not being recognized on Linux.\ntaskRunner:\ntype: io.kestra.plugin.scripts.runner.docker.Docker\nextraHosts:\n- \"host.docker.internal:host-gateway\"",
        "section": "Module 2: Workflow Orchestration",
        "question": "Fix: Add extra_hosts for taskRunner in the dbt-build"
      },
      {
        "text": "If you plan on using Kestra with Google Cloud Platform, make sure you setup the GCP_CREDS that\u2019s gonna be used in the flows that has \u201cgcp\u201d on its name.\nTo set it, go to Namespaces, and then select \u201czoomcamp\u201d if you are using the same examples used in the lessons. Then in the \u201cKV Store\u201d tab create the new key as GCP_CREDS and set the type to JSON and paste the content of the .json file with credentials for the service account created.",
        "section": "Module 2: Workflow Orchestration",
        "question": "Kestra: Don\u2019t forget to set GCP_CREDS variable"
      },
      {
        "text": "It seems to be a bug. Current fix is to remove the timezone from triggers in the script. More on this bug is here.",
        "section": "Module 3: Data Warehousing",
        "question": "Kestra: Backfill showing getting executed but not getting results or showing up in executions:"
      },
      {
        "text": "A:\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\n2) Use python ZipFile package, which is included in all modern python distributions",
        "section": "Module 3: Data Warehousing",
        "question": "Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets"
      },
      {
        "text": "Make sure to use Nullable dataTypes, such as Int64 when appliable.",
        "section": "Module 3: Data Warehousing",
        "question": "GCS Bucket - error when writing data from web to GCS:"
      },
      {
        "text": "Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for '2019-05', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of '2019-01' through '2019-04', the same column is defined as FLOAT.parquet\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\npd.read_csv(\"path_or_url\").astype({\n\t\"col1_name\": \"datatype\",\t\n\t\"col2_name\": \"datatype\",\t\n\t...\t\t\t\t\t\n\t\"colN_name\": \"datatype\" \t\n})",
        "section": "Module 3: Data Warehousing",
        "question": "GCS Bucket - te table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet"
      },
      {
        "text": "If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\n\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\nEmphasising the \u2018/releases/download\u2019 part of the URL.",
        "section": "Module 3: Data Warehousing",
        "question": "GCS Bucket - Fix Error when importing FHV data to GCS"
      },
      {
        "text": "Krishna Anand",
        "section": "Module 3: Data Warehousing",
        "question": "GCS Bucket - Load Data From URL list in to GCP Bucket"
      },
      {
        "text": "Check the Schema\nYou might have a wrong formatting\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\nSee this Slack conversation for helpful tips",
        "section": "Module 3: Data Warehousing",
        "question": "GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?"
      },
      {
        "text": "Run the following command to check if \u201cBigQuery Command Line Tool\u201d is installed or not: gcloud components list\nYou can also use bq.cmd instead of bq to make it work.",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - \u201cbq: command not found\u201d"
      },
      {
        "text": "Use big queries carefully,\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\nUse big query in free credits and destroy all the datasets after creation.\nCheck your Billing daily! Especially if you\u2019ve spinned up a VM.",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Caution in using bigquery:no"
      },
      {
        "text": "Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):"
      },
      {
        "text": "Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>"
      },
      {
        "text": "By the way, this isn\u2019t a problem/solution, but a useful hint:\nPlease, remember to save your progress in BigQuery SQL Editor.\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Remember to save your queries"
      },
      {
        "text": "Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Can I use BigQuery for real-time analytics in this project?"
      },
      {
        "text": "could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage"
      },
      {
        "text": "Background:\n`pd.read_parquet`\n`pd.to_datetime`\n`pq.write_to_dataset`\nReference:\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\nSolution:\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\npq.write_to_dataset(\ntable,\nroot_path=root_path,\nfilesystem=gcs,\nuse_deprecated_int96_timestamps=True\n# Write timestamps to INT96 Parquet format\n)",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)"
      },
      {
        "text": "Solution:\nIf you\u2019re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nif 'data_exporter' not in globals():\nfrom mage_ai.data_preparation.decorators import data_exporter\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n@data_exporter\ndef export_data(data, *args, **kwargs):\ntable = pa.Table.from_pandas(data, preserve_index=False)\ngcs = pa.fs.GcsFileSystem()\npq.write_table(\ntable,\nwhere,\n# Convert integer columns in Epoch milliseconds\n# to Timestamp columns in microseconds ('us') so\n# they can be loaded into BigQuery with the right\n# data type\ncoerce_timestamps='us',\nfilesystem=gcs\n)\nSolution 2:\nIf you\u2019re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nschema = pa.schema([\n('vendor_id', pa.int64()),\n('lpep_pickup_datetime', pa.timestamp('ns')),\n('lpep_dropoff_datetime', pa.timestamp('ns')),\n('store_and_fwd_flag', pa.string()),\n('ratecode_id', pa.int64()),\n('pu_location_id', pa.int64()),\n('do_location_id', pa.int64()),\n('passenger_count', pa.int64()),\n('trip_distance', pa.float64()),\n('fare_amount', pa.float64()),\n('extra', pa.float64()),\n('mta_tax', pa.float64()),\n('tip_amount', pa.float64()),\n('tolls_amount', pa.float64()),\n('improvement_surcharge', pa.float64()),\n('total_amount', pa.float64()),\n('payment_type', pa.int64()),\n('trip_type', pa.int64()),\n('congestion_surcharge', pa.float64()),\n('lpep_pickup_month', pa.int64())\n])\ntable = pa.Table.from_pandas(data, schema=schema)",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery"
      },
      {
        "text": "Reference:\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\nSolution:\nfrom google.cloud import bigquery\n# Set table_id to the ID of the table to create\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\n# Construct a BigQuery client object\nclient = bigquery.Client()\n# Set the external source format of your table\nexternal_source_format = \"PARQUET\"\n# Set the source_uris to point to your data in Google Cloud\nsource_uris = [ f'gs://{bucket_name}/{object_key}/*']\n# Create ExternalConfig object with external source format\nexternal_config = bigquery.ExternalConfig(external_source_format)\n# Set source_uris that point to your data in Google Cloud\nexternal_config.source_uris = source_uris\nexternal_config.autodetect = True\ntable = bigquery.Table(table_id)\n# Set the external data configuration of the table\ntable.external_data_configuration = external_config\ntable = client.create_table(table)  # Make an API request.\nprint(f'Created table with external source: {table_id}')\nprint(f'Format: {table.external_data_configuration.source_format}')",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Create External Table using Python"
      },
      {
        "text": "Reference:\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\nSolution:\nCombine with \u201cCreate External Table using Python\u201d, use it before \u201cclient.create_table\u201d function.\ndef tableExists(tableID, client):\n\"\"\"\nCheck if a table already exists using the tableID.\nreturn : (Boolean)\n\"\"\"\ntry:\ntable = client.get_table(tableID)\nreturn True\nexcept Exception as e: # NotFound:\nreturn False",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Check BigQuery Table Exist And Delete"
      },
      {
        "text": "To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Error: Missing close double quote (\") character"
      },
      {
        "text": "Solution: This problem arises if your gcs and bigquery storage is in different regions.\nOne potential way to solve it:\nGo to your google cloud bucket and check the region in field named \u201cLocation\u201d\nNow in bigquery, click on three dot icon near your project name and select create dataset.\nIn region filed choose the same regions as you saw in your google cloud bucket",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US"
      },
      {
        "text": "There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\nimport tempfile\nimport requests\nimport logging\nfrom google.cloud import bigquery\ndef hello_world(request):\n# table_id = <project_id.dataset_id.table_id>\ntable_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\n# Create a new BigQuery client\nclient = bigquery.Client()\nfor month in range(4, 13):\n# Define the schema for the data in the CSV.gz files\nurl = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\n# Download the CSV.gz file from Github\nresponse = requests.get(url)\n# Create new table if loading first month data else append\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\njob_config = bigquery.LoadJobConfig(\nschema=[\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n],\nskip_leading_rows=1,\nwrite_disposition=write_disposition_string,\nautodetect=True,\nsource_format=\"CSV\",\n)\n# Load the data into BigQuery\n# Create a temporary file to prevent the exception- AttributeError: 'bytes' object has no attribute 'tell'\"\nwith tempfile.NamedTemporaryFile() as f:\nf.write(response.content)\nf.seek(0)\njob = client.load_table_from_file(\nf,\ntable_id,\nlocation=\"US\",\njob_config=job_config,\n)\njob.result()\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\nreturn 'Data loaded into table {}.'.format(table_id)",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:"
      },
      {
        "text": "You need to uncheck cache preferences in query settings",
        "section": "Module 3: Data Warehousing",
        "question": "GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))"
      },
      {
        "text": "Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\nSolution:\nFix the data type issue in data pipeline\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\nSomething like:\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ - How to handle type error from big query and parquet data?"
      },
      {
        "text": "Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project"
      },
      {
        "text": "No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\n[source]",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ - Does BigQuery support multiple columns partition?"
      },
      {
        "text": "Error Message:\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\nSolution:\nConvert the column to datetime first.\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ - DATE() Error in BigQuery"
      },
      {
        "text": "No need to convert as you can cluster by a TIMESTAMP column directly in BigQuery. BigQuery supports clustering on TIMESTAMP, DATE, DATETIME, STRING, INT64, and BOOL types.\nclustering sorts data based on the timestamp to optimize queries with filters like WHERE tpep_pickup_datetime BETWEEN ..., rather than creating discrete partitions.\nIf your goal is to improve performance for time-based queries, combining partitioning by DATE(event_time) and clustering by tpep_pickup_datetime is a good approach.",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ - When trying to cluster by DATE(tpep_pickup_datetime) it gives an error: Entries in the CLUSTER BY clause must be column names"
      },
      {
        "text": "Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\nExternal tables: They are not stored directly in big query tables but pulled in from a data lake such as Google Cloud Storage or S3.\nMaterialized table: Copy of this external table. Now the data is stored in the bigquery table and consumes the space.\nResources:\nhttps://cloud.google.com/bigquery/docs/external-tables\nhttps://cloud.google.com/bigquery/docs/tables-intro\nWhy does my partitioned table in BigQuery show as non-partitioned even though BigQuery says it's partitioned?\nIf your partitioned table in BigQuery shows as non-partitioned, it may be due to a delay in updating the table's details in the UI. The table is likely partitioned, but it may not show the updated information immediately.\nHere\u2019s what you can do:\nRefresh your BigQuery UI:\nIf you're already inspecting the table in the BigQuery UI, try refreshing the page after a few minutes to ensure the table details are updated correctly.\nOpen a new tab:\nAlternatively, try opening a new tab in BigQuery and inspect the table details again. This can sometimes help to load the most up-to-date information.\nBe patient:\nIn some cases, there might be a slight delay in reflecting changes, but the table is very likely partitioned.",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ - Native tables vs External tables in BigQuery?"
      },
      {
        "text": "Issue: Tried running command to export ML model from BQ to GCS from Week 3\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\nIt is failing on following error:\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\nI verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldn\u2019t find the solution yet.\nSolution:  Please enter correct project_id and gcs_bucket folder address. My gcs_bucket folder address is\ngs://dtc_data_lake_optimum-airfoil-376815/tip_model",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS"
      },
      {
        "text": "To solve this error mention the location = US when creating the dim_zones table\n{{ config(\nmaterialized='table',\nlocation='US'\n) }}\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql"
      },
      {
        "text": "Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\ndocker pull tensorflow/serving\nuse\ndocker pull emacski/tensorflow-serving\nThen\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\nThen run the curl command as written, and you should get a prediction.\nOr new since Oct 2024:\nBeta release of Docker VMM - the more performant alternative to Apple Virtualization Framework on macOS (requires Apple Silicon and macOS 12.5 or later). https://docs.docker.com/desktop/features/vmm/",
        "section": "error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.",
        "question": "GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture)."
      },
      {
        "text": "Try deleting data you\u2019ve saved to your VM locally during ETLs\nKill processes related to deleted files\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\nIf you delete any files related to Prefect, eliminate caching from your flow code\nExternal Table (data remains in GCS bucket)\nRegular Table (data is copied into BigQuery storage)\nExample of creating external table:\nCREATE OR REPLACE EXTERNAL TABLE `your_project.your_dataset.tablenamel`\nOPTIONS (\nformat = 'PARQUET',\nuris = ['gs://your-bucket-name/yellow_tripdata_2024-*.parquet']\n);\nExample of creating regular table from extermal table\nCREATE OR REPLACE TABLE `your_project.your_dataset.tablename`\nAS\nSELECT * FROM `your_project.your_dataset.yellow_taxi_external`;\nOr directly load data form GCS into a regular BigQuery table without creating an external table using:\nCREATE OR REPLACE TABLE `your_project.your_dataset.yellow_taxi_table`\nOPTIONS (\nformat = 'PARQUET'\n) AS\nSELECT * FROM `your_project.your_dataset.external_table_placeholder`\nFROM EXTERNAL_QUERY(\n'your_project.region-us.gcs_external',\n'SELECT * FROM `gs://your-bucket-name/yellow_tripdata_2024-*.parquet`'\n);",
        "section": "GCP BQ - External and regular table",
        "question": "VMs - What do I do if my VM runs out of space?"
      },
      {
        "text": "Yes, you can load your Parquet files directly into your GCP (Google Cloud Platform) Bucket first, then via BigQuery, you can create an external table of these Parquet files with a query statement like this:\n\nCREATE OR REPLACE EXTERNAL TABLE `module-3-data-warehouse.taxi_data.external_yellow_tripdata_2024`\nOPTIONS (\n  format = 'PARQUET',\n  uris = ['gs://module3-dez/yellow_tripdata_2024-*.parquet']\n);\n\nMake sure to adjust the sql statement to your own situation and directories.\nThe * symbol can be used as a wildcard, which you will need to target Parquet files of all the months of 2024.",
        "section": "GCP BQ - External and regular table",
        "question": "Can BigQuery work with parquet files directly?"
      },
      {
        "text": "Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)",
        "section": "GCP BQ - External and regular table",
        "question": "Homework - What does it mean \u201cStop with loading the files into a bucket.' Stop with loading the files into a bucket?\u201d"
      },
      {
        "text": "If for whatever reason you try to read parquets directly from nyc.gov\u2019s cloudfront into pandas, you might run into this error:\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\nCause:\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\npandas uses \u201ctimestamp[ns]\u201d (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\nFix:\nUse pyarrow to read it:\nimport pyarrow.parquet as pq df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\nHowever this results in weird timestamps for the offending record\nRead the datetime columns separately using pq.read_table\n\ntable = pq.read_table(\u2018taxi.parquet\u2019)\ndatetimes = [\u2018list of datetime column names\u2019]\ndf_dts = pd.DataFrame()\nfor col in datetimes:\ndf_dts[col] = pd.to_datetime(table .column(col), errors='coerce')\n\nThe `errors=\u2019coerce\u2019` parameter will convert the out of bounds timestamps into either the max or the min\nUse parquet.compute.filter to remove the offending rows\n\nimport pyarrow.compute as pc\ntable = pq.read_table(\"\u2018taxi.parquet\")\ndf = table.filter(\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n).to_pandas()",
        "section": "GCP BQ - External and regular table",
        "question": "Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error"
      },
      {
        "text": "This can help avoid schema issues in the homework. \nDownload files locally and use the \u2018upload files\u2019 button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.",
        "section": "GCP BQ - External and regular table",
        "question": "Homework - Uploading files to GCS via GUI"
      },
      {
        "text": "Ans: Take a careful look at the format of the dates in the question.",
        "section": "GCP BQ - External and regular table",
        "question": "Homework - Qn 5: The partitioned/clustered table isn\u2019t giving me the prediction I expected"
      },
      {
        "text": "Many people aren\u2019t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.",
        "section": "GCP BQ - External and regular table",
        "question": "Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?"
      },
      {
        "text": "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte\nSolution:\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\npd.read_csv(dataset_url, low_memory=False, encoding='latin1')\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding='utf-8')\nAlternative: use pd.read_parquet(url)",
        "section": "GCP BQ - External and regular table",
        "question": "Python - invalid start byte Error Message"
      },
      {
        "text": "A generator is a function in python that returns an iterator using the yield keyword.\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.",
        "section": "GCP BQ - External and regular table",
        "question": "Python - Generators in python"
      },
      {
        "text": "The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.",
        "section": "GCP BQ - External and regular table",
        "question": "Python - Easiest way to read multiple files at the same time?"
      },
      {
        "text": "Incorrect:\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\nCorrect:\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')",
        "section": "GCP BQ - External and regular table",
        "question": "Python - These won't work. You need to make sure you use Int64:"
      },
      {
        "text": "RuntimeWarning: As the c extension couldn't be imported, google-crc32c is using a pure python implementation that is significantly slower. If possible, please configure a c build environment and compile extention warnings.warn(_SLOW_CRC32C_WARNING, RuntimeWarning)\nFailed to upload ./yellow_tripdata_2024-01.parquet to GCS: Timeout of 120.0s exceeded, last exception: ('Connection aborted.', timeout('The write operation timed out'))\nFailed to upload ./yellow_tripdata_2024-03.parquet to GCS: Timeout of 120.0s exceeded, last exception: ('Connection aborted.', timeout('The write operation timed out'))\nIm facing two separate issues in my script:\ngoogle-crc32c Warning: The Google Cloud Storage library is using a slow Python implementation instead of the optimized C version.\nUpload Timeout Error: Your file uploads are timing out after 120 seconds.\n\u2705 Solution: Install the C-optimized google-crc32c\npip install --upgrade google-crc32c\n2. Fix Google Cloud Storage Upload Timeout\n\u2705 Solution 1: Increase Timeout\nblob.upload_from_filename(file_path, timeout=300) # Set timeout to 5 minutes",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Warning when run load_yellow_data python script"
      },
      {
        "text": "Please be aware that the demos are done using dbt cloud Developer licensing. Although Team license is available to you upon creation of dbt cloud account for 14 days, the interface won't fully match the demo-ed experience.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "dbt cloud Developer"
      },
      {
        "text": "(Lower left Corner after setting all connections to BQ and Github)\n14:48:39 Running dbt...\n14:48:39 Encountered an error:\nRuntime Error\nNo dbt_project.yml found at expected path /usr/src/develop/user-70471823426120/environment-70471823422561/repository-70471823410839/dbt_project.yml\nVerify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml\nSolution: Initialize a project through UI.\nImporting git repo of an existing dbt project:\nPlease read through these details for doing it: https://docs.getdbt.com/docs/cloud/git/import-a-project-by-git-url",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT-Config ERROR on CLOUD IDE: No dbt_project.yml found at expected path"
      },
      {
        "text": "Problem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT Cloud production error: prod dataset not available in location EU"
      },
      {
        "text": "You might get this error while trying to run dbt in production aftering following the instructions in the video \u2018DE Zoomcamp 4.4.1 - Deployment Using dbt Cloud (Alternative A\u2019):\nDatabase Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nNot found: Dataset module-4-analytics-eng:prod was not found in location europe-west10\n\nThis error is easily solved. There are two solutions  to solve this issue:\n\nSolution #1: Matching the dataset's data location with the source dataset\nSet your \u2018prod\u2019 dataset's data location to match the data location of your \u2018trips_data_all\u2019 dataset's data location (in BigQuery). Running dbt in production works for the  instructor, because her \u2018 prod\u2019 is in the same region as her source data. Since your \u2018trips_data_all\u2019 is in europe-west10 (or anything else besides US), your prod needs to be there too; not US (which is a default setting when dbt creates a dataset for you in BigQuery).\n\nSolution #2: Changing the dataset to <development dataset>\nGo into your dbt production environment settings:\n1. Go to: Deploy / Environments / Production (your production environment) / Settings\n2. Now look at the Deployment credentials. There is an input field here called Dataset. The input of \u2018prod\u2019 is likely in here.\n3. Replace \u2018prod\u2019 with the name of the Dataset that you worked with while in development (before moving to Production). This is the Dataset name inside your BigQuery where you successfully ran \u2018dbt debug\u2019 and \u2018dbt build\u2019 with.\n4. After saving, you are ready to rerun your Job!",
        "section": "Module 4: analytics engineering with dbt",
        "question": "How do I solve the Dbt Cloud error: prod was not found in location?"
      },
      {
        "text": "Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Setup - No development environment"
      },
      {
        "text": "Runtime Error\ndbt was unable to connect to the specified database.\nThe database returned the following error:\n>Database Error\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\nCheck your database credentials and try again. For more information, visit:\nhttps://docs.getdbt.com/docs/configure-your-profile\nSteps to resolve error in Google Cloud:\n1. Navigate to IAM & Admin and select IAM\n2. Click Grant Access if your newly created dbt service account isn't listed\n3. In New principals field, add your service account\n4. Select a Role and search for BigQuery Job User to add\n5. Go back to dbt cloud project setup and Test your connection\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Setup - Connecting dbt Cloud with BigQuery Error"
      },
      {
        "text": "Error: Failed to clone repository.\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/\u2026\nCloning into '/usr/src/develop/...\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\nIssue: You don\u2019t have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\n[your github username]/data-engineering-zoomcamp.git\nSolution 2: create a fresh repo for dbt-lessons. We\u2019d need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you don\u2019t have to create a subfolder for the dbt project files\nSolution 3: Use https link",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Setup - Failed to clone repository."
      },
      {
        "text": "Failed to start server. Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists.\nUse the deploy keys in dbt repo details to create a public key in your repo, the issue will be solved.\nSteps in details:\nFind dbt Cloud\u2019s SSH Key\nIn dbt Cloud, go to Settings > Account Settings > SSH Keys\nCopy the public SSH key displayed there.\nAdd It to GitHub\nGo to GitHub > Settings > SSH and GPG Keys\nClick \"New SSH Key\", name it \"dbt Cloud\", and paste the key.\nClick \"Add SSH Key\".\nTry Restarting dbt Cloud",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Errors when I start the server in dbt cloud: Failed to start server. Permission denied (publickey)"
      },
      {
        "text": "Solution:\nCheck if you\u2019re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\nNote from another user: I\u2019m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "dbt job - Triggered by pull requests is disabled prerequisites when I try to create a new Continuous Integration job in dbt cloud."
      },
      {
        "text": "Issue: If the DBT cloud IDE loading indefinitely then giving you this error\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Setup - Your IDE session was unable to start. Please contact support."
      },
      {
        "text": "Issue: If you don\u2019t define the column format while converting from csv to parquet Python will \u201cchoose\u201d based on the first rows.\n\u2705Solution: Defined the schema while running web_to_gcp.py pipeline.\nSebastian adapted the script:\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\nNeed a quick change to make the file work with gz files, added the following lines (and don\u2019t forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\nopen(file_name_gz, 'wb').write(r.content)\nos.system(f\"gzip -d {file_name_gz}\")\nos.system(f\"rm {file_name_init}.*\")",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - I am having problems with columns datatype while running DBT/BigQuery"
      },
      {
        "text": "Reason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\nThere are some possible fixes:\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\nSELECT * EXCEPT (ehail_fee) FROM\u2026\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\nModify Airflow dag to make the conversion and avoid the error.\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {'ehail_fee': 'float64'}))\nSame type of ERROR - parquet files with different data types - Fix it with pandas\nHere is another possibility that could be interesting:\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\npd.from_csv(..., dtype=type_dict)\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital \u2018I\u2019. The type_dict is a python dictionary mapping the column names to the dtypes.\nSources:\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nNullable integer data type \u2014 pandas 1.5.3 documentation",
        "section": "Module 4: analytics engineering with dbt",
        "question": "\u201cParquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64\u201d"
      },
      {
        "text": "If the provided URL isn\u2019t working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\nInstructions on how to download the CLI here: https://github.com/cli/cli\nCommands to use:\ngh auth login\ngh release list -R DataTalksClub/nyc-tlc-data\ngh release download yellow -R DataTalksClub/nyc-tlc-data\ngh release download green -R DataTalksClub/nyc-tlc-data\netc.\nNow you can upload the files to a GCS bucket using the GUI.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket"
      },
      {
        "text": "I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main \u00b7 DataTalksClub/data-engineering-bootcamp (github.com)\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\nThen I found another hack shared in the slack which was suggested by Victoria.\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\nPlease watch until the end as there is few schema changes required to be done",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Hack to load yellow and green trip data for 2019 and 2020"
      },
      {
        "text": "One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder \u2018.prefect/storage\u2019 and delete the logs now and then to avoid the problem.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "GCP VM - All of sudden ssh stopped working for my VM after my last restart"
      },
      {
        "text": "If you're encountering an error when trying to create a GCP free trial account, the issue isn\u2019t related to country restrictions, credit/debit card problems, or IP issues, it's a random problem with no clear logical reason behind it. Here\u2019s a simple workaround that worked for me:\nI asked a few friends in my country to try signing up for the free trial using their Gmail accounts and their debit/credit cards. One of them was able to successfully create the account, and I\u2019m temporarily using their Gmail to access the trial.\nIf you're still running into the issue, this method could help you bypass the problem!",
        "section": "Module 4: analytics engineering with dbt",
        "question": "GCP FREE TRIAL ACCOUNT ERROR"
      },
      {
        "text": "You can try to do this steps:",
        "section": "Module 4: analytics engineering with dbt",
        "question": "GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)"
      },
      {
        "text": "404 Not found: Dataset was not found in location US\n404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1\nR: Go to BigQuery, and check the location of BOTH\nThe source dataset (trips_data_all), and\nThe schema you\u2019re trying to write to (name should be \tdbt_<first initial><last name> (if you didn\u2019t change the default settings at the end when setting up your project))\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of \u2018location: US\u2019, specify the region, so \u2018location: US-east1\u2019. See this Github comment for more detail. Additionally please see this post of Sandy\nIn DBT cloud you can actually specify the location using the following steps:\nGPo to your profile page (top right drop-down --> profile)\nThen go to under Credentials --> Analytics (you may have customised this name)\nClick on Bigquery >\nHit Edit\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - When running your first dbt model, if it fails with an error:"
      },
      {
        "text": "Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\nFix:\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated"
      },
      {
        "text": "1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\n2. Add the related roles to the service account in use in GCS.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "When executing dbt run after fact_trips.sql has been created, the task failed with error:  \u201cAccess Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\u201d"
      },
      {
        "text": "You need to create packages.yml file in main project directory and add packages\u2019 meta data:\npackages:\n- package: dbt-labs/dbt_utils\nversion: 0.8.0\nAfter creating file run:\ndbt deps\nAnd hit enter.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "When You are getting error dbt_utils not found"
      },
      {
        "text": "Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stage\u2019s logs to expand and read errors messages or warnings.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists."
      },
      {
        "text": "Make sure you use:\ndbt run --var \u2018is_test_run: false\u2019 or\ndbt build --var \u2018is_test_run: false\u2019\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Build - Why do my Fact_trips only contain a few days of data?"
      },
      {
        "text": "Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Build - Why do my fact_trips only contain one month of data?"
      },
      {
        "text": "R: After the second SELECT, change this line:\ndate_trunc('month', pickup_datetime) as revenue_month,\nTo this line:\ndate_trunc(pickup_datetime, month) as revenue_month,\nMake sure that \u201cmonth\u201d isn\u2019t surrounded by quotes!",
        "section": "Module 4: analytics engineering with dbt",
        "question": "BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model."
      },
      {
        "text": "For this instead:\n{{ dbt_utils.generate_surrogate_key([ \n     field_a, \n     field_b, \n     field_c,\n     \u2026,\n     field_z\n]) }}\nadd a global variable in dbt_project.yml(...)",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Replace: \n{{ dbt_utils.surrogate_key([ \n     field_a, \n     field_b, \n     field_c,\n     \u2026,\n     field_z     \n]) }}"
      },
      {
        "text": "Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location",
        "section": "Module 4: analytics engineering with dbt",
        "question": "I changed location in dbt, but dbt run still gives me an error"
      },
      {
        "text": "Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\nAnswer: when you create the CI/CD job, under \u2018Compare Changes against an environment (Deferral) make sure that you select \u2018 No; do not defer to another environment\u2019 - otherwise dbt won\u2019t merge your dev models into production models; it will create a new environment called \u2018dbt_cloud_pr_number of pull request\u2019",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ."
      },
      {
        "text": "Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Why do we need the Staging dataset?"
      },
      {
        "text": "Try removing the \u201cnetwork: host\u201d line in docker-compose.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - Docs Served but Not Accessible via Browser"
      },
      {
        "text": "Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\nDelete your dataset in GBQ\nRebuild project: dbt build\nNewly built dataset should be in the correct location",
        "section": "Module 4: analytics engineering with dbt",
        "question": "BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6"
      },
      {
        "text": "Create a new branch to edit. More on this can be found here in the dbt docs.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Dbt+git - Main branch is \u201cread-only\u201d"
      },
      {
        "text": "Create a new branch for development, then you can merge it to the main branch\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the \u201cmain\u201d branch.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?"
      },
      {
        "text": "Error:\nTriggered by pull requests\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\nSolution: Contrary to the guide on DTC repo, don\u2019t use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat"
      },
      {
        "text": "If you\u2019re trying to configure CI with Github and on the job\u2019s options you can\u2019t see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\nDisconnect your current Github\u2019s configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and it\u2019s ready.\nGo to the Deploy > job configuration\u2019s page and go down until Triggers and now you can see the option Run on Pull Requests:",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github"
      },
      {
        "text": "If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)"
      },
      {
        "text": "> in macro test_accepted_values (tests/generic/builtin.sql)\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\nRemember that you have to add to dbt_project.yml the vars:\nvars:\npayment_type_values: [1, 2, 3, 4, 5, 6]",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Compilation Error in test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)  'NoneType' object is not iterable"
      },
      {
        "text": "You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\nWhat you\u2019d have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting \u2018\u2019, as the initial \u2018payment_type\u2019 data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\n{#\nThis macro returns the description of the payment_type\n#}\n{% macro get_payment_type_description(payment_type) -%}\ncase {{ payment_type }}\nwhen '1' then 'Credit card'\nwhen '2' then 'Cash'\nwhen '3' then 'No charge'\nwhen '4' then 'Dispute'\nwhen '5' then 'Unknown'\nwhen '6' then 'Voided trip'\nend\n{%- endmacro %}",
        "section": "Module 4: analytics engineering with dbt",
        "question": "dbt macro errors with get_payment_type_description(payment_type)"
      },
      {
        "text": "The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Troubleshooting in dbt:"
      },
      {
        "text": "It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named \u201cgenerate_schema_name.sql\u201d:\n{% macro generate_schema_name(custom_schema_name, node) -%}\n{%- set default_schema = target.schema -%}\n{%- if custom_schema_name is none -%}\n{{ default_schema }}\n{%- else -%}\n{{ custom_schema_name | trim }}\n{%- endif -%}\n{%- endmacro %}\nNow you can override default custom schema in \u201cdbt_project.yml\u201d:",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - Why changing the target schema to \u201cmarts\u201d actually creates a schema named \u201cdbt_marts\u201d instead?"
      },
      {
        "text": "There is a project setting which allows you to set `Project subdirectory` in dbt cloud:",
        "section": "Module 4: analytics engineering with dbt",
        "question": "How to set subdirectory of the github repository as the dbt project root"
      },
      {
        "text": "Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\nExample: select * from {{ source('staging',<your table name in the database>') }}",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found"
      },
      {
        "text": "Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your \u2018seeds\u2019 folder if the seed file is inside it.\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)"
      },
      {
        "text": "1. Go to your dbt cloud service account\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "When executing dbt run after using fhv_tripdata as an external table: you get \u201cAccess Denied: BigQuery BigQuery: Permission denied\u201d"
      },
      {
        "text": "Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\nSolution:\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\ndf.fillna(-999999, inplace=True)ingesting\ndf = df.convert_dtypes()\ndf = df.replace(-999999, None)",
        "section": "Module 4: analytics engineering with dbt",
        "question": "How to automatically infer the column data type (pandas missing value issues)?"
      },
      {
        "text": "Seed files loaded from directory with name \u2018seed\u2019, that\u2019s why you should rename dir with name \u2018data\u2019 to \u2018seed\u2019",
        "section": "Module 4: analytics engineering with dbt",
        "question": "When loading github repo raise exception that \u2018taxi_zone_lookup\u2019 not found"
      },
      {
        "text": "Check the .gitignore file and make sure you don\u2019t have *.csv in it\n\nDbt error 404 was not found in location\nMy specific error:\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\nMake sure all of your datasets have the correct region and not a generalised region:\nEurope-west6 as opposed to EU\n\nMatch this in dbt settings:\ndbt -> projects -> optional settings -> manually set location to match",
        "section": "Module 4: analytics engineering with dbt",
        "question": "\u2018taxi_zone_lookup\u2019 not found"
      },
      {
        "text": "The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\nOPTIONS (\nformat = 'CSV',\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\n);\nAs an example. You should no longer have any data type issues for week 4.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Data type errors when ingesting with parquet files"
      },
      {
        "text": "This is due to the way the deduplication is done in the two staging files.\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Inconsistent number of rows when re-running fact_trips model"
      },
      {
        "text": "If you encounter data type error on trip_type column, it may due to some nan values that isn\u2019t null in bigquery.\nSolution: try casting it to FLOAT datatype instead of NUMERIC",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Data Type Error when running fact table"
      },
      {
        "text": "This error could result if you are using some select * query without mentioning the name of table for ex:\nwith dim_zones as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\nwhere borough != 'Unknown'\n),\nfhv as (\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\n)\nselect * from fhv\ninner join dim_zones as pickup_zone\non fhv.PUlocationID = pickup_zone.locationid\ninner join dim_zones as dropoff_zone\non fhv.DOlocationID = dropoff_zone.locationid\n);\nTo resolve just replace use : select fhv.* from fhv",
        "section": "Module 4: analytics engineering with dbt",
        "question": "CREATE TABLE has columns with duplicate name locationid."
      },
      {
        "text": "Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\nSolution:\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\n{{ dbt_utils.safe_cast('ehail_fee',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Bad int64 value: 0.0 error"
      },
      {
        "text": "You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\ncast(replace({{ payment_type }},'.0','') as integer)",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Bad int64 value: 2.0/1.0 error"
      },
      {
        "text": "I found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\nYou can use the queries below to address them:\nCAST(\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\.0', '') AS INT64\n) AS ratecodeid,\nCAST(\nCASE\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\.\\d+') THEN NULL\nELSE CAST(trip_type AS INT64)\nEND AS INT64\n) AS trip_type,",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Bad int64 value: 1.0 error (again)"
      },
      {
        "text": "The two solution above don\u2019t work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\n`{{ dbt.safe_cast('ehail_fee',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - Error on building fact_trips.sql: Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")"
      },
      {
        "text": "Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\nIt should be:\ndbt run --var 'is_test_run: false'\nNot able to change Environment Type as it is greyed out and inaccessibleYou don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'",
        "section": "Module 4: analytics engineering with dbt",
        "question": "The - vars argument must be a YAML dictionary, but was of type str"
      },
      {
        "text": "Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\nsources:\n- name: staging\ndatabase: your_database_name\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the \u2018Custom Branch\u2019 settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\nGo to an environment and select Settings to edit it\nSelect Only run on a custom branch in General settings\nEnter the name of your custom branch (e.g. HW)\nClick Save",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Access Denied: Table yellow_tripdata: User does not have permission to query table yellow_tripdata, or perhaps it does not exist in location US."
      },
      {
        "text": "Running the Environment on the master branch causes this error, you must activate \u201cOnly run on a custom branch\u201d checkbox and specify the branch you are working when Environment is setup.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Could not parse the dbt project. please check that the repository contains a valid dbt project"
      },
      {
        "text": "Change to main branch, make a pull request from the development branch.\nNote: this will take you to github.\nApprove the merging and rerun you job, it would work as planned now",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Made change to your modelling files and commit the your development branch, but Job still runs on old file?"
      },
      {
        "text": "Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Setup - I\u2019ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?"
      },
      {
        "text": "My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "BigQuery returns an error when i try to run \u2018dbt run\u2019:"
      },
      {
        "text": "Use the syntax below instead if the code in the tutorial is not working.\ndbt run --select stg_green_tripdata --vars '{\"is_test_run\": false}'",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:"
      },
      {
        "text": "Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\nSolution:\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - Error: No module named 'pytz' while setting up dbt with docker"
      },
      {
        "text": "If you have problems editing dbt_project.yml when using Docker after \u2018docker-compose run dbt-bq-dtc init\u2019, to change profile \u2018taxi_rides_ny\u2019 to 'bq-dbt-workshop\u2019, just run:\nsudo chown -R username path\nDBT - Internal Error: Profile should not be None if loading is completed\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).",
        "section": "Module 4: analytics engineering with dbt",
        "question": "\u200b\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)"
      },
      {
        "text": "When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Google Cloud BigQuery Location Problems"
      },
      {
        "text": "This happens because we have moved the dbt project to another directory on our repo.\nOr might be that you\u2019re on a different branch than is expected to be merged from / to.\nSolution:\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\nFor example:\n/week5/taxi_rides_ny\nMake sure your file explorer path and this Project settings path matches and there\u2019s no files waiting to be committed to github if you\u2019re running the job to deploy to PROD.\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\nIn the picture below, I had set it to ella2024 to be checked as \u201cproduction-ready\u201d by the \u201cfreshness\u201d check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found."
      },
      {
        "text": "When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on \u2018US\u2019 location, if you have your dataset, schemas and tables on \u2018EU\u2019 that will generate an error and the pull request will not be accepted. To change that location to \u2018EU\u2019 on the connection to BigQuery from dbt we need to add the location \u2018EU\u2019 on the connection optional settings:\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT Deploy + CI - Location Problems on BigQuery"
      },
      {
        "text": "When running trying to run the dbt project on prod there is some things you need to do and check on your own:\nFirst Make the pull request and Merge the branch into the main.\nMake sure you have the latest version, if you made changes to the repo in another place.\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT Deploy - Error When trying to run the dbt project on Prod"
      },
      {
        "text": "In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\nSolution:\nTurns out I forgot to specify Location to be `EU` when adding connection details.\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - Error: \u201c404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU\u201d after building from stg_green_tripdata.sql"
      },
      {
        "text": "Issue: If you\u2019re having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with \u2018?raw=true\u2019 like so:\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet?raw=true\"\nSecond, update make sure the URL_PREFIX is set to the following value:\n\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\nIt is critical that you use this link with the keyword blob. If your link has \u2018tree\u2019 here, replace it. Everything else can stay the same, including the curl -sSLf command. \u2018",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Homework - Ingesting FHV_20?? data"
      },
      {
        "text": "Add this task based on the previous ones :\n- id: if_fhv_taxi\ntype: io.kestra.plugin.core.flow.If\ncondition: \"{{inputs.taxi == 'fhv'}}\"\nthen:\n- id: bq_fhv_tripdata\ntype: io.kestra.plugin.gcp.bigquery.Query\nsql: |\nCREATE TABLE IF NOT EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.fhv_tripdata`\n(\nunique_row_id BYTES OPTIONS (description = 'A unique identifier for the trip, generated by hashing key trip attributes.'),\nfilename STRING OPTIONS (description = 'The source filename from which the trip data was loaded.'),\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID NUMERIC,\nDOlocationID NUMERIC,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nPARTITION BY DATE(pickup_datetime);\n- id: bq_fhv_table_ext\ntype: io.kestra.plugin.gcp.bigquery.Query\nsql: |\nCREATE OR REPLACE EXTERNAL TABLE `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}_ext`\n(\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID NUMERIC,\nDOlocationID NUMERIC,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'CSV',\nuris = ['{{render(vars.gcs_file)}}'],\nskip_leading_rows = 1,\nignore_unknown_values = TRUE\n);\n- id: bq_fhv_table_tmp\ntype: io.kestra.plugin.gcp.bigquery.Query\nsql: |\nCREATE OR REPLACE TABLE `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}`\nAS\nSELECT\nMD5(CONCAT(\nCOALESCE(CAST(pickup_datetime AS STRING), \"\"),\nCOALESCE(CAST(dropoff_datetime AS STRING), \"\"),\nCOALESCE(CAST(PUlocationID AS STRING), \"\"),\nCOALESCE(CAST(DOLocationID AS STRING), \"\")\n)) AS unique_row_id,\n\"{{render(vars.file)}}\" AS filename,\n*\nFROM `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}_ext`;\n- id: bq_fhv_merge\ntype: io.kestra.plugin.gcp.bigquery.Query\nsql: |\nMERGE INTO `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.fhv_tripdata` T\nUSING `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}` S\nON T.unique_row_id = S.unique_row_id\nWHEN NOT MATCHED THEN\nINSERT (unique_row_id, filename, dispatching_base_num, pickup_datetime, dropoff_datetime, PUlocationID, DOlocationID, SR_Flag, Affiliated_base_number)\nVALUES (S.unique_row_id, S.filename, S.dispatching_base_num, S.pickup_datetime, S.dropoff_datetime, S.PUlocationID, S.DOlocationID, S.SR_Flag, S.Affiliated_base_number);\nAdd a trigger too :\n- id: fhv_schedule\ntype: io.kestra.plugin.core.trigger.Schedule\ncron: \"0 11 1 * *\"\ninputs:\ntaxi: fhv\nAnd modify inputs :\ninputs:\n- id: taxi\ntype: SELECT\ndisplayName: Select taxi type\nvalues: [yellow, green, fhv]\ndefaults: green",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Ingesting FHV : alternative with kestra"
      },
      {
        "text": "I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Homework - Ingesting NYC TLC Data"
      },
      {
        "text": "If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\nGOOGLE_APPLICATION_CREDENTIALS\nGCP_GCS_BUCKET\nThe easises option to do it  is to use .env  (dotenv).\nInstall it and add a few lines of code that inject these variables for your project\npip install python-dotenv\nfrom dotenv import load_dotenv\nimport os\n# Load environment variables from .env file\nload_dotenv()\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")",
        "section": "Module 4: analytics engineering with dbt",
        "question": "How to set environment variable easily for any credentials"
      },
      {
        "text": "If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\ndispatching_base_num STRING,\npickup_datetime STRING,\ndropoff_datetime STRING,\nPUlocationID STRING,\nDOlocationID STRING,\nSR_Flag STRING,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'csv',\nuris = ['gs://bucket/*.csv']\n);\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\nwith fhv_tripdata as (\nselect * from {{ ref('stg_fhv_tripdata') }}\n),\ndim_zones as (\nselect * from {{ ref('dim_zones') }}\nwhere borough != 'Unknown'\n)\nselect fhv_tripdata.dispatching_base_num,\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp"
      },
      {
        "text": "If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\n-----Correct load with schema defination----will not throw error----------------------\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\ndispatching_base_num STRING,\npickup_datetime TIMESTAMP,\ndropoff_datetime TIMESTAMP,\nPUlocationID FLOAT64,\nDOlocationID FLOAT64,\nSR_Flag FLOAT64,\nAffiliated_base_number STRING\n)\nOPTIONS (\nformat = 'PARQUET',\nuris = ['gs://project id/fhv_2019_8.parquet']\n);\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\n\u2013 THANKYOU FOR THIS \u2013",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldn\u2019t parse datetime column as timestamp,couldn\u2019t handle NULL values in PULocationID,DOLocationID"
      },
      {
        "text": "No matching signature for operator = for argument types: STRING, INT64\nSignature: T1 = T1\nUnable to find common supertype for templated argument\nMake sure the LocationID field is in the same type. If it is in string format in one table, we can use the following code in dbt to convert it to integer:\n{{ dbt.safe_cast(\"PULocationID\", api.Column.translate_type(\"integer\")) }} as pickup_locationid",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Join Error on LocationID \u201cUnable to find common supertype for templated argument\u201d"
      },
      {
        "text": "When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Google Looker Studio - you have used up your 30-day trial"
      },
      {
        "text": "Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "How does dbt handle dependencies between models?"
      },
      {
        "text": "Try loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Loading FHV Data goes into slumber using Mage?"
      },
      {
        "text": "If you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \nYou can change the location as follows:",
        "section": "Module 4: analytics engineering with dbt",
        "question": "Region Mismatch in DBT and BigQuery"
      },
      {
        "text": "Use the PostgreSQL COPY FROM feature that is compatible with csv files\nFirst create the table like (as an example):\nCREATE TABLE taxis (\n\u2026\n);\nAnd then use copy functionality (as an example):\nCOPY taxis FROM PROGRAM\n\u2018url'\nWITH (\nFORMAT csv,\nHEADER true,\nENCODING utf8\n);\nCOPY table_name [ ( column_name [, ...] ) ]\nFROM { 'filename' | PROGRAM 'command' | STDIN }\n[ [ WITH ] ( option [, ...] ) ]\n[ WHERE condition ]",
        "section": "Module 4: analytics engineering with dbt",
        "question": "What is the fastest way to upload taxi data to dbt-postgres?"
      },
      {
        "text": "For local environment i.e. dbt-core, the profile configuration is valid for all projects. Note: dbt Cloud doesn\u2019t require it.\nThe ~/.dbt/profiles.yml file should be located in your user's home directory. On Windows, this would typically be:\nC:\\Users\\<YourUsername>\\.dbt\\profiles.yml\nReplace <YourUsername> with your actual Windows username. This file is used by dbt to store connection profiles for different projects.\nHere's how you can create the profiles.yml file in the appropriate directory:\nOpen File Explorer and navigate to C:\\Users\\<YourUsername>\\.\nCreate a new folder named .dbt if it doesn't already exist.\nInside the .dbt folder, create a new file named profiles.yml.\nUsage example can be found here.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "dbt - Where should we create `profiles.yml` ?"
      },
      {
        "text": "Second only to dbt Cloud functionality: https://github.com/AltimateAI/vscode-dbt-power-user Sign up for the community plan for free usage at Altimate and add the API into your VS Code extension.\nVSCode Snippets Package for dbt and Jinja functions in SQL, YAML, and Markdown: https://github.com/bastienboutonnet/vscode-dbt\nFor monitoring purposes: https://github.com/elementary-data/elementary Read more here.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "dbt - Are there UI for dbt Core like dbt Cloud?"
      },
      {
        "text": "Update the line:\nWith:",
        "section": "Module 4: analytics engineering with dbt",
        "question": "When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I'm getting \"Credentials in profile \"PROFILE_NAME\", target: 'dev', invalid: '5432'is not of type 'integer'"
      },
      {
        "text": "What to do if your  dbt model fails with an error similar to:\nDBT-CORE\nCheck profiles.yml:\nEnsure your profiles.yml file is correctly configured with the correct schema and database under your target. This file is typically located in ~/.dbt/.\nExample configuration:\nDBT-CLOUD-IDE\nCheck Credentials in dbt Cloud UI:\nNavigate to the Credentials section in the dbt Cloud project settings.\nEnsure the correct database and schema are set (e.g., \u2018my_dataset\u2019).\nVerify Environment Settings:\nDouble-check that you are working in the correct environment (dev, prod, etc.), as dbt Cloud allows different settings for different environments.\nNo Need for profiles.yml:\nIn dbt Cloud, you don\u2019t need to configure profiles.yml manually. All connection settings are handled via the UI.",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT - The database is correct but I get Error with Incorrect Schema in Models"
      },
      {
        "text": "Yes, DBT allows only 1 project under one account. But you can create multiple accounts as shown below:",
        "section": "Module 4: analytics engineering with dbt",
        "question": "DBT allows only 1 project in free developer version."
      },
      {
        "text": "In the free version, it does not show the docs when models are run in development environment. Create a production job and tick generate docs section. Execute it and it will generate the documentation.",
        "section": "Module 5: pyspark",
        "question": "Documentation or book sign not shown even after doing dbt docs generate."
      },
      {
        "text": "Install SDKMAN:\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\nsdk install java 11.0.22-tem\nsdk install spark 3.3.2\nOpen a new terminal or run the following in the same shell:\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nVerify the locations and versions of Java and Spark that were installed:\necho $JAVA_HOME\njava -version\necho $SPARK_HOME\nspark-submit --version",
        "section": "Module 5: pyspark",
        "question": "Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)"
      },
      {
        "text": "If you\u2019re seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\nStarter notebook:\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\nIt\u2019s advisable to spend some time setting things up locally rather than jumping right into this solution.",
        "section": "Module 5: pyspark",
        "question": "PySpark - Setting Spark up in Google Colab"
      },
      {
        "text": "If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\nmodule @0x3c947bc5\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.",
        "section": "Module 5: pyspark",
        "question": "Spark-shell: unable to load native-hadoop library for platform - Windows"
      },
      {
        "text": "I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\nSolution:\npip install findspark on the command line inside proper environment\nAdd to the top of the script\nimport findspark\nfindspark.init()",
        "section": "Module 5: pyspark",
        "question": "PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases."
      },
      {
        "text": "This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].",
        "section": "Module 5: pyspark",
        "question": "PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)"
      },
      {
        "text": "Ensure that your `PYTHONPATH` is set correctly to include the PySpark library. You can check if PySpark is pointing to the correct location by running:\nimport pyspark\nprint(pyspark.__file__)\nIt should point to the location where PySpark is installed (e.g., `/home/<your username>/spark/spark-3.x.x-bin-hadoop3.x/python/pyspark/__init__.py`)",
        "section": "Module 5: pyspark",
        "question": "Import pyspark - Error: No Module named \u2018pyspark\u2019"
      },
      {
        "text": "This is because current port is in use, Spark UI will run on a different port. You can check which port Spark is using by running this command:\nspark.sparkContext.uiWebUrl\nIf it indicates a different port, you should access that specific port instead.  Additionally, ensure that there are no other notebooks or processes that might be using the same port. Clean up unused resources to avoid port conflicts.",
        "section": "Module 5: pyspark",
        "question": "Cannot find Spark jobs UI at localhost"
      },
      {
        "text": "If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\nInstall OpenJDK 11,\non MacOS: $ brew install java11\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\nto ~/.bashrc or ~/zshrc\nActivate working environment (by pipenv / poetry / conda)\nRun $ pip install pyspark\nWork with exercises as normal\nAll default commands of spark will be also available at shell session under activated enviroment.\nHope this can help!\nP.s. you won\u2019t need findspark to firstly initialize.\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\nIf you're getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You're most likely using incompatible versions of the JDK or Python with Spark.\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\n\n$ sdk install java 17.0.10-librca\n$ sdk install spark 3.5.0\n$ sdk install hadoop 3.3.5py4j\nAs PySpark 3.5.0 supports Python 3.8+ make sure you're setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\n\n\n$ conda create -n ENV_NAME python=3.11\n$ conda activate ENV_NAME\n$ pip install pyspark==3.5.0\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet('zones')(On window)\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\nHere my\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.",
        "section": "Module 5: pyspark",
        "question": "Java+Spark - Easy setup with miniconda env (worked on MacOS)"
      },
      {
        "text": "Issue: Spark installation on Windows completed but failed to run.\nThis is a common Windows Installer error code indicating that there was a fatal error during installation. It often occurs due to issues like insufficient permissions, conflicts with other software, or problems with the installer package.\nStep to solve the issue:\nInstalling Chocolatey\nChocolatey is a package manager for Windows, which makes it easy to install, update, and manage software.\nInstallation Steps\nOpen PowerShell as an Administrator\nPress Win + X and select Windows PowerShell (Admin) or search for PowerShell, right-click, and select Run as administrator.\nRun the following command to install Chocolatey\n\n Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('<https://community.chocolatey.org/install.ps1>'))\nVerify the installation\nClose and reopen PowerShell as an administrator and run:\n\n choco -v\nYou should see the Chocolatey version number indicating that it has been installed successfully.\nCommand for Global Acceptance\nTo globally accept all licenses for all packages installed using Chocolatey, run the following command:\nchoco feature enable -n allowGlobalConfirmation\nThis command configures Chocolatey to automatically accept license agreements for all packages, streamlining the installation process and avoiding prompts for each package.",
        "section": "Module 5: pyspark",
        "question": "Spark - Installation Error Code 1603"
      },
      {
        "text": "After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName('test') \\\n.getOrCreate()\ndf = spark.read \\\n.option(\"header\", \"true\") \\\n.csv('taxi+_zone_lookup.csv')\ndf.show()\nit gives the error:\nRuntimeError: Java gateway process exited before sending its port number\n\u2705The solution (for me) was:\npip install findspark on the command line and then\nAdd\nimport findspark\nfindspark.init()\nto the top of the script.\nAnother possible solution is:\nCheck that pyspark is pointing to the correct location.\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial:\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.",
        "section": "Module 5: pyspark",
        "question": "RuntimeError: Java gateway process exited before sending its port number"
      },
      {
        "text": "Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\nThe solution which worked for me(use following in jupyter notebook) :\n!pip install findspark\nimport findspark\nfindspark.init()\nThereafter , import pyspark and create spark contex<<t as usual\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\nFilter based on conditions based on multiple columns\nfrom pyspark.sql.functions import col\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\nKrishna Anand",
        "section": "Module 5: pyspark",
        "question": "Module Not Found Error in Jupyter Notebook ."
      },
      {
        "text": "You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\nexport PYTHONPATH=\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\u201d\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\nAdditionally, you can check for the version of \u2018py4j\u2019 of the spark you\u2019re using from here and update as mentioned above.\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.",
        "section": "Module 5: pyspark",
        "question": "Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`"
      },
      {
        "text": "If below does not work, then download the latest available py4j version with\nconda install -c conda-forge py4j\nTake care of the latest version number in the website to replace appropriately.\nNow add\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\nin your  .bashrc file.",
        "section": "Module 5: pyspark",
        "question": "Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)"
      },
      {
        "text": "Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\nFull steps:\nUpdate and upgrade packages:\nsudo apt update && sudo apt -y upgrade\nInstall Python:\nsudo apt install python3-pip python3-dev\nInstall Python virtualenv:\nsudo -H pip3 install --upgrade pip\nsudo -H pip3 install virtualenv\nCreate a Python Virtual Environment:\nmkdir notebook\ncd notebook\nvirtualenv jupyterenv\nsource jupyterenv/bin/activate\nInstall Jupyter Notebook:\npip install jupyter\nRun Jupyter Notebook:\njupyter notebook",
        "section": "Module 5: pyspark",
        "question": "Exception: Jupyter command `jupyter-notebook` not found."
      },
      {
        "text": "The latest filename is just \u2018taxi_zone_lookup.sv\u2019 so it should work after removing the \u2018+\u2019 now.",
        "section": "Module 5: pyspark",
        "question": "Following 5.2.1, I am getting an error - Head:cannot open \u2018taxi+_zone_lookup.csv\u2019 for reading: No such file or directory"
      },
      {
        "text": "Code executed:\ndf = spark.read.parquet(pq_path)\n\u2026 some operations on df \u2026\ndf.write.parquet(pq_path, mode=\"overwrite\")\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=\u201doverwrite\u201d)\n\u2705Solution: Write to a different directorydf\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")",
        "section": "Module 5: pyspark",
        "question": "Error java.io.FileNotFoundException"
      },
      {
        "text": "You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .",
        "section": "Module 5: pyspark",
        "question": "Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)"
      },
      {
        "text": "Actually Spark SQL is one independent \u201ctype\u201d of SQL - Spark SQL.\nThe several SQL providers are very similar:\nSELECT [attributes]\nFROM [table]\nWHERE [filter]\nGROUP BY [grouping attributes]\nHAVING [filtering the groups]\nORDER BY [attribute to order]\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\nON [attributes table joining table2] (...)\nWhat differs the most between several SQL providers are built-in functions.\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\nExtra information on SPARK SQL :\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.",
        "section": "Module 5: pyspark",
        "question": "Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?"
      },
      {
        "text": "\u2705Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\nYou can run spark.sparkContext.uiWebUrl\nand result will be some like\n'http://172.19.10.61:4041'",
        "section": "Module 5: pyspark",
        "question": "The spark viewer on localhost:4040 was not showing the current run"
      },
      {
        "text": "\u2705Solution: replace Java Developer Kit 11 with Java Developer Kit 8.",
        "section": "Module 5: pyspark",
        "question": "Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)"
      },
      {
        "text": "Shows java_home is not set on the notebook log\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839",
        "section": "Module 5: pyspark",
        "question": "Java - RuntimeError: Java gateway process exited before sending its port number"
      },
      {
        "text": "\u2705I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\nTo create the SparkSession:\nspark = SparkSession.builder.master('local[*]') \\\n.appName('spark-read-from-bigquery') \\\n.config('BigQueryProjectId','razor-project-xxxxxxx) \\\n.config('BigQueryDatasetLocation','de_final_data') \\\n.config('parentProject','razor-project-xxxxxxx) \\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\n.config(\"spark.driver.memory\", \"4g\") \\\n.config(\"spark.executor.memory\", \"2g\") \\\n.config(\"spark.memory.offHeap.enabled\",True) \\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\n.config('google.cloud.auth.service.account.json.keyfile', \"google_credentials.json\") \\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n.getOrCreate()",
        "section": "Module 5: pyspark",
        "question": "Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries"
      },
      {
        "text": "While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\nspark = SparkSession.builder.master('local').appName('bq').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here",
        "section": "Module 5: pyspark",
        "question": "Spark BigQuery connector Automatic configuration"
      },
      {
        "text": "Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\nThere\u2019s a few extra steps to go into reading from GCS with PySpark\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n3.) In your Python script, there are a few extra classes you\u2019ll have to import:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n4.) You must set up your configurations before building your SparkSession. Here\u2019s my code snippet:\nconf = SparkConf() \\\n.setMaster('local[*]') \\\n.setAppName('test') \\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc = SparkContext(conf=conf)\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n5.) Once you run that, build your SparkSession with the new parameters we\u2019d just instantiated in the previous step:\nspark = SparkSession.builder \\\n.config(conf=sc.getConf()) \\\n.getOrCreate()\n6.) Finally, you\u2019re able to read your files straight from GCS!\nstart-slave.sh: command not found",
        "section": "Module 5: pyspark",
        "question": "Spark Cloud Storage connector"
      },
      {
        "text": "from pyarrow.parquet import ParquetFile\npf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\n#pyarrow builds tables, not dataframes\ntbl_small = next(pf.iter_batches(batch_size = 1000))\n#this function converts the table to a dataframe of manageable size\ndf = tbl_small.to_pandas()\nAlternatively without PyArrow:\ndf = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\ndf1 = df.sort('DOLocationID').limit(1000)\npdf = df1.select(\"*\").toPandas()",
        "section": "Module 5: pyspark",
        "question": "How can I read a small number of rows from the parquet file directly?"
      },
      {
        "text": "Probably you\u2019ll encounter this if you followed the video \u20185.3.1 - First Look at Spark/PySpark\u2019 and used the parquet file from the TLC website (csv was used in the video).\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and you\u2019ll get an error like:\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\nChange the schema definition from IntegerType to LongType and it should work",
        "section": "Module 5: pyspark",
        "question": "DataType error when creating Spark DataFrame with a specified schema?"
      },
      {
        "text": "df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\nKrishna Anand",
        "section": "Module 5: pyspark",
        "question": "Remove white spaces from column names in Pyspark"
      },
      {
        "text": "This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\npd.DataFrame.iteritems = pd.DataFrame.items\nNote that this problem is solved with Spark versions from 3.4.1",
        "section": "Module 5: pyspark",
        "question": "AttributeError: 'DataFrame' object has no attribute 'iteritems'"
      },
      {
        "text": "Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"",
        "section": "Module 5: pyspark",
        "question": "AttributeError: 'DataFrame' object has no attribute 'iteritems'"
      },
      {
        "text": "Open a CMD terminal in administrator mode\ncd %SPARK_HOME%\nStart a master node: bin\\spark-class org.apache.spark.deploy.master.Master\nStart a worker node: bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\nNow you can access Spark UI through localhost:8080",
        "section": "Module 5: pyspark",
        "question": "Spark Standalone Mode on Windows"
      },
      {
        "text": "You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\nimport findspark\nfindspark.init()",
        "section": "Module 5: pyspark",
        "question": "Export PYTHONPATH command in linux is temporary"
      },
      {
        "text": "In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\n\u2705solution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\necho \"downloading ${URL} to ${LOCAL_PATH}\"\nmkdir -p ${LOCAL_PREFIX}\nwget ${URL} -O ${LOCAL_PATH}\necho \"compressing ${LOCAL_PATH}\"\n# gzip ${LOCAL_PATH} <- uncomment this line",
        "section": "Module 5: pyspark",
        "question": "Compression Error: zcat output is gibberish, seems like still compressed"
      },
      {
        "text": "Occurred while running : spark.createDataFrame(df_pandas).show()\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesn\u2019t support python 3.11, try creating a new env with python version 3.8 and then run this command.\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\nconda create -n myenv python=3.10 anaconda\nThen you must run conda activate myenv to run python 3.10. Otherwise you\u2019ll still be running version 3.11. You can deactivate by typing conda deactivate.",
        "section": "Module 5: pyspark",
        "question": "PicklingError: Could not serialise object: IndexError: tuple index out of range."
      },
      {
        "text": "Make sure you have your credentials of your GCP in your VM under the location defined in the script.",
        "section": "Module 5: pyspark",
        "question": "Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?"
      },
      {
        "text": "To run spark in docker setup\n1. Build bitnami spark docker\na. clone bitnami repo using command\ngit clone https://github.com/bitnami/containers.git\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\nreference: https://github.com/bitnami/containers/issues/13409\nc. build docker image by navigating to above directory and running docker build command\nnavigate cd bitnami/spark/3.3/debian-11/\nbuild command docker build -t spark:3.3-java-17 .\n2. run docker compose\nusing following file\n```yaml docker-compose.yml\nversion: '2'\nservices:\nspark:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=master\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8080:8080'\n- '7077:7077'\nspark-worker:\nimage: spark:3.3-java-17\nenvironment:\n- SPARK_MODE=worker\n- SPARK_MASTER_URL=spark://spark:7077\n- SPARK_WORKER_MEMORY=1G\n- SPARK_WORKER_CORES=1\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\n- SPARK_RPC_ENCRYPTION_ENABLED=no\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n- SPARK_SSL_ENABLED=no\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8081:8081'\nspark-nb:\nimage: jupyter/pyspark-notebook:java-17.0.5\nenvironment:\n- SPARK_MASTER_URL=spark://spark:7077\nvolumes:\n- \"./:/home/jovyan/work:rw\"\nports:\n- '8888:8888'\n- '4040:4040'\n```\nrun command to deploy docker compose\ndocker-compose up\nAccess jupyter notebook using link logged in docker compose logs\nSpark master url is spark://spark:7077",
        "section": "Module 5: pyspark",
        "question": "Spark docker-compose setup"
      },
      {
        "text": "To do this\npip install gcsfs,\nThereafter copy the uri path to the file and use \ndf = pandas.read_csc(gs://path)",
        "section": "Module 5: pyspark",
        "question": "How do you read data stored in gcs on pandas with your local computer?"
      },
      {
        "text": "Error:\nspark.createDataFrame(df_pandas).schema\nTypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\nSolution:\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don\u2019t have to take out any data from your dataset. Something like this can help:\ndf = spark.read \\\n.options(\nheader = \"true\", \\\ninferSchema = \"true\", \\\n) \\\n.csv('path/to/your/csv/file/')\nSolution B:\nIt's because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the 'Affiliated_base_number' column. Then you will be able to apply the pyspark function createDataFrame.\n# Only take rows that have no null values\npandas_df= pandas_df[pandas_df.notnull().all(1)]",
        "section": "Module 5: pyspark",
        "question": "TypeError when using spark.createDataFrame function on a pandas df"
      },
      {
        "text": "Default executor memory is 1gb. This error appeared when working with the homework dataset.\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\nScaling row group sizes to 95.00% for 8 writers\nSolution:\nIncrease the memory of the executor when creating the Spark session like this:\nspark = SparkSession.builder \\\n.master(\"local[*]\") \\\n.appName('test') \\\n.config(\"spark.executor.memory\", \"4g\") \\\n.config(\"spark.driver.memory\", \"4g\") \\\n.getOrCreate()\nRemember to restart the Jupyter session (ie. close the Spark session) or the config won\u2019t take effect.",
        "section": "Module 5: pyspark",
        "question": "MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory"
      },
      {
        "text": "Change the working directory to the spark directory:\nif you have setup up your SPARK_HOME variable, use the following;\ncd %SPARK_HOME%\nif not, use the following;\ncd <path to spark installation>\nCreating a Local Spark Cluster\nTo start Spark Master:\nbin\\spark-class org.apache.spark.deploy.master.Master --host localhost\nStarting up a cluster:\nbin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost",
        "section": "Module 5: pyspark",
        "question": "How to spark standalone cluster is run on windows OS"
      },
      {
        "text": "I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldn\u2019t be found in .ipynb opened in VS Code\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\nsource ~/.bashrc\nexec bash\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:\nJAVA_HOME=\"${HOME}/app/java/jdk-11.0.2\"\nPATH=\"${JAVA_HOME}/bin:${PATH}\"\nSPARK_HOME=\"${HOME}/app/spark/spark-3.3.2-bin-hadoop3\"\nPATH=\"${SPARK_HOME}/bin:${PATH}\"\nPYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nPYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH\"\nPYTHONPATH=\"${SPARK_HOME}/python/lib/pyspark.zip:$PYTHONPATH\"",
        "section": "Module 5: pyspark",
        "question": "Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code"
      },
      {
        "text": "If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.",
        "section": "Module 5: pyspark",
        "question": "hadoop \u201cwc -l\u201d is giving a different result then shown in the video"
      },
      {
        "text": "If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\nFor Windows, create a new User Variable \u201cHADOOP_HOME\u201d that points to your Hadoop directory. Then add \u201c%HADOOP_HOME%\\bin\u201d to the PATH variable.\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io",
        "section": "Module 5: pyspark",
        "question": "Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z"
      },
      {
        "text": "Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master \u00b7 cdarlint/winutils (github.com)\nIf this does not work try to change other versions found in this repository.\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running \u00b7 Issue #20 \u00b7 cdarlint/winutils (github.com)",
        "section": "Module 5: pyspark",
        "question": "Java.io.IOException. Cannot run program \u201cC:\\hadoop\\bin\\winutils.exe\u201d. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using."
      },
      {
        "text": "Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\ngcloud dataproc jobs submit pyspark \\\n--cluster=my_cluster \\\n--region=us-central1 \\\n--project=my-dtc-project-1010101 \\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\n-- \\\n\u2026",
        "section": "Module 5: pyspark",
        "question": "Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag."
      },
      {
        "text": "Go to %SPARK_HOME%\\bin\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\nCreate a new Jupyter notebook:\nspark = SparkSession.builder \\\n.master(\"spark://{ip}:7077\") \\\n.appName('test') \\\n.getOrCreate()\nCheck on Spark UI the master, worker and app.",
        "section": "Module 5: pyspark",
        "question": "Run Local Cluster Spark in Windows 10 with CMD"
      },
      {
        "text": "This occurs because you are not logged in \u201cgcloud auth login\u201d and maybe the project id is not settled. Then type in a terminal:\ngcloud auth login\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\ngcloud config set project <YOUR PROJECT_ID>\nThen you can run the command to upload the pq dir to a GCS Bucket:\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq",
        "section": "Module 5: pyspark",
        "question": "lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist)."
      },
      {
        "text": "When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.",
        "section": "Module 5: pyspark",
        "question": "py4j.protocol.Py4JJavaError  GCP"
      },
      {
        "text": "Use both repartition and coalesce, like so:\ndf = df.repartition(6)\ndf = df.coalesce(6)\ndf.write.parquet('fhv/2019/10', mode='overwrite')",
        "section": "Module 5: pyspark",
        "question": "Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead"
      },
      {
        "text": "Possible solution - Try to forward the port using ssh cli instead of vs code.\nRun > \u201cssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>\u201d\nssh hostname is the name you specified in the ~/.ssh/config file\nIn case of Jupyter Notebook run\n\u201cssh -L 8888:localhost:8888 gcp-vm\u201d\nfrom your local machine\u2019s cli.\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\n```\nHost <hostname>\nHostname <external-gcp-ip>\nUser xxxx\nIdentityFile yyyy\nLocalForward 8888 localhost:8888\nLocalForward 8080 localhost:8080\nLocalForward 5432 localhost:5432\nLocalForward 4040 localhost:4040\n```\nThis should automatically forward all ports and will enable accessing localhost ports.",
        "section": "Module 5: pyspark",
        "question": "Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?"
      },
      {
        "text": "~ Abhijit Chakraborty\n`sdk list java`  to check for available java sdk versions.\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\nclick on Y if prompted to change the default java version.\nCheck for java version using `java -version `.\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.",
        "section": "Module 5: pyspark",
        "question": "Installing Java 11 on codespaces"
      },
      {
        "text": "Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster. \u2013 abhirup ghosh\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan",
        "section": "Module 5: pyspark",
        "question": "Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0."
      },
      {
        "text": "Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\nUpper Date: the closest date you have. For example dropoff_datetime\nLower Date: the farthest date you have.  For example pickup_datetime\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.",
        "section": "Module 5: pyspark",
        "question": "Homework - how to convert the time difference of two timestamps to hours"
      },
      {
        "text": "This version combination worked for me:\nPySpark = 3.3.2\nPandas = 1.5.3\n\nIf it still has an error,\nPy4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\nRun this before SparkSession\nimport os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable",
        "section": "Module 5: pyspark",
        "question": "PicklingError: Could not serialize object: IndexError: tuple index out of range"
      },
      {
        "text": "import os\nimport sys\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing",
        "section": "Module 5: pyspark",
        "question": "RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set."
      },
      {
        "text": "Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\ngcloud dataproc jobs submit pyspark \\\n--cluster=de-zoomcamp-cluster \\\n--region=europe-west6 \\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n-- \\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)",
        "section": "Module 5: pyspark",
        "question": "Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?"
      },
      {
        "text": "AttributeError: 'DataFrame' object has no attribute 'iteritems'\nthis is because the method inside the pyspark refers to a package that has been already deprecated\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\nYou can do this code below, which is mentioned in the stackoverflow link above:\nAnother work around here is to create a conda enviroment to donwgrade python\u2019s version to 3.8 and pandas to 1.5.3\nconda create -n pyspark_env python=3.8 pandas=1.5.3 jupyter\nconda activate pyspark_env",
        "section": "Module 5: pyspark",
        "question": "In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error"
      },
      {
        "text": "A: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\nMaster Node:\nMachine type: n2-standard-2\nPrimary disk size: 85 GB\nWorker Node:\nNumber of worker nodes: 2\nMachine type: n2-standard-2\nPrimary disk size: 80 GB\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.",
        "section": "Module 5: pyspark",
        "question": "Cannot create a cluster: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0."
      },
      {
        "text": "The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\nexport PATH=\"$JAVA_HOME:$PATH\"\nConfirm that your path was correctly set by running the command: which java\nYou should expect to see the output:\n/opt/homebrew/opt/openjdk/bin/java\nCheck java version with the next command:\nJava -version\nReference: https://docs.brew.sh/Installation",
        "section": "Module 5: pyspark",
        "question": "Setting JAVA_HOME with Homebrew on Apple Silicon"
      },
      {
        "text": "Search for VPC in Google Cloud Console\nNavigate to the second tab \u201cSUBNETS IN CURRENT PROJECT\u201d\nLook for the region/location where your dataproc will be located and click on it\nClick the edit button and toggle on the button for \u201cPrivate Google Access\u201d\nSave changes.",
        "section": "Module 5: pyspark",
        "question": "Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'."
      },
      {
        "text": "Since we used multiple notebooks during the course, it's possible that there are more than one Spark session active. It\u2019s highly likely that you are observing the incorrect one. Follow these steps to troubleshoot:\nSpark uses port 4040 by default, but if more than one session is active, it will try to use the next available port (e.g., 4041).\nEnsure you're viewing the correct Spark Web UI for the application where your jobs are running.\nVerify the current application session address:\nEg: Using spark.sparkContext.uiWebUrl command in your session.\nExpected output: http://your.application.session.address.internal:4041\nIndicating port 4041\nIf using a VM, make sure you forward the identified port to access the web ui on the localhost:<port>.",
        "section": "Module 6: streaming with kafka",
        "question": "Spark is working, however, nothing appears in the Spark UI (e.g., .show())?"
      },
      {
        "text": "Check Docker Compose File:\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed \u201cdocker ps.\u201d I deleted them in docker desktop and then had no problem starting up the kafka environment.",
        "section": "Module 6: streaming with kafka",
        "question": "Could not start docker image \u201ccontrol-center\u201d from the docker-compose.yaml file."
      },
      {
        "text": "Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\nTo create a virtual env and install packages (run only once)\npython -m venv env\nsource env/bin/activate\npip install -r ../requirements.txt\nTo activate it (you'll need to run it every time you need the virtual env):\nsource env/bin/activate\nTo deactivate it:\ndeactivate\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.",
        "section": "Module 6: streaming with kafka",
        "question": "Module \u201ckafka\u201d not found when trying to run producer.py"
      },
      {
        "text": "ImportError: DLL load failed while importing cimpl: The specified module could not be found\nVerify Python Version:\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\nfrom ctypes import CDLL\nCDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\librdkafka-5d2e2910.dll\")\nIt seems that the error may occur depending on the OS and python version installed.\nALTERNATIVE:\nImportError: DLL load failed while importing cimpl\n\u2705SOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\nYou need to set this DLL manually in Conda Env.\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2",
        "section": "Module 6: streaming with kafka",
        "question": "Error importing cimpl dll when running avro examples"
      },
      {
        "text": "\u2705SOLUTION: pip install confluent-kafka[avro].\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\nMore sources on Anaconda and confluent-kafka issues:\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka",
        "section": "Module 6: streaming with kafka",
        "question": "ModuleNotFoundError: No module named 'avro'"
      },
      {
        "text": "If you get an error while running the command python3 stream.py worker\nRun pip uninstall kafka-python\nThen run pip install kafka-python==1.4.6",
        "section": "Module 6: streaming with kafka",
        "question": "Error while running python3 stream.py worker"
      },
      {
        "text": "Redpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.",
        "section": "Module 6: streaming with kafka",
        "question": "What is the use of  Redpanda ?"
      },
      {
        "text": "Got this error because the docker container memory was exhausted. The data file was up to 800MB but my docker container does not have enough memory to handle that.\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.",
        "section": "Module 6: streaming with kafka",
        "question": "Negsignal:SIGKILL while converting data files to parquet format"
      },
      {
        "text": "Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv",
        "section": "Module 6: streaming with kafka",
        "question": "resources/rides.csv is missing"
      },
      {
        "text": "tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.",
        "section": "Module 6: streaming with kafka",
        "question": "Kafka - python videos have low audio and hard to follow up"
      },
      {
        "text": "There is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.",
        "section": "Module 6: streaming with kafka",
        "question": "Kafka Python Videos - Rides.csv"
      },
      {
        "text": "If you have this error, it most likely that your kafka broker docker container is not working.\nUse docker ps to confirm\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.",
        "section": "Module 6: streaming with kafka",
        "question": "kafka.errors.NoBrokersAvailable: NoBrokersAvailable"
      },
      {
        "text": "Ankush said we can focus on horizontal scaling option.\n\u201cthink of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling\u201d",
        "section": "Module 6: streaming with kafka",
        "question": "Kafka homework Q3, there are options that support scaling concept more than the others:"
      },
      {
        "text": "If you get this error, know that you have not built your sparks and juypter images. This images aren\u2019t readily available on dockerHub.\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose",
        "section": "Module 6: streaming with kafka",
        "question": "How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied"
      },
      {
        "text": "Run this command in terminal in the same directory (/docker/spark):\nchmod +x build.sh",
        "section": "Module 6: streaming with kafka",
        "question": "Python Kafka: ./build.sh: Permission denied Error"
      },
      {
        "text": "Restarting all services worked for me:\ndocker-compose down\ndocker-compose up",
        "section": "Module 6: streaming with kafka",
        "question": "Python Kafka: \u2018KafkaTimeoutError: Failed to update metadata after 60.0 secs.\u2019 when running stream-example/producer.py"
      },
      {
        "text": "While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n\u2026\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077\u2026\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n\u2026\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n\u2026\nSolution:\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\nSolution 2:\nCheck what Spark version your local machine has\npyspark \u2013version\nspark-submit \u2013version\nAdd your version to SPARK_VERSION in build.sh",
        "section": "Module 6: streaming with kafka",
        "question": "Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up."
      },
      {
        "text": "Start a new terminal\nRun: docker ps\nCopy the CONTAINER ID of the spark-master container\nRun: docker exec -it <spark_master_container_id> bash\nRun: cat logs/spark-master.out\nCheck for the log when the error happened\nGoogle the error message from there",
        "section": "Module 6: streaming with kafka",
        "question": "Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails"
      },
      {
        "text": "Make sure your java version is 11 or 8.\nCheck your version by:\njava --version\nCheck all your versions by:\n/usr/libexec/java_home -V\nIf you already have got java 11 but just not selected as default, select the specific version by:\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n(or other version of 11)",
        "section": "Module 6: streaming with kafka",
        "question": "Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext."
      },
      {
        "text": "In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\nSolution:\nIn build.gradle file, I added the following at the end:\nshadowJar {\narchiveBaseName = \"java-kafka-rides\"\narchiveClassifier = ''\n}\nAnd then in the command line ran \u2018gradle shadowjar\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar",
        "section": "Module 6: streaming with kafka",
        "question": "Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build"
      },
      {
        "text": "confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\nfastavro: pip install fastavro\nAbhirup Ghosh",
        "section": "Module 6: streaming with kafka",
        "question": "Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py"
      },
      {
        "text": "The Faust repository and library is no longer maintained - https://github.com/robinhood/faust\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.",
        "section": "Module 6: streaming with kafka",
        "question": "Can install Faust Library for Module 6 Python Version due to dependency conflicts?"
      },
      {
        "text": "In the project directory, run:\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java",
        "section": "Module 6: streaming with kafka",
        "question": "Java Kafka: How to run producer/consumer/kstreams/etc in terminal"
      },
      {
        "text": "For example, when running JsonConsumer.java, got:\nConsuming form kafka started\nRESULTS:::0\nRESULTS:::0\nRESULTS:::0\nOr when running JsonProducer.java, got:\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\nSolution:\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)",
        "section": "Module 6: streaming with kafka",
        "question": "Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent"
      },
      {
        "text": "Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn\u2019t see it at first and had to do some fixes.\nSolution:\n(Source)\nVS Code\n\u2192 Explorer (first icon on the left navigation bar)\n\u2192 JAVA PROJECTS (bottom collapsable)\n\u2192  icon next in the rightmost position to JAVA PROJECTS\n\u2192  clean Workspace\n\u2192 Confirm by clicking Reload and Delete\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\nE.g.:\nYou can also add classes and packages in this window instead of creating files in the project directory",
        "section": "Module 6: streaming with kafka",
        "question": "Java Kafka: Tests are not picked up in VSCode"
      },
      {
        "text": "In Confluent Cloud:\nEnvironment \u2192 default (or whatever you named your environment as) \u2192 The right navigation bar \u2192  \u201cStream Governance API\u201d \u2192  The URL under \u201cEndpoint\u201d\nAnd create credentials from Credentials section below it",
        "section": "Module 6: streaming with kafka",
        "question": "Confluent Kafka: Where can I find schema registry URL?"
      },
      {
        "text": "You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.",
        "section": "Module 6: streaming with kafka",
        "question": "How do I check compatibility of local and container Spark versions?"
      },
      {
        "text": "According to https://github.com/dpkp/kafka-python/\n\u201cDUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING\u201d\nUse pip install kafka-python-ng instead",
        "section": "Module 6: streaming with kafka",
        "question": "How to fix the error \"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\"?"
      },
      {
        "text": "Instead of using \u201clocalhost\u201d as the host name/address, try \u201cpostgres\u201d, or \u201chost.docker.internal\u201d instead\nAlternative Solution: For those having installed postgres locally and disabling persist data on postgres-container in docker i.e. volume: removed, remember to use postgres port other than 5432 (e.g. 5433 is usable). And for pgadmin host name/address, if localhost, postgres, and host.docker.internal is not working, you can use your own IPv4 Address which can be found in Windows OS via: Command Prompt > ipconfig > Under Wireless LAN adapter WiFi 2. E.g.:\nIPv4 Address. . . . . . . . . . . : 192.168.0.148",
        "section": "Module 6: streaming with kafka",
        "question": "How to fix \u201cconnection failed: connection to server at \"127.0.0.1\", port 5432 failed\u201d error when setting up Postgres connection in pgAdmin?"
      },
      {
        "text": "There could be a few reasons for this issue:\nRace Conditions: If you're running multiple processes in parallel.\nDatabase Connection Issues: The job might not be connecting to the correct PostgreSQL database, or there could be authentication or permission issues preventing table creation.\nMissing Table Creation Logic: The code responsible for creating the table might not be properly included or executed in the job submission process.\nAs a best practice, it's generally recommended to pre-create tables in PostgreSQL to avoid runtime errors. This ensures the database schema is properly set up before any jobs are executed.\nExtra: Use CREATE TABLE IF NOT EXISTS in your code. This will prevent errors if the table already exists and ensure smooth job execution.",
        "section": "Project",
        "question": "Why is my table not being created in PostgreSQL when I submit a job?"
      },
      {
        "text": "Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.",
        "section": "Project",
        "question": "How is my capstone project going to be evaluated?"
      },
      {
        "text": "Collaboration is not allowed for the capstone submission. However, you can discuss ideas and get feedback from peers in the forums or Slack channels.",
        "section": "Project",
        "question": "Can I collaborate with others on the capstone project?"
      },
      {
        "text": "There is only ONE project for this Zoomcamp. You do not need to submit or create two projects.\nThere are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt. Project evaluation - Reproducibility\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\nAlex clarifies: \u201cIdeally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great\u201d\nCertificates: how do I get it?\nA: See the certificate.mdx file",
        "section": "Project",
        "question": "Project 1 & Project 2"
      },
      {
        "text": "See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/projects/datasets.md",
        "section": "Project",
        "question": "Does anyone know nice and relatively large datasets?"
      },
      {
        "text": "You need to redefine the python environment variable to that of your user account",
        "section": "Project",
        "question": "How to run python as start up script?"
      },
      {
        "text": "Initiate a Spark Session\nspark = (SparkSession\n.builder\n.appName(app_name)\n.master(master=master)\n.getOrCreate())\nspark.streams.resetTerminated()\nquery1 = spark\n.readStream\n\u2026\n\u2026\n.load()\nquery2 = spark\n.readStream\n\u2026\n\u2026\n.load()\nquery3 = spark\n.readStream\n\u2026\n\u2026\n.load()\nquery1.start()\nquery2.start()\nquery3.start()\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.",
        "section": "Project",
        "question": "Spark Streaming - How do I read from multiple topics in the same Spark Session"
      },
      {
        "text": "Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.",
        "section": "Project",
        "question": "Data Transformation from Databricks to Azure SQL DB"
      },
      {
        "text": "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py",
        "section": "Project",
        "question": "Orchestrating dbt with Airflow"
      },
      {
        "text": "https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\nGive the following roles to you service account:\nDataProc Administrator\nService Account User (explanation here)\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\nBecause DataProc does not already have the BigQuery Connector.",
        "section": "Project",
        "question": "Orchestrating DataProc with Airflow"
      },
      {
        "text": "You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\nFor example\ndbt_api_trigger=dbt_**\nNavigate to job page and find api trigger  link\nThen create a custom mage Python block with a simple http request like here\nfrom dotenv import load_dotenv\nfrom pathlib import Path\ndotenv_path = Path('/home/src/.env')\nload_dotenv(dotenv_path=dotenv_path)\ndbt_api_trigger= os.getenv(dbt_api_trigger)\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\nheaders = {\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\n        \"Content-Type\": \"application/json\" }\nbody = {\n        \"cause\": \"Triggered via API\"\n    }\n    response = requests.post(url, headers=headers, json=body)\nvoila! You triggered dbt job form your mage pipeline.",
        "section": "Project",
        "question": "Orchestrating dbt cloud with Mage"
      },
      {
        "text": "The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.",
        "section": "Project",
        "question": "Key Vault in Azure cloud stack"
      },
      {
        "text": "The following line should be included in pyspark configuration\n# Example initialization of SparkSession variable\nspark = (SparkSession.builder\n.master(...)\n.appName(...)\n# Add the following configuration\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\n)",
        "section": "Project",
        "question": "How to connect Pyspark with BigQuery?"
      },
      {
        "text": "Install the astronomer-cosmos package as a dependency. (see Terraform example).\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\nYour dbt lineage graph should now appear as tasks inside a task group like this:",
        "section": "Project",
        "question": "How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key"
      },
      {
        "text": "To avoid reinstalling uv on each flow run, you can create a custom Docker image based on the official Kestra image with uv pre-installed. Here's how:\nCreate a Dockerfile (e.g., Dockerfile) with the following content:\nUpdate your docker-compose.yml to build this custom image instead of pulling the default one:\nThis approach ensures that uv is available in the container at runtime without requiring installation during each flow execution.",
        "section": "Project",
        "question": "How can I run UV in Kestra without installing it on every flow execution?"
      },
      {
        "text": "Answer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.",
        "section": "Project",
        "question": "Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?"
      },
      {
        "text": "No",
        "section": "Project",
        "question": "Is it ok to use NY_Taxi data for the project?"
      },
      {
        "text": "If you don\u2019t have access to dbt Cloud which is already natively being supported by AWS, refer here: 1, 2, 3, & 4, you can use the community built dbt-Athena Adapter for dbt-Core.\nKey Features\nEnables dbt to work with AWS Athena using dbt Core\nAllows data transformation using CREATE TABLE AS or CREATE VIEW SQL queries\nNot yet supported features:\nPython models\nPersisting documentation for views\nThis adapter can be a valuable resource for those who need to work with Athena using dbt Core, and I hope this entry can help others discover it.",
        "section": "Project",
        "question": "How to use dbt-core with Athena?"
      },
      {
        "text": "When working on a dbt-Athena project, do not install dbt-athena-adapter. Instead, always use the dbt-athena-community package, ensuring it matches the version of dbt-core to avoid compatibility conflicts.\nBest Practice\nAlways pin the versions of dbt-core and dbt-athena-community to the same version.\nExample:\n\n dbt-core~=1.9.3\n\n dbt-athena-community~=1.9.3\nWhy?\ndbt-athena-adapter is outdated and no longer maintained.\ndbt-athena-community is the actively maintained package and is compatible with the latest versions of dbt-core.\nSteps to Avoid Conflicts\nAlways check the compatibility matrix in the dbt-athena-community GitHub repository.\nUpdate requirements.txt to use the latest compatible versions of dbt-core and dbt-athena-community.\nAvoid mixing dbt-athena-adapter with dbt-athena-community in the same environment.\nBy following this practice, you can avoid the conflicts we faced previously and ensure a smooth development experience.\nTechnically you can use any code editor or Jupyter Notebook, as long as you can run dbt and answer the homework questions. A lot of code is provided by the instructor, on the homework page to give you a headstart in the right direction: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2025/workshops/dlt/dlt_homework.md\nThe most practical way is to use the provided Colabs Jupyter notebook called \u2018dlt - Homework.ipynb\u2019 which you can find here below, since all of the provided code is applicable in the Colabs set-up: https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7#scrollTo=BtsSxtFfXQs3",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Solving dbt-Athena library conflicts"
      },
      {
        "text": "Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command in a jupyter notebook: !pip install dlt[duckdb]. If you\u2019re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\nin zsh try:\npip install \u201cdlt[duckdb]\u201d",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "How do I install the necessary dependencies to run the code?"
      },
      {
        "text": "If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\n\nInstall all the necessary dependencies\npip install duckdb pandas numpy pyarrow\nOr save it into a requirements.txt file:\ndlt[duckdb]\nduckdb\npandas\nnumpy\npyarrow  # Optional, needed for Parquet support\nThen run pip install -r requirements.txt",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Other packages needed but not listed"
      },
      {
        "text": "Alternatively, you can switch to in-file storage with:",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "How can I use DuckDB In-Memory database with dlt ?"
      },
      {
        "text": "After loading, you should have a total of 8 records, and ID 3 should have age 33\nQuestion: Calculate the sum of ages of all the people loaded as described above\nThe sum of all eight records' respective ages is too big to be in the choices. You need to first filter out the people whose occupation is equal to None in order to get an answer that is close to or present in the given choices. \ud83d\ude03\n----------------------------------------------------------------------------------------\nFIXED = use a raw string and keep the file:/// at the start of your file path\nI'm having an issue with the dlt workshop notebook. The 'Load to Parquet file' section specifically. No matter what I change the file path to, it's still saving the dlt files directly to my C drive.\n# Set the bucket_url. We can also use a local folder\nos.environ['DESTINATION__FILESYSTEM__BUCKET_URL'] = r'file:///content/.dlt/my_folder'\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n# Define your pipeline\npipeline = dlt.pipeline(\npipeline_name='my_pipeline',\ndestination='filesystem',\ndataset_name='mydata'\n)\n# Run the pipeline with the generator we created earlier.\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\nprint(load_info)\n# Get a list of all Parquet files in the specified folder\nparquet_files = glob.glob('/content/.dlt/my_folder/mydata/users/*.parquet')\n# show parquet files\nfor file in parquet_files:\nprint(file)",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Homework - dlt Exercise 3 - Merge a generator concerns"
      },
      {
        "text": "Make sure you don\u2019t have a dlt.py file saved in the same directory as your working file.",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Problem with importing the dlt or dlt.sources module"
      },
      {
        "text": "In the secrets sidebar, create a secret \u2018BIGQUERY_CRENTIALS\u2019 with value being your Google Cloud service account key. Then load it with:\nimport os\nfrom google.colab import userdata\nos.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = userdata.get('BIGQUERY_CREDENTIALS')",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "How to set credentials in Google Colab notebook to connect to BigQuery"
      },
      {
        "text": "You can set up credentials for `dlt` in several ways. Here are the two most common methods:\nEnvironment Variables (Easiest)\nSet credentials via environment variables. For example, to configure Google Cloud credentials:\nThis method avoids hardcoding secrets in your code and works seamlessly with most environments.\nConfiguration Files (Recommended for Local Use)\nUse `.dlt/secrets.toml` for sensitive credentials and `.dlt/config.toml` for non-sensitive configurations.\nExample for Google Cloud in `secrets.toml`:\nPlace these files in the .dlt folder of your project.\nAdditional Notes:\nNever commit secrets.toml to version control (add it to .gitignore).\nCredentials can also be loaded via vaults, AWS Parameter Store, or custom setups.\nFor additional methods and detailed information, refer to the official dlt documentation",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "How do I set up credentials to run dlt in my environment (not Google Colab)?"
      },
      {
        "text": "You can set the environment variable in your shell init script (for Bash or ZSH):\nexport DLT_DATA_DIR=$XDG_DATA_HOME/dlt\nOr for Fish (in config.fish):\nset -x DLT_DATA_DIR \u201c$XDG_DATA_HOME/dlt\u201d",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Make DLT comply with the XDG Base Dir Specification"
      },
      {
        "text": "from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime, timedelta\nimport dlt\nfrom my_dlt_pipeline import load_data  # Import your dlt pipeline function\ndefault_args = {\n\"owner\": \"airflow\",\n\"depends_on_past\": False,\n\"start_date\": datetime(2024, 2, 16),\n\"retries\": 1,\n\"retry_delay\": timedelta(minutes=5),\n}\ndef run_dlt_pipeline():\npipeline = dlt.pipeline(\npipeline_name=\"my_pipeline\",\ndestination=\"duckdb\",  # Change this based on your database\ndataset_name=\"my_dataset\"\n)\ninfo = pipeline.run(load_data())\nprint(info)  # Logs for debugging\nwith DAG(\n\"dlt_airflow_pipeline\",\ndefault_args=default_args,\nschedule_interval=\"@daily\",\ncatchup=False,\n) as dag:\nrun_dlt_task = PythonOperator(\ntask_id=\"run_dlt_pipeline\",\npython_callable=run_dlt_pipeline,\n)\nrun_dlt_task",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Embedding dlt into Apache Airflow"
      },
      {
        "text": "id: dlt_ingestion\nnamespace: my.dlt\ndescription: \"Run dlt pipeline with Kestra\"\ntasks:\n- id: run_dlt\ntype: io.kestra.plugin.scripts.python.Commands\ncommands:\n- |\nimport dlt\nfrom my_dlt_pipeline import load_data  # Import your dlt function\npipeline = dlt.pipeline(\npipeline_name=\"kestra_pipeline\",\ndestination=\"duckdb\",\ndataset_name=\"kestra_dataset\"\n)\ninfo = pipeline.run(load_data())\nprint(info)",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Embedding dlt into Kestra"
      },
      {
        "text": "When using the filesystem destination, you may have issues reading the files exported because dlt will by default compress the files. \n\nIf you are using loader_file_format=\"parquet\" then BigQuery should cope with this compression OK. If you want to use jsonl or csv format however, then you may need to disable file compression to avoid issues with reading the files directly in BigQuery. To do this set the following config:\n\n[normalize.data_writer]\ndisable_compression = true   There is further information at https://dlthub.com/docs/dlt-ecosystem/destinations/filesystem#file-compression\n[WARNING]: Test 'test.taxi_rides_ny.relationships_stg_yellow_tripdata_dropoff_locationid__locationid__ref_taxi_zone_lookup_csv_.085c4830e7' (models/staging/schema.yml) depends on a node named 'taxi_zone_lookup.csv' in package '' which was not found\nsolve: This warning indicates that dbt is trying to reference a model or source named taxi_zone_lookup.csv, but it cannot find it. We might have a typo in our ref() function.\ntests:\n- name: relationships_stg_yellow_tripdata_dropoff_locationid\ndescription: \"Ensure dropoff_location_id exists in taxi_zone_lookup.csv\"\nrelationships:\nto: ref('taxi_zone_lookup.csv')  # \u274c Wrong reference\nfield: locationid\nto:\nto: ref('taxi_zone_lookup')  # \u2705 Correct reference\nWhen I ran df_spark = spark.createDataFrame(df_pandas), I encountered an error indicating a version mismatch between Pandas and Spark. To resolve this, I had two options: either downgrade Pandas to a version below 2 or upgrade Spark to version 3.5.5. I chose to upgrade Spark to 3.5.5, and it worked.\nAvoiding Backpressure in Flink\nWhat\u2019s Backpressure?\nIt happens when Flink processes data slower than Kafka produces it.\nThis leads to increased memory usage and can slow down or crash the job.\nHow to Fix It?\nAdjust Kafka\u2019s consumer parallelism to match the producer rate.\nIncrease partitions in Kafka to allow more parallel processing.\nMonitor Flink metrics to detect backpressure.\npython\nCopyEdit\nenv.set_parallelism(4)  # Adjust parallelism to avoid bottlenecks",
        "section": "Workshop 1 - dlthub\n \nWhich set-up should I use for my dlt homework?",
        "question": "Loading Dlt Exports from GCS Filesystems"
      }
    ]
  },
  {
    "course": "machine-learning-zoomcamp",
    "documents": [
      {
        "text": "Machine Learning Zoomcamp FAQ\nPython\nMachine Learning Zoomcamp FAQ\nThe purpose of this document is to capture frequently asked technical questions.\nEditing guidelines:\nWhen adding a new FAQ entry, make sure the question is \u201cHeading 2\u201d\nFeel free to improve if you see something is off\nDon\u2019t change the formatting in the document or add any visual \u201cimprovements\u201d\nDon\u2019t change the pages format (it should be \u201cpageless\u201d)\nAdd name and date at the end of your entry, if possible\nProblem description\nSolution description\n(optional) Added by NAME on DATE",
        "section": "General course-related question review",
        "question": "Problem title"
      },
      {
        "text": "In the course GitHub repository there\u2019s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo",
        "section": "General course-related question review",
        "question": "How do I sign up?"
      },
      {
        "text": "Potentially September 2025\nThe course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).\nThe zoomcamps are spread out throughout the year. See article https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html",
        "section": "General course-related question review",
        "question": "When is the next cohort? When does the next iteration start?"
      },
      {
        "text": "The course videos are pre-recorded, you can start watching the course right now.\nWe will also occasionally have office hours we will answer your questions. The office hours sessions are recorded too. -  hours (plive sessions where\nYou can see the officelaylist with year 20xx) as well as the pre-recorded course videos in (playlist without year) in Course Channel\u2019s Bookmarks and/or DTC\u2019s youtube channel",
        "section": "General course-related question review",
        "question": "Is it going to be live? When?"
      },
      {
        "text": "Everything is recorded, so you won\u2019t miss  office ho\u00cfurs ianything. You will be able to ask your questions forn advance and we will cover them during the live stream. Also, you can always ask questions in Slack.",
        "section": "General course-related question review",
        "question": "What if I miss a session?"
      },
      {
        "text": "The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.",
        "section": "General course-related question review",
        "question": "How much theory will you cover?"
      },
      {
        "text": "Math is not strictly a prerequisite to start learning machine learning (ML), but having a strong foundation in certain mathematical concepts can significantly improve your understanding and ability to work with ML models.\nYes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\nHere are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any questions.\n(M\u00e9lanie Fouesnard)",
        "section": "General course-related question review",
        "question": "I don't know math. Can I take the course?"
      },
      {
        "text": "The process is automated now, so you should receive the email eventually. If you haven\u2019t, check your promotions tab in Gmail as well as spam.\nIf you unsubscribed from our newsletter, you won't get course related updates too.\nBut don't worry, it\u2019s not a problem. To make sure you don\u2019t miss anything, join the #course -ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.",
        "section": "General course-related question review",
        "question": "I filled the form, but haven't received a confirmation email. Is it normal?"
      },
      {
        "text": "Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)",
        "section": "General course-related question review",
        "question": "How long is the course?"
      },
      {
        "text": "Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article",
        "section": "General course-related question review",
        "question": "How much time do I need for this course?"
      },
      {
        "text": "Yes, if you finish at least 2 out of 3 projects and review 3 peers\u2019 Projects by the deadline, you will get a certificate. This is what it looks like: k.lin",
        "section": "General course-related question review",
        "question": "Will I get a certificate?"
      },
      {
        "text": "Yes, it's possible. See the previous answer.",
        "section": "General course-related question review",
        "question": "Will I get a certificate if I missed the midterm project?"
      },
      {
        "text": "Check this article. If you know everything in this article, you know enough. If you don\u2019t, read the article and join the course Introduction to Python too :)\nIntroduction to Python \u2013 Machine Learning Bootcamp\nYou can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\n(M\u00e9lanie Fouesnard)",
        "section": "General course-related question review",
        "question": "How much Python should I know?"
      },
      {
        "text": "For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\n(Rileen Sinha; based on response by Alexey on Slack)",
        "section": "General course-related question review",
        "question": "Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ."
      },
      {
        "text": "Here\u2019s how you join in Slack: https://slack.com/help/articles/205239967-Join-a-channel\nClick \u201cAll channels\u201d at the top of your left sidebar. If you don't see this option, click \u201cMore\u201d to find it.\nBrowse the list of public channels in your workspace, or use the search bar to search by channel name or description.\nSelect a channel from the list to view it.\nClick Join Channel.\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\nYes. You are required to provide the URL to your repo in order to receive a grade",
        "section": "General course-related question review",
        "question": "I\u2019m new to Slack and can\u2019t find the course channel. Where is it?"
      },
      {
        "text": "Yes, you can. Even though you missed the start date, you can register for the course. You won\u2019t be able to submit some of the homeworks, but you can still take part in the course.\nIn order to get a certificate, you need to submit 2 out of 3 course projects an Projects by the deadlind review 3 peers\u2019e. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.",
        "section": "General course-related question review",
        "question": "The course has already started. Can I still join it?"
      },
      {
        "text": "No, it\u2019s not possible. The form is closed after the due date. But don\u2019t worry, homework is not mandatory for finishing the course.",
        "section": "General course-related question review",
        "question": "Can I submit the homework after the due date?"
      },
      {
        "text": "Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everythingGitHub - DataTalksClub/machine-learning-zoomcamp: Learn ML engineering for free in 4 months! in the cohort folder for your cohort\u2019s year.\nClick on the links and start watching the videos. Also watch office hours from previous cohorts. Go to the DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\nOr you can just use this link: http://mlzoomcamp.com/#syllabus",
        "section": "General course-related question review",
        "question": "I just joined. What should I do next? How can I access course materials?"
      },
      {
        "text": "For the 2024 cohort, you can see the deadlines here (it\u2019s taken from the 2024 cohort page) or in google calendar",
        "section": "General course-related question review",
        "question": "What are the deadlines in this course?"
      },
      {
        "text": "No, you need to do projects individually but it\u2019s ok to partner up to discuss weekly lectures or exchange ideas.",
        "section": "General course-related question review",
        "question": "Could I partner up for the mid and final projects?"
      },
      {
        "text": "There\u2019s not much difference. There was one special module (BentoML) in the previous iteration of the course, but the rest of the modules are the same as in 2022. The homework this year is different.",
        "section": "General course-related question review",
        "question": "What\u2019s the difference between the previous iteration of the course (2022) and this one (2023)?"
      },
      {
        "text": "We won\u2019t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\nIf you haven\u2019t taken part in the previous iteration, you can start watching the videos. It\u2019ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.",
        "section": "General course-related question review",
        "question": "The course videos are from the previous iteration. Will you release new ones or we\u2019ll use the videos from 2021?"
      },
      {
        "text": "When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there\u2019s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\nFor posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you\u2019ll get only 7 points.\nThe same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey\u2019s reply. (~ ellacharmed)\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.",
        "section": "General course-related question review",
        "question": "Submitting learning in public links"
      },
      {
        "text": "We kindly ask you not to share your answers",
        "section": "General course-related question review",
        "question": "Can I share my answers of the Homework with the community to compare before I submit them?"
      },
      {
        "text": "You can create your own github repository for the course with your notes, homework, projects, etc.\nThen fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\nAfter that's done, create a pull request to sync your fork with the original course repo.\n(By Wesley Barreto)",
        "section": "General course-related question review",
        "question": "Adding community notes"
      },
      {
        "text": "Leaderboard Links:\n2024 - https://courses.datatalks.club/ml-zoomcamp-2024/leaderboard\n2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\n2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\nPython Code:\nfrom hashlib import sha1\ndef compute_hash(email):\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\nYou need to call the function as follows:\nprint(compute_hash('YOUR_EMAIL_HERE'))\nThe quotes are required to denote that your email is a string.\n(By Wesley Barreto)\nYou can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the \u201cresearch\u201d bar of the leaderboard to get your scores.\n(M\u00e9lanie Fouesnard)",
        "section": "1. Introduction to Machine Learning",
        "question": "Computing the hash for the leaderboard and project review"
      },
      {
        "text": "If you get \u201cwget is not recognized as an internal or external command\u201d, you need to install it.\nOn Ubuntu, run\nsudo apt-get install wget\nOn Windows, the easiest way to install wget is to use Chocolatey:\nchoco install wget\nOr you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\nOn Mac, the easiest way to install wget is to use brew.\nBrew install wget\nAlternatively, you can use a Python wget library, but instead of simply using \u201cwget\u201d you\u2019ll need to use\npython -m wget\nYou need to install it with pip first:\npip install wget\nAnd then in your python code, for example in your jupyter notebook, use:\nimport wget\nwget.download(\"URL\")\nThis should download whatever is at the URL in the same directory as your code.\n(Memoona Tahira)\nAlternatively, you can read a CSV file from a URL directly with pandas:\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\ndf = pd.read_csv(url)\nValid URL schemes include http, ftp, s3, gs, and file.\nIn some cases you might need to bypass https checks:\nimport ssl\nssl._create_default_https_context = ssl._create_unverified_context\nOr you can use the built-in Python functionality for downloading the files:\nimport urllib.request\nurl = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\nurllib.request.urlretrieve(url, \"housing.csv\")\nUrllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\nThe urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\nOn any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\n(Mohammad Emad Sharifi)",
        "section": "1. Introduction to Machine Learning",
        "question": "wget is not recognized as an internal or external command"
      },
      {
        "text": "You can use\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\nTo download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\n!mkdir -p ../data/\n!mv housing.csv ../data/",
        "section": "1. Introduction to Machine Learning",
        "question": "Retrieving csv inside notebook"
      },
      {
        "text": "(Tyler Simpson)",
        "section": "1. Introduction to Machine Learning",
        "question": "Windows WSL and VS Code\nIf you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension \u2018WSL\u2019 this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine."
      },
      {
        "text": "This is my first time using Github to upload a code. I was getting the below error message when I type\ngit push -u origin master:\nerror: src refspec master does not match any\nerror: failed to push some refs to 'https://github.com/XXXXXX/1st-Homework.git'\nSolution:\nThe error message got fixed by running below commands:\ngit commit -m \"initial commit\"\ngit push origin main\nIf this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\n(Asia Saeed)\nYou can also use the \u201cupload file\u201d functionality from GitHub for that\nIf you write your code on Google colab you can also directly share it on your Github.\n(By Pranab Sarma)",
        "section": "1. Introduction to Machine Learning",
        "question": "Uploading the homework to Github"
      },
      {
        "text": "I'm trying to invert the matrix but I got error that the matrix is singular matrix\nThe singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.",
        "section": "1. Introduction to Machine Learning",
        "question": "Singular Matrix Error"
      },
      {
        "text": "I have a problem with my terminal. Command\nconda create -n ml-zoomcamp python=3.9\ndoesn\u2019t work. Any of 3.8/ 3.9 / 3.10 should be all fine\nIf you\u2019re on Windows and just installed Anaconda, you can use Anaconda\u2019s own terminal called \u201cAnaconda Prompt\u201d.\nIf you don\u2019t have Anaconda or Miniconda, you should install it first\n(Tatyana Mardvilko)",
        "section": "1. Introduction to Machine Learning",
        "question": "Conda is not an internal command"
      },
      {
        "text": "How do I read the dataset with Pandas in Windows?\nI used the code below but not working\ndf = pd.read_csv('C:\\Users\\username\\Downloads\\data.csv')\nUnlike Linux/Mac OS, Windows uses the backslash (\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the '\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\n\" to add a new line or \"\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\nHere\u2019s how we should be loading the file instead:\ndf = pd.read_csv(r'C:\\Users\\username\\Downloads\\data.csv')\n(Muhammad Awon)",
        "section": "1. Introduction to Machine Learning",
        "question": "Read-in the File in Windows OS"
      },
      {
        "text": "Type the following command:\ngit config -l | grep url\nThe output should look like this:\nremote.origin.url=https://github.com/github-username/github-repository-name.git\nChange this to the following format and make sure the change is reflected using command in step 1:\ngit remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\n(Added by Dheeraj Karra)",
        "section": "1. Introduction to Machine Learning",
        "question": "'403 Forbidden' error message when you try to push to a GitHub repository"
      },
      {
        "text": "I had a problem when I tried to push my code from Git Bash:\nremote: Support for password authentication was removed on August 13, 2021.\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\nfatal: Authentication failed for 'https://github.com/username\nSolution:\nCreate a personal access token from your github account and use it when you make a push of your last changes.\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\nBruno Bed\u00f3n",
        "section": "1. Introduction to Machine Learning",
        "question": "Fatal: Authentication failed for 'https://github.com/username"
      },
      {
        "text": "In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\nGetting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\nwget: unable to resolve host address 'raw.githubusercontent.com'\nSolution:\nIn your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.",
        "section": "1. Introduction to Machine Learning",
        "question": "wget: unable to resolve host address 'raw.githubusercontent.com'"
      },
      {
        "text": "I found this video quite helpful: Creating Virtual Environment for Python from VS Code\n[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a \u2018jupyter notebook \u2018 command from a remote machine + have a remote connection configured in .ssh/config (as Alexey\u2019s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\n[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code\u2019s UI:\nhttps://code.visualstudio.com/docs/sourcecontrol/overview\n(Added by Ivan Brigida)",
        "section": "1. Introduction to Machine Learning",
        "question": "Setting up an environment using VS Code"
      },
      {
        "text": "Forward your ports in .ssh.\nType \u201cnano .ssh/config\u201d\nAnd add \u201cLocalForward 8888 localhost:8888\u201d to forward your Jupyter.\n(Added by Ico)",
        "section": "1. Introduction to Machine Learning",
        "question": "If you prefer the terminal to work, port forward in your config file."
      },
      {
        "text": "With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\nAnswer:\n\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\n(Added by Wesley Barreto)\nconda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml",
        "section": "1. Introduction to Machine Learning",
        "question": "Conda Environment Setup"
      },
      {
        "text": "I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\nInverse * Original:\n[[ 1.00000000e+00 -1.38777878e-16]\n[ 3.16968674e-13  1.00000000e+00]]\nSolution:\nIt's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\n(Added by Wesley Barreto)",
        "section": "1. Introduction to Machine Learning",
        "question": "Floating Point Precision"
      },
      {
        "text": "Answer:\nIt prints the information about the dataset like:\nIndex datatype\nNo. of entries\nColumn information with not-null count and datatype\nMemory usage by dataset\nWe use it as:\ndf.info()\n(Added by Aadarsha Shrestha & Emoghena Itakpe)",
        "section": "1. Introduction to Machine Learning",
        "question": "What does pandas.DataFrame.info() do?"
      },
      {
        "text": "Pandas and numpy libraries are not being imported\nNameError: name 'np' is not defined\nNameError: name 'pd' is not defined\nIf you're using numpy or pandas, make sure you use the first few lines before anything else.\nimport pandas as pd\nimport numpy as np\nAdded by Manuel Alejandro Aponte",
        "section": "1. Introduction to Machine Learning",
        "question": "NameError: name 'np' is not defined"
      },
      {
        "text": "What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\ndf.select_dtypes(include=np.number).columns.tolist()\ndf.select_dtypes(include='object').columns.tolist()\nAdded by Gregory Morris",
        "section": "1. Introduction to Machine Learning",
        "question": "How to select column by dtype"
      },
      {
        "text": "There are many ways to identify the shape of dataset, one of them is using .shape attribute!\ndf.shape\ndf.shape[0] # for identify the number of rows\ndf.shape[1] # for identify the number of columns\nWe can also use the built-in `len` function to find the total number of rows in a dataframe, just as shown below:\nlen(df)\nAdded by Radikal Lukafiardi",
        "section": "1. Introduction to Machine Learning",
        "question": "How to identify the shape of dataset in Pandas"
      },
      {
        "text": "First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\nDimension Mismatch\nTo perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\nAdded by Leah Gotladera",
        "section": "1. Introduction to Machine Learning",
        "question": "How to avoid Value errors with array shapes in homework?"
      },
      {
        "text": "You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\nThis method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\nAdded by Anneysha Sarkar",
        "section": "1. Introduction to Machine Learning",
        "question": "Question 5: How and why do we replace the NaN values with average of the column?"
      },
      {
        "text": "When you calculate the mode using the mode() function in pandas, the function always returns a Series. This design choice allows mode() to handle cases where there may be multiple modes (i.e., multiple values with the same highest frequency). Even when there is only one mode, the function will still return a Series with that single value.\nIf you are certain that your column has only one mode and you want to extract it as a single value, you can access the first element of the Series returned by mode():\n(added by Karina)",
        "section": "1. Introduction to Machine Learning",
        "question": "Question 5: Why is the mode returned as a Series instead of a single value in my DataFrame?"
      },
      {
        "text": "It simply means all laptops of brand \u201cDell\u201d\nAdded by Nilesh Arte",
        "section": "1. Introduction to Machine Learning",
        "question": "Question 5 (Homework small note): Do not get confused by word dell \u201cnotebook\u201d in question"
      },
      {
        "text": "In Question 7 we are asked to calculate\nThe initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\nAdditional reading and videos:\nOrdinary least squares\nMultiple Linear Regression in Matrix Form\nPseudoinverse Solution to OLS\nAdded by Sylvia Schmitt\nwith commends from Dmytro Durach",
        "section": "1. Introduction to Machine Learning",
        "question": "Question 7: Mathematical formula for linear regression"
      },
      {
        "text": "This is most likely that you interchanged the first step of the multiplication\nYou used  instead of\nAdded by Emmanuel Ikpesu",
        "section": "1. Introduction to Machine Learning",
        "question": "Question 7: FINAL MULTIPLICATION not having 5 column"
      },
      {
        "text": "Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\nnumpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\nIf multiplying by a scalar numpy.multiply() or * is preferred.\nAdded by Andrii Larkin",
        "section": "1. Introduction to Machine Learning",
        "question": "Question 7: Multiplication operators."
      },
      {
        "text": "If you face an error kind of ImportError: cannot import name 'contextfilter' from 'jinja2' (anaconda\\lib\\site-packages\\jinja2\\__init__.py) when launching a new notebook for a brand new environment.\nSwitch to the main environment and run \"pip install nbconvert --upgrade\".\nAdded by George Chizhmak",
        "section": "1. Introduction to Machine Learning",
        "question": "Error launching Jupyter notebook"
      },
      {
        "text": "If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again",
        "section": "1. Introduction to Machine Learning",
        "question": "wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1"
      },
      {
        "text": "Wget doesn't ship with macOS, so there are other alternatives to use.\nNo worries, we got curl:\nexample:\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\nExplanations:\ncurl: a utility for retrieving information from the internet.\n-o: Tell it to store the result as a file.\nfilename: You choose the file's name.\nLinks: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\nMore about it at:\nCurl Documentation\nAdded by David Espejo",
        "section": "1. Introduction to Machine Learning",
        "question": "In case you are using mac os and having trouble with WGET"
      },
      {
        "text": "You can use round() function or f-strings\nround(number, 4)  - this will round number up to 4 decimal places\nprint(f'Average mark for the Homework is {avg:.3f}') - using F string\nAlso there is pandas.Series. round idf you need to round values in the whole Series\nPlease check the documentation\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\nAdded by Olga Rudakova",
        "section": "1. Introduction to Machine Learning",
        "question": "How to output only a certain number of decimal places"
      },
      {
        "text": "If you\u2019re struggling to get a Jupyter notebook running locally on your machine, or you have some other issue preventing you from using it (perhaps you\u2019ve only got a cellphone to do your homework with at the moment?) then other online platforms you can consider as a stop gap measure which don\u2019t require anything to be installed are:\nJupyterLab https://jupyter.org/try-jupyter/lab/\nJupyterLite https://jupyterlite.github.io/demo/lab/index.html\nReplit https://replit.com/\nGoogle Colab https://colab.research.google.com/\nAdded by David Peterson",
        "section": "1. Introduction to Machine Learning",
        "question": "Can\u2019t get Juypter running locally on your machine?"
      },
      {
        "text": "To avoid accidentally pushing CSV files (or any specific file type) to a Git repository, you can use a .gitignore file.\nAdd a rule to ignore CSV files   *.csv\nIf the CSV files have already been committed,you can remove them from Git tracking but keep them locally by using command\ngit.rm \u2013-cached filename.csv\nAdded by Olga Rudakova",
        "section": "2. Machine Learning for Regression",
        "question": "How to avoid accidentally pushing CSV files"
      },
      {
        "text": "Here are the crucial links for this Week 2 that starts September 18, 2023\nAsk questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\nCalendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\nWeek 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\nSubmit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\nYoutube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n~~Nukta Bhatia~~",
        "section": "2. Machine Learning for Regression",
        "question": "How do I get started with Week 2?"
      },
      {
        "text": "We can use histogram:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Load the data\nurl = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\ndf = pd.read_csv(url)\n# EDA\nsns.histplot(df['median_house_value'], kde=False)\nplt.show()\nOR ceck skewness and describe:\nprint(df['median_house_value'].describe())\n# Calculate the skewness of the 'median_house_value' variable\nskewness = df['median_house_value'].skew()\n# Print the skewness value\nprint(\"Skewness of 'median_house_value':\", skewness)\n(Mohammad Emad Sharifi)",
        "section": "2. Machine Learning for Regression",
        "question": "Checking long tail of data"
      },
      {
        "text": "It\u2019s possible that when you follow the videos, you\u2019ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don\u2019t worry, it\u2019s normal that you have it.\nYou can also have an error because you did the inverse of X once in your code and you\u2019re doing it a second time.\n(Added by C\u00e9cile Guillot)",
        "section": "2. Machine Learning for Regression",
        "question": "LinAlgError: Singular matrix"
      },
      {
        "text": "You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\nKS",
        "section": "2. Machine Learning for Regression",
        "question": "California housing dataset"
      },
      {
        "text": "I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\nI found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\nAdded by Sasmito Yudha Husada",
        "section": "2. Machine Learning for Regression",
        "question": "Getting NaNs after applying .mean()"
      },
      {
        "text": "Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\nThis can help to understand skewness and how it can be applied to the distribution of your data set.\nhttps://en.wikipedia.org/wiki/Skewness\nPastor Soto",
        "section": "2. Machine Learning for Regression",
        "question": "Target variable transformation"
      },
      {
        "text": "The dataset can be read directly to pandas dataframe from the github link using the technique shown below\ndfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\nKrishna Anand",
        "section": "2. Machine Learning for Regression",
        "question": "Reading the dataset directly from github"
      },
      {
        "text": "For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\nOnce the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\ndf = pd.read_csv('housing.csv')\nHarish Balasundaram",
        "section": "2. Machine Learning for Regression",
        "question": "Loading the dataset directly through Kaggle Notebooks"
      },
      {
        "text": "We can filter a dataset by using its values as below.\ndf = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\nYou can use | for \u2018OR\u2019, and & for \u2018AND\u2019\nAlternative:\ndf = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\nRadikal Lukafiardi",
        "section": "2. Machine Learning for Regression",
        "question": "Filter a dataset by using its values"
      },
      {
        "text": "Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\n# Get data for homework\nimport requests\nurl = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\nresponse = requests.get(url)\nif response.status_code == 200:\nwith open('housing.csv', 'wb') as file:\nfile.write(response.content)\nelse:\nprint(\"Download failed.\")\nTyler Simpson",
        "section": "2. Machine Learning for Regression",
        "question": "Alternative way to load the data using requests"
      },
      {
        "text": "When creating a duplicate of your dataframe by doing the following:\nX_train = df_train\nX_val = df_val\nYou\u2019re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\nX_train = df_train.copy()\nX_val = df_val.copy()\nAdded by Ixchel Garc\u00eda",
        "section": "2. Machine Learning for Regression",
        "question": "Null column is appearing even if I applied .fillna()"
      },
      {
        "text": "Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn\u2019s functions. If you want to start using it earlier \u2014 feel free to do it",
        "section": "2. Machine Learning for Regression",
        "question": "Can I use Scikit-Learn\u2019s train_test_split for this week?"
      },
      {
        "text": "Yes, you can. We will also do that next week, so don\u2019t worry, you will learn how to do it.",
        "section": "2. Machine Learning for Regression",
        "question": "Can I use LinearRegression from Scikit-Learn for this week?"
      },
      {
        "text": "What are equivalents in Scikit-Learn for the linear regression with and without regularization used in week 2.\nCorresponding function for model without regularization:\nsklearn.linear_model.LinearRegression\nCorresponding function for model with regularization:\nsklearn.linear_model.Ridge\nThe linear model from Scikit-Learn are explained  here:\nhttps://scikit-learn.org/stable/modules/linear_model.html\nAdded by Sylvia Schmitt",
        "section": "2. Machine Learning for Regression",
        "question": "Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization)"
      },
      {
        "text": "In the context of regression, particularly with regularization:\nr typically represents the regularization parameter in some algorithms. It controls the strength of the penalty applied to the coefficients of the regression model to prevent overfitting.\nIn sklearn.Ridge(), the parameter alpha serves the same purpose as r. It specifies the amount of regularization applied to the model. A higher value of alpha increases the amount of regularization, which can reduce model complexity and improve generalization.\n`r` is a regularization parameter.\nIt\u2019s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here's how both are used:\nsklearn.Ridge()\n||y - Xw||^2_2 + alpha * ||w||^2_2\nlesson\u2019s notebook (`train_linear_regression_reg` function)\nXTX = XTX + r * np.eye(XTX.shape[0])\n`r` adds \u201cnoise\u201d to the main diagonal to prevent multicollinearity, which \u201cbreaks\u201d finding inverse matrix.",
        "section": "2. Machine Learning for Regression",
        "question": "Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?"
      },
      {
        "text": "linear regression often provides a good approximation of the underlying relationship but rarely achieves a \"perfect\" fit in real-world applications.\nQ: \u201cIn lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?\u201d\nA: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\nAs our model is linear, how would you draw a line to fit all the \"dots\"?\nYou could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\nAdded by Andrii Larkin",
        "section": "2. Machine Learning for Regression",
        "question": "Why linear regression doesn\u2019t provide a \u201cperfect\u201d fit?"
      },
      {
        "text": "One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.",
        "section": "2. Machine Learning for Regression",
        "question": "Random seed 42"
      },
      {
        "text": "It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\nSetting frac=1 will result in returning a shuffled version of the complete Dataset.\nSetting random_state=seed will result in the same randomization as used in the course resources.\ndf_shuffled = df.sample(frac=1, random_state=seed)\ndf_shuffled.reset_index(drop=True, inplace=True)\nAdded by Sylvia Schmitt",
        "section": "2. Machine Learning for Regression",
        "question": "Shuffling the initial dataset using pandas built-in function"
      },
      {
        "text": "While the lectures have you use the shuffle function to shuffle the index of the dataframe, it no longer accepts random seed as a parameter. This is because Numpy converted this feature into its own \u201cGenerator Class\u201d. In order to assign the random generator a seed, you have to specify the object (rng) that you are going to utilize in your code:\n#Create index from range of values in array\nidx = np.arange(n)\n#Create random generator object and set seed\nrng = np.random.default_rng(random_seed)\n#Shuffle values using Generator object\nrng.shuffle(idx)\nAdded by Emmanuel Lopez",
        "section": "2. Machine Learning for Regression",
        "question": "Shuffling data using Numpy\u2019s Generator Feature"
      },
      {
        "text": "That\u2019s normal. We all have different environments: our computers have different versions of OS and different versions of libraries \u2014 even different versions of Python.\nIf it\u2019s the case, just select the option that\u2019s closest to your answer",
        "section": "2. Machine Learning for Regression",
        "question": "The answer I get for one of the homework questions doesn't match any of the options. What should I do?"
      },
      {
        "text": "In question 3 of HW02 it is mentioned: \u2018For computing the mean, use the training only\u2019. What does that mean?\nIt means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\ndf_train['column_name'].mean( )\nAnother option:\ndf_train[\u2018column_name\u2019].describe()\n(Bhaskar Sarma)",
        "section": "2. Machine Learning for Regression",
        "question": "Meaning of mean in homework 2, question 3"
      },
      {
        "text": "When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work",
        "section": "2. Machine Learning for Regression",
        "question": "When should we transform the target variable to logarithm distribution?"
      },
      {
        "text": "If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\nIf this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\n(Santhosh Kumar)",
        "section": "2. Machine Learning for Regression",
        "question": "ValueError: shapes not aligned"
      },
      {
        "text": "Copy of a dataframe is made with X_copy = X.copy().\nThis is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\nAny changes to X_copy will reflect back to X. This is not a real copy, instead it is a \u201cview\u201d.\n(Memoona Tahira)",
        "section": "2. Machine Learning for Regression",
        "question": "How to copy a dataframe without changing the original dataframe?"
      },
      {
        "text": "One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\n(Tatiana D\u00e1vila)",
        "section": "2. Machine Learning for Regression",
        "question": "What does \u2018long tail\u2019 mean?"
      },
      {
        "text": "In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\n(Aadarsha Shrestha)",
        "section": "2. Machine Learning for Regression",
        "question": "What is standard deviation?"
      },
      {
        "text": "The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\n(Daniel Mu\u00f1oz Viveros)",
        "section": "2. Machine Learning for Regression",
        "question": "Do we need to apply regularization techniques always? Or only in certain scenarios?"
      },
      {
        "text": "As it speeds up the development:\nprepare_df(initial_df, seed, fill_na_type)  - that prepared all 3 dataframes and 3 y_vectors. Fillna() can be done before the initial_df is split.\nOf course, you can reuse other functions: rmse() and train_linear_regression(X,y,r) from the class notebook\n(Ivan Brigida)",
        "section": "2. Machine Learning for Regression",
        "question": "Shortcut: define functions for faster execution"
      },
      {
        "text": "When applying a function to a DataFrame, it is important to consider that if you do not want to alter the original DataFrame, you should create a copy of it first. Failing to do so may result in unintended modifications to the original dataset. To preserve the integrity of your data, always use df.copy() before making any changes.\n(added by Karina)",
        "section": "2. Machine Learning for Regression",
        "question": "Warning about modifying Dataframes inside functions"
      },
      {
        "text": "If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\n(Quinn Avila)",
        "section": "2. Machine Learning for Regression",
        "question": "How to use pandas to find standard deviation"
      },
      {
        "text": "Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\nNumpy\nPandas\npandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\nimport numpy as np\nnp.std(df.weight, ddof=1)\nThe result will be similar if we change the dof = 1 in numpy\n(Harish Balasundaram)",
        "section": "2. Machine Learning for Regression",
        "question": "Standard Deviation Differences in Numpy and Pandas"
      },
      {
        "text": "In pandas you can use built in Pandas function names std() to get standard deviation. For example\ndf['column_name'].std() to get standard deviation of that column.\ndf[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\n(Khurram Majeed)",
        "section": "2. Machine Learning for Regression",
        "question": "Standard deviation using Pandas built in Function"
      },
      {
        "text": "Use \u2018pandas.concat\u2019 function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\ndf_train_combined = pd.concat([df_train, df_val])\ny_train = np.concatenate((y_train, y_val), axis=0)\n(George Chizhmak)",
        "section": "2. Machine Learning for Regression",
        "question": "How to combine train and validation datasets"
      },
      {
        "text": "The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model's predicted values and the actual observed values, offering insight into the model's ability to accurately forecast the target variable. To calculate RMSE score:\nLibraries needed\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(actual_values, predicted_values)\nrmse = np.sqrt(mse)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n(Aminat Abolade)",
        "section": "2. Machine Learning for Regression",
        "question": "Understanding RMSE and how to calculate RMSE score"
      },
      {
        "text": "If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\n(Olga Rudakova)\n\u2013",
        "section": "2. Machine Learning for Regression",
        "question": "What syntax use in Pandas for multiple conditions using logical AND and OR"
      },
      {
        "text": "I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression",
        "section": "2. Machine Learning for Regression",
        "question": "Deep dive into normal equation for regression"
      },
      {
        "text": "(Hrithik Kumar Advani)",
        "section": "2. Machine Learning for Regression",
        "question": "Useful Resource for Missing Data Treatment\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook"
      },
      {
        "text": "The instruction for applying log transformation to the \u2018median_house_value\u2019 variable is provided before Q3 in the homework for Week-2 under the \u2018Prepare and split the dataset\u2019 heading.\nHowever, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\n(Added by Soham Mundhada)",
        "section": "2. Machine Learning for Regression",
        "question": "Caution for applying log transformation in Week-2 2023 cohort homework"
      },
      {
        "text": "Version 0.24.2 and Python 3.8.11\n(Added by Diego Giraldo)",
        "section": "2. Machine Learning for Regression",
        "question": "What sklearn version is Alexey using in the youtube videos?"
      },
      {
        "text": "You can find homework 2 in this folder for the 2024 cohort via this link:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2024/02-regression/homework.md\n(Added by Victor Emenike)",
        "section": "2. Machine Learning for Regression",
        "question": "For Homework #2, there\u2019s no 2024 cohort folderhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/02-regression/homework.md?"
      },
      {
        "text": "For questions 5 and 6, do not forget to reinitialize:\nIdx = np.arange(n)\nFor each iteration of r in question 5 and also for question 6\n(Added by Victor Emenike)",
        "section": "2. Machine Learning for Regression",
        "question": "Hello, for Q6 of homework 2, is the RMSE result close to the options? Mine is about 12.4 different from the closets option, but all the above questions I have pretty close answers, so I don\u2019t know why there is such a great difference in Q6\u2026"
      },
      {
        "text": "You can handle missing values by:\nImputing the missing values with the mean, median, or mode.\nUsing algorithms that support missing values inherently (e.g., some tree-based methods).\nRemoving rows or columns with missing data, depending on the extent of missingness. Feature engineering might also help derive new features from incomplete data\u200b\n(added by David Peterson)",
        "section": "3. Machine Learning for Classification",
        "question": "What is the best way to handle missing values in the dataset before training a regression model?"
      },
      {
        "text": "Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\nSubmit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\nYoutube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\n~~Nukta Bhatia~~",
        "section": "3. Machine Learning for Classification",
        "question": "How do I get started with Week 3?"
      },
      {
        "text": "The error message \u201ccould not convert string to float: \u2018Nissan\u2019\u201d typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand \u2018Nissan\u2019 into a numerical value, which isn\u2019t possible.\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\nHere\u2019s an example of how you can perform one-hot encoding using pandas:\nimport pandas as pd\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\ndata_encoded = pd.get_dummies(data, columns=['brand'])\nIn this code, pd.get_dummies() creates a new DataFrame where the \u2018brand\u2019 column is replaced with binary columns for each brand (e.g., \u2018brand_Nissan\u2019, \u2018brand_Toyota\u2019, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\n-Mohammad Emad Sharifi-",
        "section": "3. Machine Learning for Classification",
        "question": "Could not convert string to float:\u2019Nissan\u2019rt string to float: 'Nissan'"
      },
      {
        "text": "Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutualinformation score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\n\u2014Odimegwu David\u2014-",
        "section": "3. Machine Learning for Classification",
        "question": "Why did we change the targets to binary format when calculating mutual information score in the homework?"
      },
      {
        "text": "Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\nPastor Soto",
        "section": "3. Machine Learning for Classification",
        "question": "What data should we use for correlation matrix"
      },
      {
        "text": "First, you have to consider whether the data is numerical or categorical. If it\u2019s numerical, you can correlate it directly - if it\u2019s categorial you can find the correlations for the data indirectly by vectorizing the data using One-Hot encoding or other similar method.\nTo find if it\u2019s numerical, check the dtypes() of the dataframe you\u2019re looking at. Anything that\u2019s integer, float, etc is numerical, while data types such as objects are categorical. You can correlate the numerical data by specifying which columns are numerical and using that as input to a correlation matrix.\nnumerical = ['tenure', 'monthlycharges', 'totalcharges']\ndf[numerical].corr()\nAdded by Michael Friske",
        "section": "3. Machine Learning for Classification",
        "question": "How do you find the correlation matrix?"
      },
      {
        "text": "The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\nHere an example on how to color the correlation matrix. A color map of choice can get passed, here \u2018viridis\u2019 is used.\n# ensure to have only numerical values in the dataframe before calling 'corr'\ncorr_mat = df_numerical_only.corr()\ncorr_mat.style.background_gradient(cmap='viridis')\nHere is an example of how the coloring will look like using a dataframe containing random values and applying \u201cbackground_gradient\u201d to it.\nnp.random.seed = 3\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\ndf_random.style.background_gradient(cmap='viridis')\nAdded by Sylvia Schmitt",
        "section": "3. Machine Learning for Classification",
        "question": "Coloring the background of the pandas.DataFrame.corr correlation matrix directly"
      },
      {
        "text": "data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\ndata_corr.head(10)\nAdded by Harish Balasundaram\nYou can also use seaborn to create a heatmap with the correlation. The code for doing that:\nsns.heatmap(df[numerical_features].corr(),\nannot=True,\nsquare=True,\nfmt=\".2g\",\ncmap=\"crest\")\nAdded by Cecile Guillot\nYou can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\nWhich outputs, in the case of churn dataset:\n(M\u00e9lanie Fouesnard)",
        "section": "3. Machine Learning for Classification",
        "question": "Identifying highly correlated feature pairs easily through unstack"
      },
      {
        "text": "Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\nIt's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\nAlena Kniazeva",
        "section": "3. Machine Learning for Classification",
        "question": "What data should be used for EDA?"
      },
      {
        "text": "Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\nEdidiong Esu\nBelow is an extract of Alexey's book explaining this point. Hope is useful\nWhen we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\nWith this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven't seen. By not applying the fit to the validation model we can know how well it was trained.\nBelow is an extract of Alexey's book explaining this point.\nHumberto Rodriguez\nThere is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\nThe correct way is to fit_transform the train set, and only transform the validation and test sets.\nMemoona Tahira",
        "section": "3. Machine Learning for Classification",
        "question": "Fitting DictVectorizer on validation"
      },
      {
        "text": "For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\nWe should select the \u201csmallest\u201d difference, and not the \u201clowest\u201d, meaning we should reason in absolute values.\nIf the difference is negative, it means that the model actually became better when we removed the feature.",
        "section": "3. Machine Learning for Classification",
        "question": "Feature elimination"
      },
      {
        "text": "In newer versions of scikit-learn, the method has been replaced by get_feature_names_out().\nInstead use the method \u201c.get_feature_names_out()\u201d from DictVectorizer function and the warning will be resolved , but we need not worry about the waning as there won't be any warning\nSanthosh Kumar",
        "section": "3. Machine Learning for Classification",
        "question": "FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2"
      },
      {
        "text": "Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\nMake sure that the target variable for the logistic regression is binary.\nKonrad Muehlberg",
        "section": "3. Machine Learning for Classification",
        "question": "Logistic regression crashing Jupyter kernel"
      },
      {
        "text": "Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\nsag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It's particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\nAlpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha=alpha, solver='sag', random_state=42)\nridge.fit(X_train, y_train)\nAminat Abolade",
        "section": "3. Machine Learning for Classification",
        "question": "Understanding Ridge"
      },
      {
        "text": "DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\nUsing \u201csparse\u201d format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit \u201cworse\u201d results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\nLarkin Andrii",
        "section": "3. Machine Learning for Classification",
        "question": "pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:"
      },
      {
        "text": "Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\nPlay with different scalers. See notebook-scaling-ohe.ipynb\nDmytro Durach\n(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.",
        "section": "3. Machine Learning for Classification",
        "question": "Convergence Problems in W3Q6"
      },
      {
        "text": "When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\nFeature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \tsimilar scale, preventing convergence issues.\nCategorical Feature Encoding: If your dataset includes categorical features, apply \tcategorical encoding techniques such as OneHotEncoder (OHE) to \tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\nCombine Features: After \tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\nBy following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\nYou can find an example here.\n \t\t\t\t\t\t\t\t\t\t\t\tOsman Ali",
        "section": "3. Machine Learning for Classification",
        "question": "Dealing with Convergence in Week 3 q6"
      },
      {
        "text": "A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\nThe default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn\u2019t give an error message like dense mode.\n \t\t\t\t\t\t\t\t\t\t\t\tQuinn Avila",
        "section": "3. Machine Learning for Classification",
        "question": "Sparse matrix compared dense matrix"
      },
      {
        "text": "The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\nImport warnings\nwarnings.filterwarnings(\u201cignore\u201d)\nKrishna Anand",
        "section": "3. Machine Learning for Classification",
        "question": "How  to Disable/avoid Warnings in Jupyter Notebooks"
      },
      {
        "text": "Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\nAnswer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\nAsia Saeed",
        "section": "3. Machine Learning for Classification",
        "question": "How to select the alpha parameter in Q6"
      },
      {
        "text": "Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\nAnswer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\nAsia Saeed",
        "section": "3. Machine Learning for Classification",
        "question": "Second variable that we need to use to calculate the mutual information score"
      },
      {
        "text": "Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\nYou need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\nWhile calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\nSince order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)",
        "section": "3. Machine Learning for Classification",
        "question": "Features for homework Q5"
      },
      {
        "text": "Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\nBoth will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\nTanya Mard",
        "section": "3. Machine Learning for Classification",
        "question": "What is the difference between OneHotEncoder and DictVectorizer?"
      },
      {
        "text": "They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]",
        "section": "3. Machine Learning for Classification",
        "question": "What is the difference between pandas get_dummies and sklearn OnehotEncoder?"
      },
      {
        "text": "For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\nAnswer: for both splits random_state = 42 should be used\n(Bhaskar Sarma)",
        "section": "3. Machine Learning for Classification",
        "question": "Use of random seed in HW3"
      },
      {
        "text": "Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\nAnswer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.",
        "section": "3. Machine Learning for Classification",
        "question": "Correlation before or after splitting the data"
      },
      {
        "text": "Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\nDrop all categorical features first before proceeding.\n(Aileah Gotladera)\nWhile it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\n(Erjon)",
        "section": "3. Machine Learning for Classification",
        "question": "Features in Ridge Regression Model"
      },
      {
        "text": "You need to use all features. and price for target. Don't include the average variable we created before.\nIf you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\nI also used StandardScalar for numerical variable you can try running with or without this\n(Peter Pan)",
        "section": "3. Machine Learning for Classification",
        "question": "Handling Column Information for Homework 3 Question 6"
      },
      {
        "text": "Warning: When searching for the best value of C that yields the highest accuracy, be mindful that you should be looking for the maximum accuracy, not the minimum. Although the goal is to find the smallest C value, ensure that it corresponds to the highest accuracy achieved. Always prioritize accuracy maximization while minimizing C.\n(added by Karina)",
        "section": "3. Machine Learning for Classification",
        "question": "Choosing smaller C that leads to best accuracy in Homework 3 Question 6"
      },
      {
        "text": "Use sklearn.preprocessing encoders and scalers, e.g. OneHotEncoder, OrdinalEncoder, and StandardScaler.",
        "section": "3. Machine Learning for Classification",
        "question": "Transforming Non-Numerical Columns into Numerical Columns"
      },
      {
        "text": "These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\nWhen you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\nYou can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\nOlga Rudakova",
        "section": "3. Machine Learning for Classification",
        "question": "What is the better option FeatureHasher or DictVectorizer"
      },
      {
        "text": "(Question by Connie S.)\nThe reason it's good/recommended practice to do it after splitting is to avoid data leakage - you don't want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\nAnswered/added by Rileen Sinha",
        "section": "3. Machine Learning for Classification",
        "question": "Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?"
      },
      {
        "text": "If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\nAdded by Akshar Goyal",
        "section": "3. Machine Learning for Classification",
        "question": "HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?"
      },
      {
        "text": "We can use sklearn & numpy packages to calculate Root Mean Squared Error\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nRmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\nAdded by Radikal Lukafiardi\nYou can also refer to Alexey\u2019s notebook for Week 2:\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\nwhich includes the following code:\ndef rmse(y, y_pred):\nerror = y_pred - y\nmse = (error ** 2).mean()\nreturn np.sqrt(mse)\n(added by Rileen Sinha)",
        "section": "3. Machine Learning for Classification",
        "question": "How to calculate Root Mean Squared Error?"
      },
      {
        "text": "The solution is to use \u201cget_feature_names_out\u201d instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\nGeorge Chizhmak",
        "section": "3. Machine Learning for Classification",
        "question": "AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'"
      },
      {
        "text": "To use RMSE without math or numpy, \u2018sklearn.metrics\u2019 has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\nfrom sklearn.metrics import mean_squared_error\nrms = mean_squared_error(y_actual, y_predicted, squared=False)\nSee details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\nAhmed Okka",
        "section": "3. Machine Learning for Classification",
        "question": "Root Mean Squared Error"
      },
      {
        "text": "This article explains different encoding techniques used https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02\nHrithik Kumar Advani",
        "section": "3. Machine Learning for Classification",
        "question": "Encoding Techniques"
      },
      {
        "text": "I got this error multiple times here is the code:\n\u201caccuracy_score(y_val, y_pred >= 0.5)\u201d\nTypeError: 'numpy.float64' object is not callable\nI solve it using\nfrom sklearn import metrics\nmetrics.accuracy_score(y_train, y_pred>= 0.5)\nOMAR Wael",
        "section": "3. Machine Learning for Classification",
        "question": "Error in use of accuracy_score from sklearn in jupyter (sometimes)"
      },
      {
        "text": "In Scikit-Learn\u2019s LogisticRegression, a model that is trained will have raw values and the predicted probabilities.\n.decision_function() returns raw values which are a linear combination of the features and weights, similar to the output of Linear Regression.\n.predict_proba() goes one step further by inputting these raw values into the sigmoid function, to convert them into probabilities (between 0 and 1).\nKemal Dahha",
        "section": "3. Machine Learning for Classification",
        "question": "What is the difference between .decision_function() and .predict_proba()?"
      },
      {
        "text": "The error occurs because some features you try to drop have been one-hot encoded into multiple columns. After encoding, the original column may no longer exist, leading to the KeyError. To resolve this, identify and drop all related one-hot encoded columns (e.g., those starting with the original feature name) instead of the original feature itself.\nFor example, after one-hot encoding, the column 'marital' could have been split into columns like 'marital_single', 'marital_married', etc. This means that the original column 'marital' no longer exists, leading to the KeyError.\n~ David Peterson",
        "section": "3. Machine Learning for Classification",
        "question": "Why do I get a KeyError when dropping features after one-hot encoding?"
      },
      {
        "text": "This is not possible since the parameter C represents the inverse of the regularization strength, and setting C to 0 means infinite regularization and hence trying this through the scikit learn module of Logistic Regression will give ValueError",
        "section": "3. Machine Learning for Classification",
        "question": "Question 6 of HW 3 asks to train a regularized logistic regression with c = 0."
      },
      {
        "text": "Import the data using df = pd.read_csv(\u201cbank-full.csv\u201d, sep=';').\n\nThe data is not separated by a comma but by a semicolon.",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Homework 3"
      },
      {
        "text": "Week 4 HW 2024:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2024/04-evaluation/homework.md\nWeek 4 HW 2023: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\nML Zoomcamp project evaluation criteria: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\nYouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\nSci-Kit Learn on Evaluation:\nhttps://scikit-learn.org/stable/model_selection.html\n~~Nukta Bhatia~~",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How do I get started with Week 4?"
      },
      {
        "text": "https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\nMetrics can be used on a series or a dataframe\n~~Ella Sahnan~~",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Using a variable to score"
      },
      {
        "text": "Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\nRefer to the sklearn docs, random_state is to ensure the \u201crandomness\u201d that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\n~~Ella Sahnan~~",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Why do we sometimes use random_state and not at other times?"
      },
      {
        "text": "How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\nUse classification_report from sklearn. For more info check here.\nAbhishek N",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How to get all classification metrics?"
      },
      {
        "text": "I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\nChoose the one closest to any of the options\nAdded by Azeez Enitan Edunwale\nYou can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  \u201cclassification_report\u201d (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\nAdded by Rileen Sinha",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Multiple thresholds for Q4"
      },
      {
        "text": "Solution description: duplicating the\ndf.churn = (df.churn == 'yes').astype(int)\nThis is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\nIt is telling us that it only contains 0's.\nDelete one of the below cells and you will get the accuracy\nHumberto Rodriguez",
        "section": "4. Evaluation Metrics for Classification",
        "question": "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
      },
      {
        "text": "Use Yellowbrick. Yellowbrick is a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\nKrishna Annad",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Method to get beautiful classification report"
      },
      {
        "text": "That\u2019s fine, use the closest option",
        "section": "4. Evaluation Metrics for Classification",
        "question": "I\u2019m not getting the exact result in homework"
      },
      {
        "text": "Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Use AUC to evaluate feature importance of numerical variables"
      },
      {
        "text": "When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters \u201cy_true\u201d and \u201cy_score\u201d. So for each numerical value in the dataframe it will be passed as the \u201cy_score\u201d to the function and the target variable will get passed a \u201cy_true\u201d each time.\nSylvia Schmitt",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Help with understanding: \u201cFor each numerical value, use it as score and compute AUC\u201d"
      },
      {
        "text": "You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\nDiego Giraldo",
        "section": "4. Evaluation Metrics for Classification",
        "question": "What dataset should I use to compute the metrics in Question 3"
      },
      {
        "text": "KFold is a cross-validation technique that splits your dataset into k equal parts (folds). It trains the model k times, each time using a different fold as the validation set while training on the remaining folds. This process helps provide a more reliable estimate of a model's performance by ensuring every data point gets to be in both the training and validation sets. The average score across all folds offers a robust evaluation, minimizing the risk of overfitting to a specific train-test split.\nWhat does this line do?\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\nIf I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\nDid you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\nIn my case changing random state changed results\n(Arthur Minakhmetov)\nChanging the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\n(Bhaskar Sarma)\nIn case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\n(Ani Mkrtumyan)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "What does KFold do?"
      },
      {
        "text": "I\u2019m getting \u201cValueError: multi_class must be in ('ovo', 'ovr')\u201d when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\nI was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\nAsia Saeed",
        "section": "4. Evaluation Metrics for Classification",
        "question": "ValueError: multi_class must be in ('ovo', 'ovr')"
      },
      {
        "text": "from tqdm.auto import tqdm\nTqdm - terminal progress bar\nKrishna Anand",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Monitoring Wait times and progress of the code execution can be done with:"
      },
      {
        "text": "Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\nAileah Gotladera",
        "section": "4. Evaluation Metrics for Classification",
        "question": "What is the use of inverting or negating the variables less than the threshold?"
      },
      {
        "text": "In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\nThe solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\nVladimir Yesipov\nPredict_proba shows probailites per class.\nAni Mkrtumyan",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Difference between predict(X) and predict_proba(X)[:, 1]"
      },
      {
        "text": "For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\nThe threshold is 1.0\nFPR is 0.0\nAnd TPR is 0.0\nWhen the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\nThat is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\nAlena Kniazeva",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Why are FPR and TPR equal to 0.0, when threshold = 1.0?"
      },
      {
        "text": "Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\nOptimal F1 Score: {optimal_f1_score:.2f}',\nxy=(optimal_threshold, optimal_f1_score),\nxytext=(0.3, 0.5),\ntextcoords='axes fraction',\narrowprops=dict(facecolor='black', shrink=0.05))\nQuinn Avila",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How can I annotate a graph?"
      },
      {
        "text": "It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.",
        "section": "4. Evaluation Metrics for Classification",
        "question": "I didn\u2019t fully understand the ROC curve. Can I move on?"
      },
      {
        "text": "One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\nAlthough the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\n1)\ndf_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\ndf_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n2)\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\nTherefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\nIbraheem Taha",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Why do I have different values of accuracy than the options in the homework?"
      },
      {
        "text": "You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\nI suppose here that you have your df_scores ready with your three columns \u2018threshold\u2019, \u2018precision\u2019 and \u2018recall\u2019:\nYou want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\nidx = np.argwhere(\nnp.diff(\nnp.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\n)\n).flatten()\nYou can print the result to easily read it:\nprint(\nf\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx]['threshold']}.\"\n)\n(M\u00e9lanie Fouesnard)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How to find the intercept between precision and recall curves by using numpy?"
      },
      {
        "text": "In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nprecision_score(y_true, y_pred, average='binary')\nrecall_score(y_true, y_pred, average='binary')\nf1_score(y_true, y_pred, average='binary')\nRadikal Lukafiardi",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Compute Recall, Precision, and F1 Score using scikit-learn library"
      },
      {
        "text": "Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\n\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\nSmaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\nLarger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\nAminat Abolade",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Why do we use cross validation?"
      },
      {
        "text": "Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\nfrom sklearn.metrics import (accuracy_score,\nprecision_score,\nrecall_score,\nf1_score,\nroc_auc_score\n)\naccuracy = accuracy_score(y_val, y_pred)\nprecision = precision_score(y_val, y_pred)\nrecall = recall_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nroc_auc = roc_auc_score(y_val, y_pred)\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1-Score: {f1}')\nprint(f'ROC AUC: {roc_auc}')\n(Harish Balasundaram)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Evaluate the Model using scikit learn metrics"
      },
      {
        "text": "Scikit-learn offers another way: precision_recall_fscore_support\nExample:\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n(Gopakumar Gopinathan)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Are there other ways to compute Precision, Recall and F1 score?"
      },
      {
        "text": "ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\nThe reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\nThis is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\nIf the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\n(Anudeep Vanjavakam)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "When do I use ROC vs Precision-Recall curves?"
      },
      {
        "text": "You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (\u2018above_average\u2019) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\n(Denys Soloviov)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How to evaluate feature importance for numerical variables with AUC?"
      },
      {
        "text": "Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\n(George Chizhmak)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Dependence of the F-score on class imbalance"
      },
      {
        "text": "We can import precision_recall_curve from scikit-learn and plot the graph as follows:\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(y_val, y_predict)\nplt.plot(thresholds, precision[:-1], label='Precision')\nplt.plot(thresholds, recall[:-1], label='Recall')\nplt.legend()\nHrithik Kumar Advani",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Quick way to plot Precision-Recall Curve"
      },
      {
        "text": "For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\nPlease check the realisation in sk-learn library:\nhttps://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\nOlga Rudakova",
        "section": "4. Evaluation Metrics for Classification",
        "question": "What is Stratified k-fold?"
      },
      {
        "text": "Accuracy is the proportion of correct predictions made by the model, but it can be misleading, especially with imbalanced datasets. For example, if 95% of your data belongs to one class, a model that always predicts this majority class will have high accuracy, even though it completely fails to identify the minority class. In such cases, metrics like precision, recall, F1-score, or AUROC might be more appropriate, as they provide a clearer view of model performance on both classes.\nDavid Peterson",
        "section": "4. Evaluation Metrics for Classification",
        "question": "Why is accuracy not always the best metric for evaluating a classification model?"
      },
      {
        "text": "Precision is TruePositive/PredictedPositive and recall means TruePositive / ActualPositive\nPrecision \u2192 Precise predictions (how accurate are our YES predictions?)\nRecall \u2192 Remembering  (how many real YES cases did we find?)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How to easily remember precision and recall?"
      },
      {
        "text": "Precision:\nMemory tip: Think of Precision as \"How Precise Are Our Positive Predictions?\". It relates to the accuracy of the positive results. It emphasizes how many of the predicted positive instances are actually correct\nInterpretation:\n- High Precision:\n- Most of the predicted positives are correct.\n- This makes the model more reliable.\n- Low Precision:\n- Indicates a higher rate of false positives.\n- This decreases trust in the positive predictions.\nWhen to prioritize precision: In scenarios like email spam detection, where marking a legitimate email as spam (false positive) can lead to missed communications, high precision is preferred to ensure that most flagged emails are indeed spam.\nRecall:\nMemory tip: Think of Recall as \"How Sensitive Are We to the Positives?\". It emphasizes capturing all actual positive cases. A high recall means that the model is good at identifying most of the positives.\nInterpretation:\n- High Recall:\n- The model captures most of the true positives.\n- This is crucial in situations where missing a positive case is costly.\n- Low Recall:\n- Many actual positives are overlooked.\n- This highlights potential issues in detection.\nWhen to prioritize recall: In medical diagnostics for a severe or highly contagious disease, missing a true positive (an actual case of the disease) can have serious public health implications.\nBalancing Precision and Recall:\n- Improving one metric may lead to a decrease in the other.\n- The choice between precision and recall depends on specific goals and acceptable trade-offs in a given application.\n(added by Karina)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How do I interpret precision and recall?"
      },
      {
        "text": "This warning occurs when your model doesn't predict any samples for certain labels, which causes a zero-division error when calculating the F-score. Specifically, the warning is triggered when there are no true positives or predicted positives for certain labels, leading to undefined precision or recall.\nTo address this, you can use the zero_division parameter in scikit-learn's f1_score function. This parameter defines what should happen in cases of zero division:\nSet zero_division=1: This will set the precision, recall, and F-score to 1 when no positive samples are predicted.\nSet zero_division=0: This is the default behavior, setting the metric to 0 when there are no predicted samples for a given label.\nSet zero_division=\u2019warn\u2019 (default behavior): This is the default behavior, acts like 0 but also raises a warning.\nJust as shown below:\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n# For precision score \nprecision = precision_score(y_true, y_pred, average='weighted', zero_division=\u2019warn\u2019) \n\n# For recall score \nrecall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n# For f1-score\nf1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n(added by Jon Areas)",
        "section": "4. Evaluation Metrics for Classification",
        "question": "How to address UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples?"
      },
      {
        "text": "The idea of the question is not something presented in the lecture videos, but it is a concept that could be useful when evaluating the importance of features with respect to the prediction of the binary target variable (yes/no).\nIn my case, I did the following:\nidentified the numerical features in the dataset\nFor each feature in the list of numerical features, I calculated the AUC like so: roc_auc_score(y_target, feature_vector). Here, the y_target is the y_triangle and the feature_vector contains the value for each numerical vector/column in the train dataset.\nNext, I then created a data frame with two columns: name of numerical feature and ROC AUC score.\nFinally, I sorted the data frame by the ROC AUC score to determine the numerical feature with the highest ROC AUC.\nVictor Emenike",
        "section": "5. Deploying Machine Learning Models",
        "question": "Homework 4 , q1 is not making sense to me.  The score should be between 0 to 1. I tried computing roc_curve (df_train['age'], y] and the graph does not have the model line. Please can anyone clarify."
      },
      {
        "text": "Week 5 HW 2024:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2024/05-deployment/homework.md\nWeek 5 HW 2023: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\nHW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\nYouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\n~~~ Nukta Bhatia ~~~",
        "section": "5. Deploying Machine Learning Models",
        "question": "How do I get started with Week 5?"
      },
      {
        "text": "While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\nIt is advised to prepare your \u201chomework environment\u201d with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\nhttps://www.youtube.com/watch?v=IXSiYkP23zo\nNote that (only) small  instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\nAlternative ways are sketched here:\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md",
        "section": "5. Deploying Machine Learning Models",
        "question": "Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc."
      },
      {
        "text": "You\u2019ll need a kaggle account\nGo to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\nIn the same location as your Jupyter NB, place the `kaggle.json` file\nRun `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\nMake sure to import os via `import os` and then run:\nos.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\nFinally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\nAnd then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\n>>> Michael Fronda <<<",
        "section": "5. Deploying Machine Learning Models",
        "question": "How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience"
      },
      {
        "text": "Cd .. (go back)\nLs (see current folders)\nCd \u2018path\u2019/ (go to this path)\nPwd (home)\nCat \u201cfile name\u2019 --edit txt file in ubuntu\nAileah Gotladera",
        "section": "5. Deploying Machine Learning Models",
        "question": "Basic Ubuntu Commands:"
      },
      {
        "text": "Open terminal and type the code below to check the version on your laptop\npython3 --version\nFor windows,\nVisit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\nRun the installer and  ensure to check the box that says \u201cAdd Python to PATH\u201d during installation and complete the installation by following the prompts\nOr\nFor Python 3,\nOpen your command prompt or terminal and run the following command:\npip install --upgrade python\nAminat Abolade",
        "section": "5. Deploying Machine Learning Models",
        "question": "Installing and updating to the python version 3.10 and higher"
      },
      {
        "text": "Windows 10:\nOpen PowerShell as Admin.\nRun: wsl --install\nRestart your computer.\nSet up your Linux distribution (e.g., Ubuntu).\nWindows 11:\nOpen Windows Terminal as Admin.\nRun: wsl --install\nRestart if prompted.\nSet up your Linux distribution.\nNote: To install a specific distribution, use wsl --install -d <DistributionName>.\nFor updates, run: wsl --update.\nIt is quite simple, and you can follow these instructions here:\nhttps://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\nMake sure that you have \u201cVirtual Machine Platform\u201d feature activated in your Windows \u201cFeatures\u201d. To do that, search \u201cfeatures\u201d in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\nIn the Microsoft Store: look for \u2018Ubuntu\u2019 or \u2018Debian\u2019 (or any linux distribution you want) and install it\nOnce it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\nYou are now inside of your linux system. You can test some commands such as \u201cpwd\u201d. You are not in your Windows system.\nTo go to your windows system: you need to go back two times with cd ../.. And then go to the \u201cmnt\u201d directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\nPython should be already installed but you can check it by running sudo apt install python3 command.\nYou can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\nYou can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\nYou have to uncomment the set bell-style none line -> to do that, press the \u201ci\u201d keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press \u201c:wq\u201d to write (it saves your modifications) then quit.\nYou can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\nYou will need to install pip by running this command sudo apt install python3-pip\nNB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\n/sbin/ldconfig.real: Can't link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\nSo I had to create the following symbolic link:\nsudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\n(M\u00e9lanie Fouesnard)",
        "section": "5. Deploying Machine Learning Models",
        "question": "How to install WSL on Windows 10 and 11 ?"
      },
      {
        "text": "Do you get errors building the Docker image on the Mac M1 chipset?\nThe error I was getting was:\nCould not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\nThe fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\nOpen mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\nReplace line 1 with\nFROM --platform=linux/amd64 ubuntu:latest\nNow build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\nDavid Colton",
        "section": "5. Deploying Machine Learning Models",
        "question": "Error building Docker images on Mac with M1 silicon"
      },
      {
        "text": "Import waitress\nprint(waitress.__version__)\nKrishna Anand",
        "section": "5. Deploying Machine Learning Models",
        "question": "Method to find the version of any install python libraries in jupyter notebook"
      },
      {
        "text": "Ensure Docker Daemon Is Running\nOn Windows:\nOpen Docker Desktop (admin rights may be required).\nCheck if it\u2019s running, and restart Docker Desktop if necessary.\nOn Linux:\nRun sudo systemctl start docker to start the Docker daemon.\nVerify it\u2019s running with sudo systemctl status docker.\nVerify Docker Group Membership (Linux Only)\nCheck if your user is in the Docker group:\ngroups $USER\nIf \"docker\" isn\u2019t listed, add yourself with:\nsudo usermod -aG docker $USER\nLog out and back in to apply changes.\nRestart the Docker Service (Linux)\nsudo systemctl restart docker\nCheck Docker Socket Permissions (Linux)\nRun the following command to confirm Docker socket permissions:\nsudo chmod 666 /var/run/docker.sock\nTry Running Docker with sudo (Linux)\nRun sudo docker ps to check if permissions are causing the issue.\nTest Docker Setup\nRun a test Docker command to verify connection:\ndocker run hello-world\nWorking on getting Docker installed - when I try running hello-world I am getting the error.\nDocker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\nSolution description\nIf you\u2019re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\nOn Linux, start the docker daemon with either of these commands:\nsudo dockerd\nsudo service docker start\nAdded by Ugochukwu Onyebuchi",
        "section": "5. Deploying Machine Learning Models",
        "question": "Cannot connect to the docker daemon. Is the Docker daemon running?"
      },
      {
        "text": "After using the command \u201cdocker build -t churn-prediction .\u201d to build the Docker image, the above error is raised and the image is not created.\nIn your Dockerfile, change the Python version in the first line the Python version installed in your system:\nFROM python:3.7.5-slim\nTo find your python version, use the command python --version. For example:\npython --version\n>> Python 3.9.7\nThen, change it on your Dockerfile:\nFROM python:3.9.7-slim\nAdded by Filipe Melo",
        "section": "5. Deploying Machine Learning Models",
        "question": "The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1"
      },
      {
        "text": "When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\nThe solution is to use the full name of sklearn. That is, run it as \u201cpipenv install scikit-learn==1.0.2\u201d and the error will go away, allowing you to install sklearn for the version in your virtual environment.\nOdimegwu David\nHomework asks you to install 1.3.1\nPipenv install scikit-learn==1.3.1\nUse Pipenv to install Scikit-Learn version 1.3.1\nGopakumar Gopinathan",
        "section": "5. Deploying Machine Learning Models",
        "question": "Running \u201cpipenv install sklearn==1.0.2\u201d gives errors. What should I do?"
      },
      {
        "text": "When adding libraries to the virtual environment in the lesson 5.5, the trainer used the command pipenv install numpy scikit-learn==0.24.2 flask; however, some people using Python 3.11 or later, may encounter an error, failing to lock files correctly with Pipfile.lock. You may need to install `scikit-learn==1.4.2` as the error you got when running the application is a bit different from the one of the trainer. This should solve the problem.\nIf you are still having problems, try the following:\n\u25cf Delete the Pipfile.lock via rm Pipfile; then rebuild the lock via pipenv lock from the terminal.\n\u25cf If it still doesn't work, delete the pipenv environment, Pipfile and Pipfile.lock, and create a new one. The commands to delete the pipenv environment and pip files are:\n`pipenv --rm`\n`rm Pipfile*`\nMaximilien Eyengue",
        "section": "5. Deploying Machine Learning Models",
        "question": "Error: Failed to lock files with Pipfile.lock"
      },
      {
        "text": "I initially installed Flask with pipenv, but I received a \"No module named 'flask'\" error. I then reinstalled Flask using pip, and after that, I was able to import Flask successfully.\n\u2014 Kuzey Edes Huyal",
        "section": "5. Deploying Machine Learning Models",
        "question": "How do I resolve the \"No module named flask\" error?"
      },
      {
        "text": "What is the reason we don\u2019t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\nFor best practice, you don\u2019t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\nThey consume extra space on your disk. Unless you don\u2019t want to re-run the previously existing containers, it is better to use the `--rm` option.\nThe right way to say: \u201cWhy do we remove the docker container in our system?\u201d. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\nThe option `--rm` is for removing containers. The images remain until you remove them manually. If you don\u2019t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\nDuring development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It\u2019s important to understand the difference between the term \u201cdocker image\u201d and \u201cdocker container\u201d. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\nAdded by Muhammad Awon",
        "section": "5. Deploying Machine Learning Models",
        "question": "Why do we need the --rm flag"
      },
      {
        "text": "When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\nAdded by Pastor Soto",
        "section": "5. Deploying Machine Learning Models",
        "question": "Failed to read Dockerfile"
      },
      {
        "text": "Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.",
        "section": "5. Deploying Machine Learning Models",
        "question": "Install docker on MacOS"
      },
      {
        "text": "Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\nUsing default tag: latest\nError response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\nSolution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\ndocker pull svizor/zoomcamp-model:3.10.12-slim\nAdded by Vladimir Yesipov",
        "section": "5. Deploying Machine Learning Models",
        "question": "I cannot pull the image with docker pull command"
      },
      {
        "text": "Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\ndocker image ls <image name>\nOr alternatively:\ndocker images <image name>\nIn action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\ndocker image ls --format \"{{.Size}}\" <image name>\nOr alternatively:\ndocker images --format \"{{.Size}}\" <image name>\nSylvia Schmitt",
        "section": "5. Deploying Machine Learning Models",
        "question": "Dumping/Retrieving only the size of for a specific Docker image"
      },
      {
        "text": "It creates them in\nOSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\nWindows: C:\\Users\\<USERNAME>\\.virtualenvs\\folder-name_cyrptic-hash\nEg: C:\\Users\\Ella\\.virtualenvs\\code-qsdUdabf (for module-05 lesson)\nThe environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\nAll libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\n(Memoona Tahira)",
        "section": "5. Deploying Machine Learning Models",
        "question": "Where does pipenv create environments and how does it name them?"
      },
      {
        "text": "Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\ndocker run -it --entrypoint bash <image>\nIf the container is already running, execute a command in the specific container:\ndocker ps (find the container-id)\ndocker exec -it <container-id> bash\n(Marcos MJD)",
        "section": "5. Deploying Machine Learning Models",
        "question": "How do I debug a docker container?"
      },
      {
        "text": "$ docker exec -it 1e5a1b663052 bash\nthe input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\nFix:\nwinpty docker exec -it 1e5a1b663052 bash\nA TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\nWinpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\nMore info on terminal, shell, console applications hi and so on:\nhttps://conemu.github.io/en/TerminalVsShell.html\n(Marcos MJD)",
        "section": "5. Deploying Machine Learning Models",
        "question": "The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)"
      },
      {
        "text": "Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\nCOPY [\"model2.bin\", \"dv.bin\", \"./\"]\nthen I got the error above in MINGW64 (git bash) on Windows.\nThe temporary solution I found was to use\nCOPY [\"*\", \"./\"]\nwhich I assume combines all the files from the original docker image and the files in your working directory.\nAdded by Muhammed Tan",
        "section": "5. Deploying Machine Learning Models",
        "question": "Error: failed to compute cache key: \"/model2.bin\" not found: not found"
      },
      {
        "text": "Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\nKrishna Anand",
        "section": "5. Deploying Machine Learning Models",
        "question": "Failed to write the dependencies to pipfile and piplock file"
      },
      {
        "text": "f-String not properly keyed in: does anyone knows why i am getting error after import pickle?\nThe first error showed up because your f-string is using () instead of {} around C. So, should be: f\u2019model_C={C}.bin\u2019\nThe second error as noticed by Sriniketh, your are missing one parenthesis it should be pickle.dump((dv, model), f_out)\n(Humberto R.)",
        "section": "5. Deploying Machine Learning Models",
        "question": "f-strings"
      },
      {
        "text": "This error happens because pipenv is already installed but you can't access it from the path.\nThis error comes out if you run.\npipenv  --version\npipenv shell\nSolution for Windows\nOpen this option\nClick here\nClick in Edit Button\nMake sure the next two locations are on the PATH, otherwise, add it.\nC:\\Users\\AppData\\....\\Python\\PythonXX\\\nC:\\Users\\AppData\\....\\Python\\PythonXX\\Scripts\\\nAdded by Alejandro Aponte\nNote: this answer assumes you don\u2019t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.",
        "section": "5. Deploying Machine Learning Models",
        "question": "'pipenv' is not recognized as an internal or external command, operable program or batch file."
      },
      {
        "text": "Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\nSolution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\nAdded by Hareesh Tummala",
        "section": "5. Deploying Machine Learning Models",
        "question": "AttributeError: module \u2018collections\u2019 has no attribute \u2018MutableMapping\u2019"
      },
      {
        "text": "After entering `pipenv shell` don\u2019t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are \u201cin the shell\u201d(using Windows) at the moment as there are no clear markers for it.\nIt can also mess up PATH, if that\u2019s the case, here\u2019s terminal commands for fixing that:\n# for Windows\nset VIRTUAL_ENV \"\"\n# for Unix\nexport VIRTUAL_ENV=\"\"\nAlso manually re-creating removed folder at `C:\\Users\\username\\.virtualenvs\\removed-envname` can help, removed-envname can be seen at the error message.\nAdded by Andrii Larkin",
        "section": "5. Deploying Machine Learning Models",
        "question": "Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')"
      },
      {
        "text": "Set the host to \u20180.0.0.0\u2019 on the flask app and dockerfile then RUN the url using localhost.\n(Theresa S.)",
        "section": "5. Deploying Machine Learning Models",
        "question": "ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
      },
      {
        "text": "Solution:\nThis error occurred because I used single quotes around the filenames. Stick to double quotes",
        "section": "5. Deploying Machine Learning Models",
        "question": "docker  build ERROR [x/y] COPY \u2026"
      },
      {
        "text": "I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn\u2019t resolve it. But the following switch to the pipenv installation worked\nRUN pipenv install --system --deploy --ignore-pipfile",
        "section": "5. Deploying Machine Learning Models",
        "question": "Fix error during installation of Pipfile inside Docker container"
      },
      {
        "text": "Solution\nThis error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn\u2019t let me remove the orphan container. So I did the following\nRunning the following commands\ndocker ps -a <to list all docker containers>\ndocker images <to list images>\ndocker stop <container ID>\ndocker rm <container ID>\ndocker rmi image\nI rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.",
        "section": "5. Deploying Machine Learning Models",
        "question": "How to fix error after running the Docker run command"
      },
      {
        "text": "I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\nError message:\nError response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\nSolution description\nIssue has been resolved running the following command:\ndocker kill $(docker ps -q)\nhttps://github.com/docker/for-win/issues/2722\nAsia Saeed",
        "section": "5. Deploying Machine Learning Models",
        "question": "Bind for 0.0.0.0:9696 failed: port is already allocated"
      },
      {
        "text": "I was getting the error on client side with this\nClient Side:\nFile \"C:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen \u2026\u2026\u2026\u2026\u2026\u2026\u2026..\nraise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\nSevrer Side:\nIt showed error for gunicorn\nThe waitress  cmd was running smoothly from server side\nSolution:\nUse the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\nAamir Wani",
        "section": "5. Deploying Machine Learning Models",
        "question": "Bind for 127.0.0.1:5000 showing error"
      },
      {
        "text": "Install it by using command\n% brew install md5sha1sum\nThen run command to check hash for file to check if they the same with the provided\n% md5sum model1.bin dv.bin\nOlga Rudakova",
        "section": "5. Deploying Machine Learning Models",
        "question": "Installing md5sum on Macos"
      },
      {
        "text": "Problem description:\nI started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\nSolution description:\nJust open another terminal (command window, powershell, etc.) and run a python script.\nAlena Kniazeva",
        "section": "5. Deploying Machine Learning Models",
        "question": "How to run a script while a web-server is working?"
      },
      {
        "text": "Problem description:\nIn video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\nUserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\nSolution description:\nWhen you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\nBhaskar Sarma",
        "section": "5. Deploying Machine Learning Models",
        "question": "Version-conflict in pipenv"
      },
      {
        "text": "If you install packages via pipenv install, and get an error that ends like this:\npipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\npython_full_version: 'python_version' must not be present with 'python_full_version'\npython_version: 'python_full_version' must not be present with 'python_version'\nDo this:\nopen Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\nType pipenv lock to create the Pipfile.lock.\nDone. Continue what you were doing",
        "section": "5. Deploying Machine Learning Models",
        "question": "Python_version and Python_full_version error after running pipenv install:"
      },
      {
        "text": "If during running the  docker build command, you get an error like this:\nYour Pipfile.lock (221d14) is out of date. Expected: (939fe0).\nUsage: pipenv install [OPTIONS] [PACKAGES]...\nERROR:: Aborting deploy\nOption 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\nOption 2:  If it still doesn\u2019t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\npipenv  --rm\nrm Pipfile*",
        "section": "5. Deploying Machine Learning Models",
        "question": "Your Pipfile.lock (221d14) is out of date (during Docker build)"
      },
      {
        "text": "Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\nAdded by \ud83c\udd71\ud83c\udd7b\ud83c\udd70\ud83c\udd80",
        "section": "5. Deploying Machine Learning Models",
        "question": "You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run."
      },
      {
        "text": "Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\nAdded by Edidiong Esu",
        "section": "5. Deploying Machine Learning Models",
        "question": "Completed creating the environment locally but could not find the environment on AWS."
      },
      {
        "text": "Running 'pip install waitress' as a command on GitBash was not downloading the executable file 'waitress-serve.exe'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\nopen a Jupyter notebook and run the same command ' pip install waitress'. This way the executable file will be downloaded. The notebook may give you this warning : 'WARNING: The script waitress-serve.exe is installed in 'c:\\Users\\....\\anaconda3\\Scripts' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.'\nAdd the path where 'waitress-serve.exe' is installed into gitbash's PATH as such:\nenter the following command in gitbash: nano ~/.bashrc\nadd the path to 'waitress-serve.exe' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\nclose gitbash and open it again and you should be good to go\nAdded by Bachar Kabalan",
        "section": "5. Deploying Machine Learning Models",
        "question": "Installing waitress on Windows via GitBash: \u201cwaitress-serve\u201d command not found"
      },
      {
        "text": "Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\nThis is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\nhttps://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\nBut one can proceed without addressing it.\nAdded by Abhirup Ghosh",
        "section": "5. Deploying Machine Learning Models",
        "question": "Warning: the environment variable LANG is not set!"
      },
      {
        "text": "The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\nAdded by Quinn Avila",
        "section": "5. Deploying Machine Learning Models",
        "question": "Module5 HW Question 6"
      },
      {
        "text": "https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO\nAdded by Dawuta Smit",
        "section": "5. Deploying Machine Learning Models",
        "question": "Terminal Used in Week 5 videos:"
      },
      {
        "text": "Question:\nWhen running\npipenv run waitress-serve --listen=localhost:9696 q4-predict:app\nI get the following:\nThere was an exception (ValueError) importing your module.\nIt had these arguments:\n1. Malformed application 'q4-predict:app'\nAnswer:\nWaitress doesn\u2019t accept a dash in the python file name.\nThe solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\nAdded by Alex Litvinov",
        "section": "5. Deploying Machine Learning Models",
        "question": "waitress-serve shows Malformed application"
      },
      {
        "text": "I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running \u2018curl\u2019. \n(Used with WSL2 on Windows, should also work on Linux and MacOS)\ncurl --json '<json data>' <url>\n# piping the structure to the command\ncat <json file path> | curl --json @- <url>\necho '<json data>' | curl --json @- <url>\n# example using piping\necho '{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}'\\\n| curl --json @- http://localhost:9696/predict\nAdded by Sylvia Schmitt",
        "section": "5. Deploying Machine Learning Models",
        "question": "Testing HTTP POST requests from command line using curl"
      },
      {
        "text": "Question:\nWhen executing\neb local run  --port 9696\nI get the following error:\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\nAnswer:\nThere are two options to fix this:\nRe-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\nEdit the \u2018.elasticbeanstalk/config.yml\u2019 directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\nThe disadvantage of the second approach is that the option might not be available the following years\nAdded by Alex Litvinov\nAn alternative solution is to re-run the init command but change the value after the -p flag from docker to a string like \"Docker running on 64bit Amazon Linux\". Then, re-run the first command. For example:\neb init -p \"Docker running on 64bit Amazon Linux\" <appname>\neb local run --port 9696\nAdded by Lynn Samson\nOriginal solution from Stack Overflow",
        "section": "5. Deploying Machine Learning Models",
        "question": "NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms."
      },
      {
        "text": "You need to include the protocol scheme: 'http://localhost:9696/predict'.\nWithout the http:// part, requests has no idea how to connect to the remote server.\nNote that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won\u2019t find the http:// connection adapter either.\nAdded by George Chizhmak",
        "section": "5. Deploying Machine Learning Models",
        "question": "Requests Error: No connection adapters were found for 'localhost:9696/predict'."
      },
      {
        "text": "While running the docker image if you get the same result check which model you are using.\nRemember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\nAdded by Ahmed Okka",
        "section": "5. Deploying Machine Learning Models",
        "question": "Getting the same result"
      },
      {
        "text": "Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal",
        "section": "5. Deploying Machine Learning Models",
        "question": "Trying to run a docker image I built but it says it\u2019s unable to start the container process"
      },
      {
        "text": "You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\ndocker cp /path/t\u00b2o/local/file_or_directory container_id:/path/in/container\nHrithik Kumar Advani",
        "section": "5. Deploying Machine Learning Models",
        "question": "How do I copy files from my local machine to docker container?"
      },
      {
        "text": "You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan",
        "section": "5. Deploying Machine Learning Models",
        "question": "How do I copy files from a different folder into docker container\u2019s working directory?"
      },
      {
        "text": "I struggled with the command :\neb init -p docker tumor-diagnosis-serving -r eu-west-1\nWhich resulted in an error when running : eb local run --port 9696\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\nI replaced it with :\neb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\nThis allowed the recognition of the Dockerfile and the build/run of the docker container.\nAdded by M\u00e9lanie Fouesnard",
        "section": "5. Deploying Machine Learning Models",
        "question": "I can\u2019t create the environment on AWS Elastic Beanstalk with the command proposed during the video"
      },
      {
        "text": "I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\nERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\nI did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\nAdded by M\u00e9lanie Fouesnard\nWhen creating and launching an AWS Elastic Bean environment with eb create why does this error show up \u201cERROR: CommandError - git could not find the HEAD; most likely because there are no commits present\u201d\nThe error indicates that your project directory has not been initialized as a Git repository or is in a \"detached HEAD\" state. Elastic Beanstalk's CLI (eb) relies on Git for managing application versions, so resolving this issue is necessary. Here is how you can fix it:\nCheck if Git is initialized in your project directory by running: git status\nIf Git is not initialized, you\u2019ll see an error or a message indicating no repository exists. Initialize it: git init\nIf the Git repository exists but doesn\u2019t have any commits, create an initial commit:\no   git add .\no   git commit -m \"Initial commit\"\nThis will allow Elastic Beanstalk to track your project files\nIf Git reports that it is in a \"detached HEAD\" state, switch to a valid branch:\no   Create a new branch (if none exists): git checkout -b main\no   Or switch to an existing branch: git checkout main\nIf the warnings persist, reinitialize Elastic Beanstalk to ensure it\u2019s correctly configured: eb init\nRetry the Deployment by running the eb command again: eb create <env_name> --enable-spot\n(added by Siddhartha Gogoi)",
        "section": "5. Deploying Machine Learning Models",
        "question": "Dockerfile missing when creating the AWS ElasticBean environment"
      },
      {
        "text": "When you make local changes to Dockerfile or any other files and do not commit the changes, AWS won\u2019t  deploy the changes. The reason is that by default, the EB CLI deploys the latest commit in the current branch. If you want to deploy to your environment without committing, you can use the \u2013stage option to deploy changes that have been added to the staging area.\nIf the docker image creation fails during the \u201ceb create\u201d process, you can still create the image and deploy it by running eb deploy.\nTo deploy changes without committing\nAdd new and changed files to the staging area:\n~/eb$ git add .\nDeploy the staged changes with eb deploy:\n~/eb$ eb deploy --staged\n(added by Karina)",
        "section": "5. Deploying Machine Learning Models",
        "question": "Why doesn\u2019t the eb create command use the latest version of my Dockerfile?"
      },
      {
        "text": "Create your environment using the --enable-spot flag which automatically uses Launch Templates\nExample: eb create med-app-env --enable-spot\nAnother option is to run only eb create and follow the wizard options:\nEnter Environment Name\n(default is churn-serving-dev): churn-serving-dev\nEnter DNS CNAME prefix\n(default is churn-serving-dev): churn-serving-dev\nSelect a load balancer type\n1) classic\n2) application\n3) network\n(default is 2): 1\nWould you like to enable Spot Fleet requests for this environment? (y/N): y\nEnter a list of one or more valid EC2 instance types separated by commas (at least two instance types are recommended).\n(Defaults provided on Enter): just type \u2018enter\u2019",
        "section": "5. Deploying Machine Learning Models",
        "question": "Elastic Beanstalk \u2018eb create\u2019: ERROR   Creating Auto Scaling launch configuration failed Reason: Resource handler returned message: \"The Launch Configuration creation operation is not available in your account. Use launch templates to create configuration templates for your Auto Scaling groups."
      },
      {
        "text": "Starting on October 1, 2024, the Amazon EC2 Auto Scaling service will no longer support the creation of launch configurations for new accounts. This change is due to launch configurations being phased out and replaced by launch templates by the Amazon EC2 Auto Scaling service.\nFor more details refer to: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-autoscaling-launch-templates.html\nThis replacement of launch configurations by launch templates is what caused the error described previously (\u201c...use launch templates to create configuration templates for your Auto Scaling groups)\n(added by Karina)",
        "section": "5. Deploying Machine Learning Models",
        "question": "AWS Discontinues Support for Launch Configurations"
      },
      {
        "text": "In case of encounter this error when following the tutorial about deploying app to AWS. When you enter the command \u201ceb create churn-prediction-env\u201d it will prompt created successfully but later shows an error of VPC configuration, there is no default VPC for the selected region, go to AWS Console, select your region from the top bar (example: us-east-2), search for VPC and from the left menu go to \u201cYour VPCs\u201d. It is possible that you dont have any at this point, the action to create default VPC will be available, click on it and run the command again.\n(Added by Kelly Vergara)",
        "section": "5. Deploying Machine Learning Models",
        "question": "Default VPC Error when deploying to AWS Elastic Beanstalk:"
      },
      {
        "text": "Gunicorn is a Python WSGI HTTP server that is more suitable for production than the default Flask development server:\nPerformance:\nBetter at handling multiple simultaneous requests.\nStability:\nMore robust and can manage worker processes.\nUsage:\nModify the CMD in your Dockerfile:\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:9696\", \"app:app\"]\n(added by David Peterson)",
        "section": "5. Deploying Machine Learning Models",
        "question": "What's the advantage of using Gunicorn with Flask in Docker?"
      },
      {
        "text": "This is due to the fact the Pipfile is expecting Python 3.12, but the local container is probably running an older version (mainly the one shown in video 5.6 - Environment Management : Docker, which is Python 3.8.12-slim. In order to fix this simply the `Dockerfile` to run an appropriate version, like shown below:\nFROM python:3.12.7-slim\nBoth python versions (local version, shown in Pipfile and container version) must be congruent so as to guarantee compatibility.\n(added by Jon Areas)",
        "section": "5. Deploying Machine Learning Models",
        "question": "Fix warning Warning: Python 3.12 was not found on your system\u2026 Neither 'pyenv' nor 'asdf' could be found to install Python."
      },
      {
        "text": "First of all, you should avoid being in a virtual environment when using pipenv. You can point pipenv directly to the Python 3.11 interpreter from your Conda installation:\nActivate conda env conda activate env_name\nGet python path which python\nDeactivate conda env conda deactivate\nUse pipenv with the python path found in 3 pipenv --python /path/to/python\n(added by Kemal Dahha)",
        "section": "5. Deploying Machine Learning Models",
        "question": "How to use a specific python version (e.g. 3.11) from conda with pipenv?"
      },
      {
        "text": "You could try running your homework on GitHub Codespaces instead of your local computer. In my experience, the compute resources on GitHub Codespaces are quite sufficient for Homework 5. No issues at all in terms of speed.\n(added by Victor Emenike)",
        "section": "5. Deploying Machine Learning Models",
        "question": "Pipenv is taking forever to lock file. I have deleted the lockfile, and restarted my pc. Please, what is a possible solution?"
      },
      {
        "text": "Solution: \nIf you replaced Docker desktop with \u2018lima\u2019, you may be able to create an instance of Lima using the following template. Follow the instructions listed on the page and create an instance with the template supplied. You\u2019ll have to switch your current (if any) docker context to the context associated with this new (running) image. You should be able to use \u2018svizor/zoomcamp-model:3.11.5-slim\u2019 as a base image and run your own built image without issues.\nBy Alex Khvatov\nSimple solution:\nspecify the platform. try this\ndocker run --platform linux/amd64 -it --rm -p 9696:9696 <your-docker-image-name>\nAdded by Till Meineke, solution from Aaron\nQ: Why do I get the error TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> when using xgb.DMatrix?\nA: This error occurs because recent versions of xgb.DMatrix expect the feature_names parameter to be a list of strings rather than a NumPy array. If you\u2019re following older tutorial videos, they may use feature_names=dv.get_feature_names_out() directly, which now results in this error.\nSolution: To resolve this, convert dv.get_feature_names_out() to a list using .tolist(). Here\u2019s an updated example:\n# Convert feature names to a list\nfeature_names = dv.get_feature_names_out().tolist()\n# Create DMatrix objects with the corrected feature names\ndfulltrain = xgb.DMatrix(X_full_train,\nlabel=y_full_train,\nfeature_names=feature_names)\ndtest = xgb.DMatrix(X_test,\nfeature_names=feature_names)\nExplanation: The dv.get_feature_names_out() method returns a NumPy array, but xgb.DMatrix now expects feature_names to be a list of strings. Using .tolist() converts the array to a compatible format, allowing the code to run without errors.",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Unable to complete HW 5 due to running Docker on M1 and not using Docker Desktop."
      },
      {
        "text": "Week 6 HW 2024:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2024/06-trees/homework.md\nTill Meineke\nWeek 6 HW 2023: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\nAll HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\nHW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\nEvaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\nGitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\nYouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\nFAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n~~~Nukta Bhatia~~~",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "How to get started with Week 6?"
      },
      {
        "text": "To ensure that the string values like \"None\" are treated as valid strings rather than being converted to NaN when reading a CSV file, you can read the CSV file with keep_default_na set to False and specify the values you want to consider as NaN with the na_values parameter.\nHere\u2019s an example of how to do this:\nimport pandas as pd\ndf = pd.read_csv(\"dataset_path.csv\", keep_default_na=False, na_values=['', 'NaN', 'null'])\nUsing keep_default_na=False prevents Pandas from applying its default set of NaN values, allowing \"None\" to be read as a regular string.\n(added by Karina)",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "How to ensure \"none\" values are not interpreted as NaN when reading a CSV file in Pandas"
      },
      {
        "text": "I was using Google Collab Notebook for the 2024 cohort HW 06. For the Q6 here, the\nwas not working in the Collab Notebook. This led me to find a solution which is as follows:\n# import the required libraries\nimport io\nimport sys\n# Capture output using io.StringIO\noutput_capture = io.StringIO()\nsys.stdout = output_capture  # Redirect stdout to the StringIO buffer\n# Train the model with eta=0.3\nmodel_eta_03 = xgb.train(xgb_params, dtrain, num_boost_round=num_rounds, verbose_eval=2, evals=watchlist)\n# Reset stdout\nsys.stdout = sys.__stdout__\n# Retrieve and print the captured output\ncaptured_output = output_capture.getvalue()\nAnd, we need to slightly modify the parser function for only one line:\nfor line in output.stdout.strip().split('\\n'):    # replace this line 3 in Alexey\u2019s parser function with\nfor line in output.strip().split('\\n'):\nAnd  then call the df_score_03 = parse_xgb_output(captured_output)for getting the desired dataframe.\n(Added by Siddhartha Gogoi)",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "How to fix when %%capture output is not working in Google Collab Notebook"
      },
      {
        "text": "During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\nWe can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\nAdded by Daniel Coronel",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "How to get the training and validation metrics from XGBoost?"
      },
      {
        "text": "You should create sklearn.ensemble.RandomForestRegressor object. It\u2019s rather similar to sklearn.ensemble.RandomForestClassifier for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\nAlena Kniazeva",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "How to solve regression problems with random forest in scikit-learn?"
      },
      {
        "text": "In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\nSolution description\nThe cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\nfeatures= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\nAsia Saeed\nAlternative Solution:\nIn my case the equal sign \u201c=\u201d was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\nFor me this works:\nfeatures = []\nfor f in dv.feature_names_:\nstring = f.replace(\u201c=<\u201d, \u201c-le\u201d)\nfeatures.append(string)\nPeter Ernicke",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "ValueError: feature_names must be string, and may not contain [, ] or <"
      },
      {
        "text": "If you\u2019re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\nAli Osman",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model."
      },
      {
        "text": "If you\u2019re getting TypeError:\n\u201cTypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>\u201d,\nprobably you\u2019ve done this:\nfeatures = dv.get_feature_names_out()\nIt gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\nIf you\u2019re getting ValueError:\n\u201cValueError: feature_names must be string, and may not contain [, ] or <\u201d,\nprobably you\u2019ve either done:\nfeatures = list(dv.get_feature_names_out())\nor:\nfeatures = dv.feature_names_\nreason is what you get from DictVectorizer here looks like this:\n['households',\n'housing_median_age',\n'latitude',\n'longitude',\n'median_income',\n'ocean_proximity=<1H OCEAN',\n'ocean_proximity=INLAND',\n'population',\n'total_bedrooms',\n'total_rooms']\nit has symbols XGBoost doesn\u2019t like ([, ] or <).\nWhat you can do, is either do not specify \u201cfeature_names=\u201d while creating xgb.DMatrix or:\nimport re\nfeatures = dv.feature_names_\npattern = r'[\\[\\]<>]'\nfeatures = [re.sub(pattern, '  ', f) for f in features]\nAdded by Andrii Larkin",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)"
      },
      {
        "text": "To install Xgboost, use the code below directly in your jupyter notebook:\n(Pip 21.3+ is required)\npip install xgboost\nYou can update your pip by using the code below:\npip install --upgrade pip\nFor more about xgbboost and installation, check here:\nhttps://xgboost.readthedocs.io/en/stable/install.html\nAminat Abolade",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "How to Install Xgboost"
      },
      {
        "text": "Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\nETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "What is eta in XGBoost"
      },
      {
        "text": "For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\nRandom Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\nXGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\nNote that boosting is not necessarily better than bagging.\nM\u00e9lanie Fouesnard\nBagging stands for \u201cBootstrap Aggregation\u201d - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\nBoosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\nRileen",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "What is the difference between bagging and boosting?"
      },
      {
        "text": "The line of code below convert a notebook to python script which will have the same name as the notebook with a .py extension. This should be run on the terminal\njupyter nbconvert --to python notebnook.ipynb\nOR on the jupyter notebook\nFile -> Save and Export Notebook As -> Executable Scripts. This will download the file in the download folder\nSaikou Y Bah",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "How to save/download jupyter notebook to python script"
      },
      {
        "text": "I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\nUsing the magic cell command \u201c%%capture output\u201d I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\n# This would be the content of the Jupyter Notebook cell\nfrom IPython.utils.capture import capture_output\nimport sys\ndifferent_outputs = {}\nfor i in range(3):\nwith capture_output(sys.stdout) as output:\nprint(i)\nprint(\"testing capture\")\ndifferent_outputs[i] = output.stdout\n# different_outputs\n# {0: '0\\ntesting capture\\n',\n#  1: '1\\ntesting capture\\n',\n#  2: '2\\ntesting capture\\n'}\nAdded by Sylvia Schmitt",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Capture stdout for each iterations of a loop separately"
      },
      {
        "text": "Calling roc_auc_score() to get auc is throwing the above error.\nSolution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\nroc_auc_score(y_train, y_pred)\nHareesh Tummala",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "ValueError: continuous format is not supported"
      },
      {
        "text": "When rmse stops improving means, when it stops to decrease or remains almost similar.\nPastor Soto",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?"
      },
      {
        "text": "dot_data = tree.export_graphviz(regr, out_file=None,\nfeature_names=boston.feature_names,\nfilled=True)\ngraphviz.Source(dot_data, format=\"png\")\nKrishna Anand\nfrom sklearn import tree\ntree.plot_tree(dt,feature_names=dv.feature_names_)\nAdded By Ryan Pramana",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "One of the method to visualize the decision trees"
      },
      {
        "text": "Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\nAlejandro Aponte",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "ValueError: Unknown label type: 'continuous'"
      },
      {
        "text": "When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\nSolution: try setting the random seed e.g\ndt = DecisionTreeClassifier(random_state=22)\nBhaskar Sarma",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Different values of auc, each time code is re-run"
      },
      {
        "text": "They both do the same, it's just less typing from the script.\nAsked by Andrew Katoch, Added by Edidiong Esu",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Does it matter if we let the Python file create the server or if we run gunicorn directly?"
      },
      {
        "text": "When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\n\nfrom [file name] import ping\nOlga Rudakova",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "No module named \u2018ping\u2019?"
      },
      {
        "text": "The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\nQuinn Avila",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "DictVectorizer feature names"
      },
      {
        "text": "They both do the same, it's just less typing from the script.",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Does it matter if we let the Python file create the server or if we run gunicorn directly?"
      },
      {
        "text": "This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\nYou can address this error by replacing problematic characters in the feature names with underscores, like so:\nfeatures = [f.replace('=<', '_').replace('=', '_') for f in features]\nThis code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "ValueError: feature_names must be string, and may not contain [, ] or <"
      },
      {
        "text": "To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\n1. # extract the feature importances from the model\nfeature_importances = list(zip(features_names, rdr_model.feature_importances_))\nimportance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\n2. # sort descending the dataframe by using feature_importances value\nimportance_df = importance_df.sort_values(by='feature_importances', ascending=False)\n3. # create a horizontal bar chart\nplt.figure(figsize=(8, 6))\nsns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature Names')\nplt.title('Feature Importance Chart')\nRadikal Lukafiardi",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Visualize Feature Importance by using horizontal bar chart"
      },
      {
        "text": "Instead of using np.sqrt() as the second step. You can extract it using like this way :\nmean_squared_error(y_val, y_predict_val,squared=False)\nAhmed Okka",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "RMSE using metrics.root_meas_square()"
      },
      {
        "text": "I like this visual implementation of features importance in scikit-learn library:\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\nIt actually adds std.errors to features importance -> so that you can trace stability of features (important for a model\u2019s explainability) over the different params of the model.\nIvan Brigida",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Features Importance graph"
      },
      {
        "text": "Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\nGeorge Chizhmak",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks."
      },
      {
        "text": "Information gain  in Y due to X, or the mutual information of Y and X\nWhere  is the entropy of Y. \n\nIf X is completely uninformative about Y:\nIf X is completely informative about Y: )\nHrithik Kumar Advani",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Information Gain"
      },
      {
        "text": "Filling in missing values using an entire dataset before splitting for training/testing/validation causes",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Data Leakage"
      },
      {
        "text": "Save model by calling \u2018booster.save_model\u2019, see eg\nLoad model:\nDawuta Smit",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Serialized Model Xgboost error"
      },
      {
        "text": "In lesson 6.3 around 6:00, there is an error due to missing values. Subsequently .fillna(0) is used on df_train to deal with this. However, since v1.3, support for missing values has been added for DecisionTreeClassifier and DecisionTreeRegressor. More details can be found here, under sklearn.tree.\n(added by Kemal Dahha)",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Why does DecisionTreeClassifier and DecisionTreeRegressor not throw an error when there are nan (missing) values in the feature matrix?"
      },
      {
        "text": "To pair feature names with their importance values, use dv.get_feature_names_out() to retrieve the feature names and rf.feature_importances_ for the importances. Then, combine them with zip(feature_names, importances) to view or sort by importance.\nJust as shown below:\n# Assuming rf is your RandomForest model and dv is your DictVectorizer\nfeature_names = dv.get_feature_names_out()\nfeature_importances = rf.feature_importances_\n# Pair feature names with their importance values\nfeature_importance_dict = dict(zip(feature_names, feature_importances))\n# Sort by importance (optional)\nsorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n# Display results\nfor feature, importance in sorted_feature_importance:\nprint(f\"{feature}: {importance}\")\n(added by Jon Areas)",
        "section": "6. Decision Trees and Ensemble Learning",
        "question": "Traversing feature names and feature importance values"
      },
      {
        "text": "XGBoost\u2019s performance stems from its flexibility, thanks to a range of parameters. For initial tuning, focus on:\nlearning_rate: Controls the impact of each tree. Lower values (e.g., 0.01\u20130.1) typically improve performance but require more trees (n_estimators).\nn_estimators: Sets the number of boosting rounds; adjust this in conjunction with learning_rate.\nmax_depth: Prevents overfitting by limiting the tree\u2019s depth.\nsubsample: Dictates the fraction of samples used for training each tree, adding randomness to improve generalization.\nBegin with these parameters before exploring others like gamma and min_child_weight for additional control over model complexity and performance.\n(added by David Peterson)\nThis section is moved to Projects",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Why does XGBoost have so many parameters, and which are the most critical to start with?"
      },
      {
        "text": "Week 8 HW 2024:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2024/08-deep-learning/homework.md\nTill Meineke",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How to get started with Week 8?"
      },
      {
        "text": "Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How to setup TensorFlow with GPU support on Ubuntu?"
      },
      {
        "text": "Create or import your notebook into Kaggle.\nClick on the Three dots at the top right hand side\nClick on Accelerator\nChoose T4 GPU\nKhurram Majeed",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How to use Kaggle for Deep Learning?"
      },
      {
        "text": "Create or import your notebook into Google Colab.\nClick on the Drop Down at the top right hand side\nClick on \u201cChange runtime type\u201d\nChoose T4 GPU\nKhurram Majeed",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How to use Google Colab for Deep Learning?"
      },
      {
        "text": "Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\nSolution description: Follow the instructions in these github docs to create an SSH private and public key:\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\ny-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\nThen the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\nOr alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\nClick on your username and on manage\nDown below you will see the Git SSH keys section.\nCopy the default public key provided by Saturn Cloud\nPaste these key into the SSH keys section of your github repo\nOpen a terminal on Saturn Cloud and run this command \u201cssh -T git@github.com\u201d\nYou will receive a successful authentication notice.\nOdimegwu David\nQ: What versions of TensorFlow and NumPy should I use to ensure compatibility in Saturn Cloud notebooks?\nA: To avoid compatibility issues when using TensorFlow in Saturn Cloud notebooks, we recommend the following versions:\nbash\nCopy code\n!pip install numpy==1.24 tensorflow==2.10.0 These versions are tested and work seamlessly in the Saturn Cloud environment. Once installed, you should be able to develop and run your machine learning workflows without any issues.\nAdded by Abdiaziz Qaladid",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How do I push from Saturn Cloud to Github?"
      },
      {
        "text": "This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\nbut the location shown in the video is no longer correct.\nThis template has been moved to \u201cpython deep learning tutorials\u2019 which is shown on the Saturn Cloud home page.\nSteven Christolis\nYou can follow the updated (Nov 2024) text instructions for setup SaturnCloud with tensorflow and GPU under the above given link.\nTill Meineke",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Where is the Python TensorFlow template on Saturn Cloud?"
      },
      {
        "text": "The above error happens since module scipy is not installed in the saturn cloud tensorflow image. While creating the Jupyter server resource, in the \u201cExtra Packages\u201d section under pip in the textbox write scipy. Below the textbox, the pip install scipy command will be displayed. This will ensure when the resource spins up, the scipy package will be automatically installed. This approach can also be followed for additional python packages.\nSumeet Lalla",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Getting error module scipy not found during model training in Saturn Cloud tensorflow image"
      },
      {
        "text": "Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\nYou can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\nOn your notebook run:\n!pip install -q kaggle\nGo to Kaggle website (you need to have an account for this):\nClick on your profile image -> Account\nScroll down to the API box\nClick on Create New API token\nIt will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\nOn the notebook click on folder icon on the left upper corner\nThis will take you to the root folder\nClick on the .kaggle folder\nOnce inside of the .kaggle folder upload the kaggle.json file that you downloaded\nRun this command on your notebook:\n!chmod 600 /home/jovyan/.kaggle/kaggle.json\nDownload the data using this command:\n!kaggle datasets download -d agrigorev/dino-or-dragon\nCreate a folder to unzip your files:\n!mkdir data\nUnzip your files inside that folder\n!unzip dino-or-dragon.zip -d data\nPastor Soto",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How to upload kaggle data to Saturn Cloud?"
      },
      {
        "text": "In order to run tensorflow with gpu on your local machine you\u2019ll need to setup cuda and cudnn.\nThe process can be overwhelming. Here\u2019s a simplified guide\nOsman Ali",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How to install CUDA & cuDNN on Ubuntu 22.04"
      },
      {
        "text": "Problem description:\nWhen loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\nSolution description:\nBefore loading model need to evaluate the model on input data: model.evaluate(train_ds)\nAdded by Vladimir Yesipov",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model."
      },
      {
        "text": "Problem description:\nWhen follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\nSolution description:\nAlternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn\u2019s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\nAdded by Ryan Pramana",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Getting error when connect git on Saturn Cloud: permission denied"
      },
      {
        "text": "Problem description:\nGetting an error using <git clone git@github.com:alexeygrigorev/clothing-dataset-small.git>\nThe error:\nCloning into 'clothing-dataset'...\nHost key verification failed.\nfatal: Could not read from remote repository.\nPlease make sure you have the correct access rights\nand the repository exists.\nSolution description:\nwhen cloning the repo, you can also chose https - then it should work. This happens when you don't have your ssh key configured.\n<git clone https://github.com/alexeygrigorev/clothing-dataset-small.git>\nAdded by Gregory Morris",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Host key verification failed."
      },
      {
        "text": "Problem description\nThe accuracy and the loss are both still the same or nearly the same while training.\nSolution description\nIn the homework, you should set class_mode='binary' while reading the data.\nAlso, problem occurs when you choose the wrong optimizer, batch size, or learning rate\nAdded by Ekaterina Kutovaia",
        "section": "8. Neural Networks and Deep Learning",
        "question": "The same accuracy on epochs"
      },
      {
        "text": "Problem:\nWhen resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 \u2013 i.e. the model becomes as good as a random coin flip.\nSolution:\nCheck that the augmented ImageDataGenerator still includes the option \u201crescale\u201d as specified in the preceding step.\nAdded by Konrad M\u00fchlberg",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Model breaking after augmentation \u2013 high loss + bad accuracy"
      },
      {
        "text": "While doing:\nimport tensorflow as tf\nfrom tensorflow import keras\nmodel = tf.keras.models.load_model('model_saved.h5')\nIf you get an error message like this:\nValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\nSolution:\nSaving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\n# model architecture:\ninputs = keras.Input(shape=(input_size, input_size, 3))\nbase = base_model(inputs, training=False)\nvectors = keras.layers.GlobalAveragePooling2D()(base)\ninner = keras.layers.Dense(size_inner, activation='relu')(vectors)\ndrop = keras.layers.Dropout(droprate)(inner)\noutputs = keras.layers.Dense(10)(drop)\nmodel = keras.Model(inputs, outputs)\n(Memoona Tahira)",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Missing channel value error while reloading model:"
      },
      {
        "text": "Problem:\nA dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you\u2019ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\nSolution:\nExecute the next cell:\n%%capture\n! unzip zipped_folder_name.zip -d destination_folder_name\nAdded by Alena Kniazeva\nInside a Jupyter Notebook:\nimport zipfile\nlocal_zip = 'data.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('data')\nzip_ref.close()",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How to unzip a folder with an image dataset and suppress output?"
      },
      {
        "text": "Problem:\nWhen we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\nSolution:\nThe name of class is the folder name\nIf you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\na clear explanation below:\nhttps://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\nAdded by Bhaskar Sarma\nError with Tensorflow importing using saturn Cloud preconfigured template:\nTypeError: Unable to convert function return value to a Python type! The signature was \t() -> handle.\nSuccessful Dependency Reinstallation:\nJust try uninstalling and re-installing tensorflow you got from saturnCloud twice or thrice.\nWhen you uninstalled and reinstalled TensorFlow, it might have resolved underlying issues with dependencies. TensorFlow relies on several libraries (e.g., protobuf, numpy, grpcio) that can sometimes cause conflicts. By reinstalling, you ensured that all dependencies were reinstalled and aligned with the correct version, allowing TensorFlow to work as expected.\nLate Recognition of the Change:\nIn some cases, it might take a moment for the system to recognize the changes in the Python environment after installing packages. It could be that the environment was still \"stuck\" in an old state before, and after several installation attempts, the correct state was finally loaded, especially after restarting the Jupyter kernel.\nAdded by Abdiaziz Qaladid",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How keras flow_from_directory know the names of classes in images?"
      },
      {
        "text": "Problem:\nI created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\nSolution:\nInstall the module in a new cell: !pip install scipy\nRestart the kernel and fit the model again\nAdded by Erick Calderin",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Error with scipy missing module in SaturnCloud"
      },
      {
        "text": "The command to read folders in the dataset in the tensorflow source code is:\nfor subdir in sorted(os.listdir(directory)):\n\u2026\nReference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\nThis means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\nWhen a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\nprob(class(0)) = 1- prob(class(1))\nIn case of using from_logits to get results, you will get two values for each of the labels.\nA prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\n(Added by Memoona Tahira)",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:"
      },
      {
        "text": "Problem:\nI found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\nSolution:\nTry running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run\u2019s faster than just CPU\nAdded by Quinn Avila",
        "section": "8. Neural Networks and Deep Learning",
        "question": "What if your accuracy and std training loss don\u2019t match HW?"
      },
      {
        "text": "When running \u201cmodel.fit(...)\u201d an additional parameter \u201cworkers\u201d can be specified for speeding up the data loading/generation. The default value is \u201c1\u201d. Try out which value between 1 and the cpu count on your system performs best.\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\nAdded by Sylvia Schmitt",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Using multi-threading for data generation in \u201cmodel.fit()\u201d"
      },
      {
        "text": "Reproducibility for training runs can be achieved following these instructions: \nhttps://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\nseed = 1234\ntf.keras.utils.set_random_seed(seed)\ntf.config.experimental.enable_op_determinism()\nThis will work for a script, if this gets executed multiple times.\nAdded by Sylvia Schmitt",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Reproducibility with TensorFlow using a seed point"
      },
      {
        "text": "Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\nhttps://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\nThe functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\nM\u00e9lanie Fouesnard",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Can we use pytorch for this lesson/homework ?"
      },
      {
        "text": "While training a Keras model you get the error \u201cFailed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>\u201d you may have unintentionally passed the image generator instead of the dataset to the model\ntrain_gen = ImageDataGenerator(rescale=1./255)\ntrain_ds = train_gen.flow_from_directory(\u2026)\nhistory_after_augmentation = model.fit(\ntrain_gen, # this should be train_ds!!!\nepochs=10,\nvalidation_data=test_gen # this should be test_ds!!!\n)\nThe fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\nAdded by Tzvi Friedman",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Keras model training fails with \u201cFailed to find data adapter\u201d"
      },
      {
        "text": "The command \u2018nvidia-smi\u2019 has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command \u2018watch\u2019.\nnvidia-smi -l <N seconds>\nThe following command will run \u2018nvidia-smi\u2019 every 2 seconds until interrupted using CTRL+C.\nnvidia-smi -l 2\nAdded by Sylvia Schmitt",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Running \u2018nvidia-smi\u2019 in a loop without using \u2018watch\u2019"
      },
      {
        "text": "The Python package \u2018\u2019 is an interactive GPU process viewer similar to \u2018htop\u2019 for CPU.\nhttps://pypi.org/project//\nImage source: https://pypi.org/project//\nAdded by Sylvia Schmitt",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Checking GPU and CPU utilization using \u2018nvitop\u2019"
      },
      {
        "text": "Let\u2019s say we define our Conv2d layer like this:\n>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\nIt means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer\u2019s width) is 32.\nIf we check model.summary() we will get this:\n_________________________________________________________________\nLayer (type)                Output Shape              Param #\n=================================================================\nconv2d (Conv2D)             (None, 148, 148, 32)      896\nSo where does 896 params come from? It\u2019s computed like this:\n>>> (3*3*3 +1) * 32\n896\n# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\nWhat about the number of \u201cfeatures\u201d we get after the Flatten layer?\nFor our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\n_________________________________________________________________\nLayer (type)                Output Shape              Param #\n=================================================================\nmax_pooling2d_3       (None, 7, 7, 128)         0\nflatten (Flatten)           (None, 6272)              0\nSo where do 6272 vectors come from? It\u2019s computed like this:\n>>> 7*7*128\n6272\n# 7x7 \u201cimage shape\u201d after several convolutions and poolings, 128 filters\nAdded by Andrii Larkin",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Q: Where does the number of Conv2d layer\u2019s params come from? Where does the number of \u201cfeatures\u201d we get after the Flatten layer come from?"
      },
      {
        "text": "It\u2019s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\nYou can simply start from an \u201cempty\u201d model and add more and more layers in a sequential order.\nThis mode is called \u201cSequential Model API\u201d  (easier)\nIn Alexey\u2019s videos it is implemented as chained calls of different entities (\u201cinputs\u201d,\u201cbase\u201d, \u201cvectors\u201d,  \u201coutputs\u201d) in a more advanced mode \u201cFunctional Model API\u201d.\nMaybe a more complicated way makes sense when you do Transfer Learning and want to separate \u201cBase\u201d model vs. rest, but in the HW you need to recreate the full model from scratch \u21d2 I believe it is easier to work with a sequence of \u201csimilar\u201d layers.\nYou can read more about it in this TF2 tutorial.\nA really useful Sequential model example is shared in the Kaggle\u2019s \u201cBee or Wasp\u201d dataset folder with code: notebook\nAdded by Ivan Brigida\nFresh Run on Neural Nets\nWhile correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\nAdded by Abhijit Chakraborty",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Sequential vs. Functional Model Modes in Keras (TF2)"
      },
      {
        "text": "I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n```\nphysical_devices = tf.configlist_physical_devices('GPU')\ntry:\ntf.config.experimental.set_memory_growth(physical_devices[0],True)\nexcept:\n# Invalid device or cannot modify virtual devices once initialized.\npass\n```",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Out of memory errors when running tensorflow"
      },
      {
        "text": "To address the out of memory (OOM) issue, I followed these steps:\n1. Check GPU Memory Usage:\nI ran the following command to see how much memory was being used and which processes were occupying it:\n!nvidia-smi\nThis command provided details about memory usage and active processes on the GPU.\n2. Identify Active Processes:\nFrom the output of nvidia-smi, I noticed that a Python process (...a3/envs/tensorflow2_p310/bin/python) was consuming a significant amount of GPU memory.\n3. Terminate the Python Process:\nI used the process ID (PID) to kill the Python process that was consuming the excessive memory. For example, to kill a process with PID 11208, I executed:\n!kill 11208\n4. Kernel Restart:\nAfter terminating the process, I noticed that the kernel automatically restarted, freeing up the GPU memory.\n5. Recheck GPU Memory:\nI ran nvidia-smi again to confirm that the memory usage had decreased, and there were no longer any blocking processes.\nBy following these steps, I was able to free up GPU memory and continue training my model successfully.\n(added by Karina)",
        "section": "8. Neural Networks and Deep Learning",
        "question": "How did I resolve the out of memory (OOM) issue when training my model on a GPU?"
      },
      {
        "text": "When training the models, in the fit function, you can specify the number of workers/threads.\nThe number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\nI changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\nAdded by Ibai Irastorza",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Model training very slow in google colab with T4 GPU"
      },
      {
        "text": "From the keras documentation:\nDeprecated: tf.keras.preprocessing.image.ImageDataGenerator is not recommended for new code. Prefer loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers. For more information, see the tutorials for loading images and augmenting images, as well as the preprocessing layer guide.\nHrithik Kumar Advani",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images"
      },
      {
        "text": "This error occurs because the OpenSSH client is built against a specific version of OpenSSL (e.g., 3.0.0), but the system tries to use a different version (e.g., 3.0.3). This mismatch prevents the SSH client from working properly.\nSolution:\nSet the correct OpenSSL library path by running the following line in the terminal:\nexport LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu\nAdded by Kemal Dahha",
        "section": "8. Neural Networks and Deep Learning",
        "question": "Saturn Cloud: OpenSSL version mismatch. Built against 30000020, you have 30300020"
      },
      {
        "text": "Data augmentation artificially expands the training dataset by applying transformations like flipping, cropping, and adjusting brightness or contrast. This improves model robustness by exposing it to varied data and helps reduce overfitting.\nAdded by David Peterson",
        "section": "9. Serverless Deep Learning",
        "question": "How can data augmentation improve model performance?"
      },
      {
        "text": "Week 9 HW 2024:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2024/09-serverless/homework.md\nHW Submission:\nhttps://courses.datatalks.club/ml-zoomcamp-2024/homework/hw09\nImportant for environment setup:  Python 3.12 vs TF Lite 2.17\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/09-serverless/updates.md\nGithub repo:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/tree/master/09-serverless\nTill Meineke",
        "section": "9. Serverless Deep Learning",
        "question": "How to get started with Week 9?"
      },
      {
        "text": "The week 9 uses a link to github to fetch the models.\nThe original link was moved to here:\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/releases",
        "section": "9. Serverless Deep Learning",
        "question": "Where is the model for week 9?"
      },
      {
        "text": "Solution description\nIn the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\nI also had the same problem on Ubuntu terminal. I executed the following two commands:\n$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n$ echo $REMOTE_URI\n111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\nNote: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\n2. Replace REMOTE_URI with your URI\n(Bhaskar Sarma)",
        "section": "9. Serverless Deep Learning",
        "question": "Executing the command echo ${REMOTE_URI} returns nothing."
      },
      {
        "text": "The command aws ecr get-login --no-include-email returns an invalid choice error:\nThe solution is to use the following command instead:  aws ecr get-login-password\nCould simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\nexport PASSWORD=`aws ecr get-login-password`\ndocker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\nAdded by Martin Uribe",
        "section": "9. Serverless Deep Learning",
        "question": "Getting a syntax error while trying to get the password from aws-cli"
      },
      {
        "text": "We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\nKrishna Anand",
        "section": "9. Serverless Deep Learning",
        "question": "Pass many parameters in the model at once"
      },
      {
        "text": "This error is produced sometimes when building your docker image from the Amazon python base image.\nSolution description: The following could solve the problem.\nUpdate your docker desktop if you haven\u2019t done so.\nOr restart docker desktop and terminal and then build the image all over again.\nOr if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\n(optional) Added by Odimegwu David\nQ: How can I deploy a machine learning model using AWS Lambda and API Gateway?\nTo deploy a machine learning model using AWS Lambda and API Gateway, follow these steps:\nPrepare the Code for Lambda: Start by developing and testing your machine learning model (e.g., using TensorFlow Lite) locally. Ensure the model is optimized for serverless execution.\nUse AWS Lambda: AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. You can upload your code, which may include the model, dependencies, and necessary libraries.\nPrepare a Docker Image (Optional): If your model requires custom dependencies or environments, you can package your application in a Docker image, upload it to Amazon Elastic Container Registry (ECR), and then link it to AWS Lambda.\nCreate the Lambda Function: After preparing your code or Docker image, create a Lambda function that will process the requests and return results.\nExpose the Lambda Function using API Gateway: Set up an API Gateway to expose the Lambda function as a RESTful endpoint. This allows you to interact with your model via HTTP requests.\nBy following these steps, you can deploy and expose your machine learning model in a scalable and efficient serverless architecture using AWS Lambda and API Gateway.\n- Added by Abdiaziz Qaladid",
        "section": "9. Serverless Deep Learning",
        "question": "Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8"
      },
      {
        "text": "When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says \u201c'ls' is not recognized as an internal or external command,operable program or batch file.\nSolution description :\nInstead of !ls -lh , you can use this command !dir , and you will get similar output\nAsia Saeed",
        "section": "9. Serverless Deep Learning",
        "question": "Problem: 'ls' is not recognized as an internal or external command, operable program or batch file."
      },
      {
        "text": "When I run   import tflite_runtime.interpreter as tflite , I get an error message says \u201cImportError: generic_type: type \"InterpreterWrapper\" is already registered!\u201d\nSolution description\nThis error occurs when you import both tensorflow  and tflite_runtime.interpreter  \u201cimport tensorflow as tf\u201d and \u201cimport tflite_runtime.interpreter as tflite\u201d in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\nAsia Saeed",
        "section": "9. Serverless Deep Learning",
        "question": "ImportError: generic_type: type \"InterpreterWrapper\" is already registered!"
      },
      {
        "text": "Problem description:\nIn command line try to do $ docker build -t dino_dragon\ngot this Using default tag: latest\n[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\nerror during connect: This error may indicate that the docker daemon is not running.: Post\n.\nSolution description:\nYou need to make sure that Docker is not stopped by a third-party program.\nAndrei Ilin",
        "section": "9. Serverless Deep Learning",
        "question": "Windows version might not be up-to-date"
      },
      {
        "text": "When running docker build -t dino-dragon-model it returns the above error\nThe most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\nPastor Soto",
        "section": "9. Serverless Deep Learning",
        "question": "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available"
      },
      {
        "text": "Problem description:\nIn video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\nSolution description:\nYes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\nAdded by Bhaskar Sarma",
        "section": "9. Serverless Deep Learning",
        "question": "How to do AWS configure after installing awscli"
      },
      {
        "text": "Problem:\nWhile passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\n{\u2018errorMessage\u2019: \u2018Unable to marshal response: Object of type float32 is not JSON serializable\u2019, \u2018errorType\u2019: \u2018Runtime.MarshalError\u2019, \u2018requestId\u2019: \u2018f155492c-9af2-4d04-b5a4-639548b7c7ac\u2019, \u2018stackTrace\u2019: []}\nThis happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become \u201cserializable\u201d.\nSolution:\nIn my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\npreds = [interpreter.get_tensor(output_index)[0][0], \\\n1-interpreter.get_tensor(output_index)[0][0]]\nIn which case the above described solution will look like this:\npreds = [float(interpreter.get_tensor(output_index)[0][0]), \\\nfloat(1-interpreter.get_tensor(output_index)[0][0])]\nThe rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\nAdded by Konrad Muehlberg",
        "section": "9. Serverless Deep Learning",
        "question": "Object of type float32 is not JSON serializable"
      },
      {
        "text": "I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\nValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\nThis is because the X is an int but a float is expected.\nSolution:\nI found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\n# Need to convert to float32 before set_tensor\nX = np.float32(X)\nThen, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\nAdded by M\u00e9lanie Fouesnard",
        "section": "9. Serverless Deep Learning",
        "question": "Error with the line \u201cinterpreter.set_tensor(input_index, X\u201d)"
      },
      {
        "text": "To check your file size using the powershell terminal, you can do the following command lines:\n$File = Get-Item -Path path_to_file\n$FileSize = (Get-Item -Path $FilePath).Length\nNow you can check the size of your file, for example in MB:\nWrite-host \"MB\":($FileSize/1MB)\nSource: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\nAdded by M\u00e9lanie Fouesnard",
        "section": "9. Serverless Deep Learning",
        "question": "How to easily get file size in powershell terminal ?"
      },
      {
        "text": "I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\nhttps://docs.aws.amazon.com/lambda/latest/dg/images-create.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\nAdded by Alejandro aponte",
        "section": "9. Serverless Deep Learning",
        "question": "How do Lambda container images work?"
      },
      {
        "text": "The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\nhttps://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\nAdded by Sumeet Lalla",
        "section": "9. Serverless Deep Learning",
        "question": "How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?"
      },
      {
        "text": "Problem:\nWhile trying to build docker image in Section 9.5 with the command:\ndocker build -t clothing-model .\nIt throws a pip install error for the tflite runtime whl\nERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\nTry to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\nIf the link above does not work:\nThe problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\nOr try the code bellow.\nAdded by Dashel Ruiz Perez\nSolution:\nTo build the Docker image, use the command:\ndocker build --platform linux/amd64 -t clothing-model .\nTo run the built image, use the command:\ndocker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\nAdded by Daniel Egbo",
        "section": "9. Serverless Deep Learning",
        "question": "Error building docker image on M1 Mac"
      },
      {
        "text": "Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\nWith error message:\n{'message': 'Missing Authentication Token'}\nSolution:\nNeed to get the deployed API URL for the specific path you are invoking. Example:\nhttps://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\nAdded by Andrew Katoch",
        "section": "9. Serverless Deep Learning",
        "question": "Error invoking API Gateway deploy API locally"
      },
      {
        "text": "Problem: When trying to install tflite_runtime with\n!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\none gets an error message above.\nSolution:\nfflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\nyour combination must be missing here\nyou can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\nand install the needed one using pip\neg\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\nas it is done in the lectures code:\nhttps://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\nAlternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\nAdded by Alena Kniazeva, modified by Alex Litvinov",
        "section": "9. Serverless Deep Learning",
        "question": "Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)"
      },
      {
        "text": "After installing tflite runtime by using the wheel suggested in the homework 9 (https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl), I was getting a runtime error while testing the lambda handler. The error was:\n\u201c..ImportError:\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12' ..\u201d\nThe issue I was experiencing with the version of NumPy was due to it being overwritten by the installation of tflite-runtime. To prevent this from happening, you should install the wheel using the --no-deps option\nRUN pip install --no-deps https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\n(added by Karina)",
        "section": "9. Serverless Deep Learning",
        "question": "Error: A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.0 as it may crash"
      },
      {
        "text": "docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\nYou need to restart the docker services to get rid of the above error\nKrishna Anand",
        "section": "9. Serverless Deep Learning",
        "question": "Docker run error"
      },
      {
        "text": "The docker image can be saved/exported to tar format in local machine using the below command:\ndocker image save <image-name> -o <name-of-tar-file.tar>\nThe individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\nSumeet Lalla",
        "section": "9. Serverless Deep Learning",
        "question": "Save Docker Image to local machine and view contents"
      },
      {
        "text": "On vscode running jupyter notebook. After I \u2018pip install pillow\u2019, my notebook did not recognize using the import for example from PIL import image. After restarting the jupyter notebook the imports worked.\nQuinn Avila",
        "section": "9. Serverless Deep Learning",
        "question": "Jupyter notebook not seeing package"
      },
      {
        "text": "Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune",
        "section": "9. Serverless Deep Learning",
        "question": "Running out of space for AWS instance."
      },
      {
        "text": "Using the 2.14 version with python 3.11 works fine.\nIn case it doesn\u2019t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\nAdded by Abhijit Chakraborty",
        "section": "9. Serverless Deep Learning",
        "question": "Using Tensorflow 2.15 for AWS deployment"
      },
      {
        "text": "see here",
        "section": "9. Serverless Deep Learning",
        "question": "Command aws ecr get-login --no-include-email returns \u201caws: error: argument operation: Invalid choice\u2026\u201d"
      },
      {
        "text": "Sign in to the AWS Console: Log in to the AWS Console.\nNavigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\nCreate a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\nSelect the service and actions:\nClick on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\nReview and create the policy:\nClick on \"Review policy.\"\nProvide a name and description for the policy.\nClick on \"Create policy.\"\nJSON policy:\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"VisualEditor0\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ecr:CreateRepository\",\n\"ecr:GetAuthorizationToken\",\n\"ecr:BatchCheckLayerAvailability\",\n\"ecr:BatchGetImage\",\n\"ecr:InitiateLayerUpload\",\n\"ecr:UploadLayerPart\",\n\"ecr:CompleteLayerUpload\",\n\"ecr:PutImage\"\n],\n\"Resource\": \"*\"\n}\n]\n}\nAdded by: Daniel Mu\u00f1oz-Viveros\nERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\n(WSL2 system)\nSolved: Delete the file ~/.docker/config.json\nYishan Zhan",
        "section": "9. Serverless Deep Learning",
        "question": "What IAM permission policy is needed to complete Week 9: Serverless?"
      },
      {
        "text": "Add the next lines to vim /etc/docker/daemon.json\n{\n\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\n}\nThen, restart docker:  sudo service docker restart\nIbai Irastorza",
        "section": "9. Serverless Deep Learning",
        "question": "Docker Temporary failure in name resolution"
      },
      {
        "text": "Solution: add compile = False to the load_model function\nkeras.models.load_model('model_name.h5', compile=False)\nNadia Paz",
        "section": "9. Serverless Deep Learning",
        "question": "Keras model *.h5 doesn\u2019t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`"
      },
      {
        "text": "This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\nBasically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for \u201cdocker run\u201d and a certain \u201clocalhost link\u201d for testing:\ndocker run -it --rm -p 9000:8080 name\nThis command runs the image as a container and starts up an endpoint locally at:\nlocalhost:9000/2015-03-31/functions/function/invocations\nPost an event to the following endpoint using a curl command:\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\nExamples of curl testing:\n* windows testing:\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\"url\\\": \\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\"}\"\n* unix testing:\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\nIf during testing you encounter an error like this:\n# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\njust turn your response at lambda_handler() to string - str(result).\nAdded by Andrii Larkin",
        "section": "9. Serverless Deep Learning",
        "question": "How to test AWS Lambda + Docker locally?"
      },
      {
        "text": "Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\nAdded by Ryan Pramana",
        "section": "9. Serverless Deep Learning",
        "question": "\"Unable to import module 'lambda_function': No module named 'tensorflow'\" when run python test.py"
      },
      {
        "text": "I\u2019ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\nhttps://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\n\uec03%%shell\npip install udocker\nudocker --allow-root install\n\uec02!udocker --allow-root run hello-world\nAdded by Ivan Brigida",
        "section": "9. Serverless Deep Learning",
        "question": "Install Docker (udocker) in Google Colab"
      },
      {
        "text": "`Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header.`\n`Missing Authentication Token`\nimport boto3\nclient = boto3.client('apigateway')\nresponse = client.test_invoke_method(\nrestApiId='your_rest_api_id',\nresourceId='your_resource_id',\nhttpMethod='POST',\npathWithQueryString='/test/predict', #depend how you set up the api\nbody='{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n)\nprint(response['body'])\nYishan Zhan",
        "section": "9. Serverless Deep Learning",
        "question": "Lambda API Gateway errors:"
      },
      {
        "text": "To overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\nCOPY <file-name> .\nRUN pip install <file-name>\nAbhijit Chakraborty",
        "section": "9. Serverless Deep Learning",
        "question": "Unable to run pip install tflite_runtime from github wheel links?"
      },
      {
        "text": "The latest versions of TF Lite don't support Python 3.12 yet. See update.md for 2024 cohort in main repository.\nTill Meineke",
        "section": "9. Serverless Deep Learning",
        "question": "Python 3.12 vs TF Lite 2.17"
      },
      {
        "text": "To monitor Lambda deployments, use AWS CloudWatch to access detailed logs, metrics, and alarms. Metrics like invocation count, duration, error rate, and memory usage can help diagnose performance issues. Use AWS X-Ray for tracing requests and analyzing latency. For model maintenance, set up an automated CI/CD pipeline to retrain models on updated data and redeploy using tools like Amazon SageMaker or custom workflows. Regularly evaluate model performance with a monitoring service to detect drift in predictions or data quality issues.\n~ David Peterson",
        "section": "9. Serverless Deep Learning",
        "question": "How can I monitor and maintain models deployed on AWS Lambda?"
      },
      {
        "text": "Set Up SAM CLI on Your Machine\nFollow the installation guide for the AWS SAM CLI here:\nGetting started with AWS SAM\nCreate a New Project\nOpen your command prompt and run the following command to generate boilerplate code:\nsam init\nFollow the SAM CLI Wizard\nSelect \"AWS Quick Start Templates\".\nChoose \"Machine Learning\" as the application type.\nSelect the version of Python you will use for your runtime.\nWhen prompted for the starter template, choose \"TensorFlow Machine Learning Inference API\".\nAfter completing these steps, a new folder with the name you selected will be created. This will be your \"SAM project folder\" from now on. Inside this folder, you should see an \"app\" folder.\nAdd Required Files for Deployment\nMove all the files you need for deployment (such as the TensorFlow Lite model and your Lambda function) into the \"app\" folder.\nModify the Following Files Inside the \"app\" Folder\nrequirements.txt\nReplace the TensorFlow dependency with tflite-runtime, adjusting the version as necessary. You may also add any other dependencies you require, such as the requests library, and adjust the version of numpy if needed.\nExample content for requirements.txt:\n\npillow==11.1.0\nrequests==2.32.3\nnumpy==1.26.4\ntflite-runtime==2.7.0\nDockerfile\nModify the Dockerfile to copy the necessary files for your deployment after running pip install. In the default file created, it assumes that the Lambda function is in app.py and the model is inside the app/models folder. However, in this example, we assume the model is at the same level as the Lambda function.\nHere's an example of an updated Dockerfile:\n\nFROM public.ecr.aws/lambda/python:3.9\nCOPY requirements.txt ./\nRUN python3.9 -m pip install -r requirements.txt -t .\nCOPY app.py ./\nCOPY class_indices.json ./\nCOPY classification_model.tflite ./\nENV MODEL_PATH ./classification_model.tflite\nENV CLASSES_PATH ./class_indices.json\nCMD [\"app.lambda_handler\"]\nBuild the Lambda Function\nFrom the SAM project directory, build the Lambda function by running:\nsam build --build-dir .aws-build\nAfter completing this step, the Docker image will be created. You can verify this by running:\ndocker images\nTest the Lambda Function Locally\nTo test the image, you can run a container based on it and send a request to the service using a script, as we learned in class, or you can use SAM CLI as follows:\nModify the app/event/event.json file to include the JSON input expected by your Lambda function. For example:\n{\n\"url\": \"http://bit.ly/mlbookcamp-pants\"\n}\nFrom the SAM project folder, run the following command:\n\nsam local invoke -t .aws-build/template.yaml -e events/event.json\nThis will start a container, send the event, and display the response. The output will also show the name of the Docker image used for the container.\nDeploy the image\nTo deploy the image you can follow the instructions learned at classes or you can use this command and follow the prompt\nsam deploy --guided\n(AWS SAM takes care of creating and ECR repository)\n(added by Karina)",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "How to Use AWS SAM CLI to Create a Lambda Function as a Container Image"
      },
      {
        "text": "TODO",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "How to get started with Week 10?"
      },
      {
        "text": "Running a CNN on your CPU can take a long time and once you\u2019ve run out of free time on some cloud providers, it\u2019s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\nI was able to get it working by using the following resources:\nCUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\nInstall TensorFlow with pip\nStart Locally | PyTorch\nI included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\nAdded by Martin Uribe",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "How to install Tensorflow in Ubuntu WSL2"
      },
      {
        "text": "If you are running tensorflow on your own machine and you start getting the following errors:\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\nTry adding this code in a cell at the beginning of your notebook:\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\nAfter doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\nAdded by Martin Uribe",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Getting: Allocator ran out of memory errors?"
      },
      {
        "text": "In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\nTypeError: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n1. Downgrade the protobuf package to 3.20.x or lower.\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\nThis will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\npipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\nkeras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\nAdded by \u00c1ngel de Vicente",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Problem with recent version of protobuf"
      },
      {
        "text": "Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\n\u201dCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\u201d\nSolution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\nJust enable additional distros. That\u2019s all. Even if the additional distro is the same as the default WSL distro.\nOdimegwu David",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "WSL Cannot Connect To Docker Daemon"
      },
      {
        "text": "In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nAnd the targets still appear as <unknown>\nRun >>kubectl edit deploy -n kube-system metrics-server\nAnd search for this line:\nargs:\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\nAdd this line in the middle:  - --kubelet-insecure-tls\nSo that it stays like this:\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\nSave and run again >>kubectl get hpa\nAdded by Marilina Orihuela",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "HPA instance doesn\u2019t run properly"
      },
      {
        "text": "In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\n>>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\nAnd the targets still appear as <unknown>\nRun the following command:\nkubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\nWhich uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\nAdded by Giovanni Pecoraro",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "HPA instance doesn\u2019t run properly (easier solution)"
      },
      {
        "text": "When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\nERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Asia\\\\anaconda3\\\\Lib\\\\site-packages\\\\google\\\\protobuf\\\\internal\\\\_api_implementation.cp39-win_amd64.pyd'\nConsider using the `--user` option or check the permissions.\nSolution description :\nI was able to install the libraries using below command:\npip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\nAsia Saeed\nI'm trying to deploy my machine learning model using Kubernetes, but I'm getting an error stating that my Pods are not starting. What could be the problem?\nSolution:\nThis issue can be caused by several factors:\nResource Allocation: Ensure that your Pods have enough CPU and memory resources allocated. If resources are too low, the Kubernetes scheduler might fail to schedule your Pods.\nImage Issues: Verify that the Docker image specified for your Pod is correctly built and accessible. If the image cannot be pulled from the repository, the Pod won\u2019t start.\nAdded by Abdiaziz Qaladid",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Could not install packages due to an OSError: [WinError 5] Access is denied"
      },
      {
        "text": "Problem description\nI was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\nFile \"C:\\Users\\Asia\\Data_Science_Code\\Zoompcamp\\Kubernetes\\gat.py\", line 9, in <module>\nfrom tensorflow_serving.apis import predict_pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow_serving\\apis\\predict_pb2.py\", line 14, in <module>\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\", line 14, in <module>\nfrom tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 14, in <module>\nfrom tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py\", line 36, in <module>\n_descriptor.FieldDescriptor(\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 560, in __new__\n_message.Message._CheckCalledFromGeneratedFile()\nTypeError: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n1. Downgrade the protobuf package to 3.20.x or lower.\n2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\nSolution description:\nIssue has been resolved by downgrading protobuf to version 3.20.1.\npipenv install protobuf==3.20.1\nAsia Saeed",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "TypeError: Descriptors cannot not be created directly."
      },
      {
        "text": "To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\nI first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\nAt step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\nThen I added this folder path to PATH in my environment variables.\nKind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\nAdded by M\u00e9lanie Fouesnard",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "How to install easily kubectl on windows ?"
      },
      {
        "text": "First you need to launch a powershell terminal with administrator privilege.\nFor this we need to install choco library first through the following syntax in powershell:\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\nKrishna Anand",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Install kind through choco library"
      },
      {
        "text": "If you are having challenges installing Kind through the Windows Powershell as provided on the website and Choco Library as I did, you can simply install Kind through Go.\n> Download and Install Go (https://go.dev/doc/install)\n> Confirm installation by typing the following in Command Prompt -  go version\n> Proceed by installing Kind by following this command - go install sigs.k8s.io/kind@v0.20.0\n>Confirm Installation kind --version\nIt works perfectly.",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Install Kind via Go package"
      },
      {
        "text": "I ran into an issue where kubectl wasn't working.\nI kept getting the following error:\nkubectl get service\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\nI searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\nAll hogwash.\nThe solution to my problem was to just start over.\nkind delete cluster\nrm -rf ~/.kube\nkind create cluster\nNow when I try the same command again:\nkubectl get service\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\nAdded by Martin Uribe",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "The connection to the server localhost:8080 was refused - did you specify the right host or port?"
      },
      {
        "text": "Problem description\nDue to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\nMy first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn\u2019t help much.\nSolution description\n> docker images\nrevealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\na bunch of those \u2014 but to no avail!\nIt turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\n> docker system prune\nSee also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\nAdded by Konrad M\u00fchlberg",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Running out of storage after building many docker images"
      },
      {
        "text": "Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\nPastor Soto",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "In HW10 Q6 what does it mean \u201ccorrect value for CPU and memory\u201d? Aren\u2019t they arbitrary?"
      },
      {
        "text": "In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\ncpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\ncpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\nThese values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\nAdded by Andrii Larkin",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Why cpu vals for Kubernetes deployment.yaml look like \u201c100m\u201d and \u201c500m\u201d? What does \"m\" mean?"
      },
      {
        "text": "Problem: Failing to load docker-image to cluster (when you\u2019ved named a cluster)\nkind load docker-image zoomcamp-10-model:xception-v4-001\nERROR: no nodes found for cluster \"kind\"\nSolution: Specify cluster name with -n\nkind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\nAndrew Katoch",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Kind cannot load docker image"
      },
      {
        "text": "Problem: I download kind from the next command:\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\nWhen I try\nkind --version\nI get: 'kind' is not recognized as an internal or external command, operable program or batch file\nSolution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\nAlejandro Aponte",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)"
      },
      {
        "text": "Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind \u2013 Rootless (k8s.io).\nSylvia Schmitt",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Running kind on Linux with Rootless Docker or Rootless Podman"
      },
      {
        "text": "Deploy and Access the Kubernetes Dashboard\nLuke",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Kubernetes-dashboard"
      },
      {
        "text": "Make sure you are on AWS CLI v2 (check with aws --version)\nhttps://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Correct AWS CLI version for eksctl"
      },
      {
        "text": "Problem Description:\nIn video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\nSolution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\nBy running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\nAdded by Bhaskar Sarma",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask"
      },
      {
        "text": "As per AWS documentation:\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\nYou need to do: (change the fields in red)\naws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\nAlternatively you can run the following command without changing anything given you have a default region configured\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\nAdded by Humberto Rodriguez",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Command aws ecr get-login --no-include-email returns \u201caws: error: argument operation: Invalid choice\u2026\u201d"
      },
      {
        "text": "While trying to run the docker code on M1:\ndocker run --platform linux/amd64 -it --rm \\\n-p 8500:8500 \\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n-e MODEL_NAME=\"clothing-model\" \\\ntensorflow/serving:2.7.0\nIt outputs the error:\nError:\nStatus: Downloaded newer image for tensorflow/serving:2.7.0\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\nterminate called after throwing an instance of 'google::protobuf::FatalException'\nwhat():  CHECK failed: file != nullptr:\nqemu: uncaught target signal 6 (Aborted) - core dumped\n/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\nSolution\ndocker pull emacski/tensorflow-serving:latest\ndocker run -it --rm \\\n-p 8500:8500 \\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n-e MODEL_NAME=\"clothing-model\" \\\nemacski/tensorflow-serving:latest-linux_arm64\nSee more here: https://github.com/emacski/tensorflow-serving-arm\nAdded by Daniel Egbo",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac"
      },
      {
        "text": "Similar to the one above but with a different solution the main reason is that emacski doesn\u2019t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\nProblem:\nWhile trying to run the docker code on Mac M2 apple silicon:\ndocker run --platform linux/amd64 -it --rm \\\n-p 8500:8500 \\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n-e MODEL_NAME=\"clothing-model\" \\\ntensorflow/serving\nYou get an error:\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\nSolution:\nUse bitnami/tensorflow-serving base image\nLaunch it either using docker run\ndocker run -d \\\n--name tf_serving \\\n-p 8500:8500 \\\n-p 8501:8501 \\\n-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\n-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\nbitnami/tensorflow-serving:2\nOr the following docker-compose.yaml\nversion: '3'\nservices:\ntf_serving:\nimage: bitnami/tensorflow-serving:2\nvolumes:\n- ${PWD}/clothing-model:/bitnami/model-data/1\nports:\n- 8500:8500\n- 8501:8501\nenvironment:\n- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\nAnd run it with\ndocker compose up\nAdded by Alex Litvinov\nOr new since Oct 2024:\nBeta release of Docker VMM - the more performant alternative to Apple Virtualization Framework on macOS (requires Apple Silicon and macOS 12.5 or later). https://docs.docker.com/desktop/features/vmm/",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)"
      },
      {
        "text": "Problem: CPU metrics Shows Unknown\nNAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\ncredit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\nFailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\nSolution:\n-> Delete HPA (kubectl delete hpa credit-hpa)\n-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\n-> Create HPA\nThis should solve the cpu metrics report issue.\nAdded by Priya V",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "HPA doesn\u2019t show CPU metrics"
      },
      {
        "text": "This command never worked:\nkubectl autoscale deployment subscription --name subscription-hpa --cpu-percent=20 --min=1 --max=3\nGoing through the error logs, it indicated some sort of certificate validation issues because of the server's certificate not having a valid Subject Alternative Name (SAN) for the node's IP address.\nchatGPT suggested to run in terminal:\nkubectl patch deployment metrics-server -n kube-system --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--kubelet-insecure-tls\"}]'\nto skip the TLS verification and\nkubectl rollout restart deployment metrics-server -n kube-system\nto restart the deployment. then the metrics server started working.\nAvoiding TLS certificate validation may not be a good solution for production ready systems, but it would be enough for our case.\nTill Meineke, Dec 2024",
        "section": "10. Kubernetes and TensorFlow Serving",
        "question": "HW10 Autoscaling (optional) command does not work"
      },
      {
        "text": "Several tools can help set up a local Kubernetes environment:\nKind: Runs Kubernetes clusters in Docker containers, suitable for testing and development.\nMinikube: Runs a single-node Kubernetes cluster on your local machine.\nK3s: A lightweight Kubernetes distribution ideal for local development.\nMicroK8s: A minimal Kubernetes distribution for local development.\nDocker Desktop: Includes a standalone Kubernetes server and client for development.\n~ David Peterson",
        "section": "11. KServe",
        "question": "What tools are recommended for setting up a local Kubernetes environment for model deployment practice?"
      },
      {
        "text": "Problem description:\nRunning this:\ncurl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\nFails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\nCheck kubectl version with kubectl version\nSolution description\nEdit the file \u201cquick_install.bash\u201d by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\nRun the bash script now.\nAdded by Andrew Katoch",
        "section": "Projects (Midterm and Capstone)",
        "question": "Errors with istio during installation"
      },
      {
        "text": "Answer: For 2024 cohort you can see them here.\nCapstone 1:\n7 Jan 2025 due date for submission\n14 Jan 2025 due date for evaluation\nCapstone 2:\n21 Jan 2025 due date for submission\n28 Jan 2025 due date for evaluation",
        "section": "Projects (Midterm and Capstone)",
        "question": "What are the project deadlines?"
      },
      {
        "text": "Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]",
        "section": "Projects (Midterm and Capstone)",
        "question": "Are projects solo or collaborative/group work?"
      },
      {
        "text": "Answer: Ideally midterms up to module-06, capstones include all modules in that cohort\u2019s syllabus. But you can include anything  to feature. Just be sure to document anything not covered in class.\nAlso watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\nMore discussions:\n[source1] [source2] [source3]",
        "section": "Projects (Midterm and Capstone)",
        "question": "What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?"
      },
      {
        "text": "These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort\u2019s folder as well for additional or different instructions, if any.\nMidterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\nMidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\nSubmit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\nDatasets:\nhttps://www.kaggle.com/datasets and https://www.kaggle.com/competitions\nhttps://archive.ics.uci.edu/ml/index.php\nhttps://data.europa.eu/en\nhttps://www.openml.org/search?type=data\nhttps://newzealand.ai/public-data-sets\nhttps://datasetsearch.research.google.com\nWhat to do and Deliverables\nThink of a problem that's interesting for you and find a dataset for that\nDescribe this problem and explain how a model could be used\nPrepare the data and doing EDA, analyze important features\nTrain multiple models, tune their performance and select the best model\nExport the notebook into a script\nPut your model into a web service and deploy it locally with Docker\nBonus points for deploying the service to the cloud",
        "section": "Projects (Midterm and Capstone)",
        "question": "Crucial Links"
      },
      {
        "text": "Answer: Previous cohorts projects page has instructions (youtube).\nhttps://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\nAlexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\n~~~ Added by Nukta Bhatia ~~~",
        "section": "Projects (Midterm and Capstone)",
        "question": "How to conduct peer reviews for projects?"
      },
      {
        "text": "For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\n14 posts, one for each day, and another 2 posts for each evaluation of other participants projects",
        "section": "Projects (Midterm and Capstone)",
        "question": "Learning in public links for the projects"
      },
      {
        "text": "You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\nRyan Pramana",
        "section": "Projects (Midterm and Capstone)",
        "question": "My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?"
      },
      {
        "text": "If you have submitted two projects (and peer-reviewed at least 3 course-mates\u2019 projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\n(optional) David Odimegwu",
        "section": "Projects (Midterm and Capstone)",
        "question": "What If I submitted only two projects and failed to submit the third?"
      },
      {
        "text": "Yes. You only need to review peers when you submit your project.\nConfirmed on Slack by Alexey Grigorev (added by Rileen Sinha)",
        "section": "Projects (Midterm and Capstone)",
        "question": "I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?"
      },
      {
        "text": "Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you're on the right track.",
        "section": "Projects (Midterm and Capstone)",
        "question": "How many models should I train?"
      },
      {
        "text": "I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\nAnswer:\nThe link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\nTo calculate your hash value run the python code below:\nfrom hashlib import sha1\ndef compute_hash(email):\nreturn sha1(email.lower().encode('utf-8')).hexdigest()\n# Example usage **** enter your email below (Example1@gmail.com)****\nemail = \"Example1@gmail.com\"\nhashed_email = compute_hash(email)\nprint(\"Original Email:\", email)\nprint(\"Hashed Email (SHA-1):\", hashed_email)\nEdit the above code to replace Example1@gmail.com as your email address\nStore and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\nYou then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\nLastly, copy the \u201cHashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675\u201d value and search for 3 identical entries. There you should see your peer project to be reviewed.\nBy Emmanuel Ayeni",
        "section": "Projects (Midterm and Capstone)",
        "question": "How does the project evaluation work for you as a peer reviewer?"
      },
      {
        "text": "Alexey Grigorev: \u201cIt\u2019s based on all the scores to make sure most of you pass.\u201d                                                   By Annaliese Bronz",
        "section": "Projects (Midterm and Capstone)",
        "question": "Do you pass a project based on the average of everyone else\u2019s scores or based on the total score you earn?"
      },
      {
        "text": "No, even though it\u2019s mentioned in the marking rubric, it\u2019s not compulsory, it\u2019s just one of the many possible methods you may use.                                                   By David Peterson\nOther course-related questions that don\u2019t fall into any of the categories above or can apply to more than one category/module",
        "section": "Miscellaneous",
        "question": "Does your mid term project need to use a neural network to get maximum number of points?"
      },
      {
        "text": "Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else\u2019s system. It should also be included in the environment in conda or with pipenv.\nOdimegwu David",
        "section": "Miscellaneous",
        "question": "Why do I need to provide a train.py file when I already have the notebook.ipynb file?"
      },
      {
        "text": "Pip install pillow - install pillow library\nfrom PIL import Image\nimg = Image.open('aeroplane.png')\nFrom numpy import asarray\nnumdata=asarray(img)\nKrishna Anand",
        "section": "Miscellaneous",
        "question": "Loading the Image with PILLOW library and converting to numpy array"
      },
      {
        "text": "Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.",
        "section": "Miscellaneous",
        "question": "Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?"
      },
      {
        "text": "Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\nYou can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\nAlejandro Aponte",
        "section": "Miscellaneous",
        "question": "Is there a way to serve up a form for users to enter data for the model to crunch on?"
      },
      {
        "text": "Using model.feature_importances_ can gives you an error:\nAttributeError: 'Booster' object has no attribute 'feature_importances_'\nAnswer: if you train the model like this: model = xgb.train you should use get_score() instead\nEkaterina Kutovaia",
        "section": "Miscellaneous",
        "question": "How to get feature importance for XGboost model"
      },
      {
        "text": "In the Elastic Container Service task log, error \u201c[Errno 12] Cannot allocate memory\u201d showed up.\nJust increase the RAM and CPU in your task definition.\nHumberto Rodriguez",
        "section": "Miscellaneous",
        "question": "[Errno 12] Cannot allocate memory in AWS Elastic Container Service"
      },
      {
        "text": "When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\nThis does not happen when Flask is used directly, i.e. not through waitress.\nThe problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\nWhen using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\nSolution:\nPut the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\nNote: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\nDetailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\nMarcos MJD",
        "section": "Miscellaneous",
        "question": "Pickle error: can\u2019t get attribute XXX on module __main__"
      },
      {
        "text": "There are different techniques, but the most common used are the next:\nDataset transformation (for example, log transformation)\nClipping high values\nDropping these observations\nAlena Kniazeva",
        "section": "Miscellaneous",
        "question": "How to handle outliers in a dataset?"
      },
      {
        "text": "I was getting the below error message when I was trying to create docker image using bentoml\n[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn'\nSolution description\nThe cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\npackages: # Additional pip packages required by the service\n- xgboost\n- scikit-learn\n- pydantic\nAsia Saeed",
        "section": "Miscellaneous",
        "question": "Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn'"
      },
      {
        "text": "You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with \u201c\u201d (empty string) as output.\nPotential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\n(Memoona Tahira)",
        "section": "Miscellaneous",
        "question": "BentoML not working with \u2013production flag at any stage: e.g. with bentoml serve and while running the bentoml container"
      },
      {
        "text": "Problem description:\nDo we have to run everything?\nYou are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\nNot everyone will be able to run all the files, in particular the neural networks.\nSolution description:\nAlternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\nRelated slack conversation here.\n(Gregory Morris)",
        "section": "Miscellaneous",
        "question": "Reproducibility"
      },
      {
        "text": "If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\nQuinn Avila",
        "section": "Miscellaneous",
        "question": "Model too big"
      },
      {
        "text": "When you try to push the docker image to Google Container Registry and get this message \u201cunauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.\u201d, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\ngcloud auth configure-docker\n(Jesus Acu\u00f1a)",
        "section": "Miscellaneous",
        "question": "Permissions to push docker to Google Container Registry"
      },
      {
        "text": "I am getting this error message when I tried to install tflite in a pipenv environment\nError:  An error occurred while installing tflite_runtime!\nError text:\nERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\nERROR: No matching distribution found for tflite_runtime\nThis version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\nPastor Soto\nCheck all available versions here:\nhttps://google-coral.github.io/py-repo/tflite-runtime/\nIf you don\u2019t find a combination matching your setup, try out the options at\nhttps://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\nwhich you can install as shown in the lecture, e.g.\npip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\nFinally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\nRileen Sinha (based on discussions on Slack)",
        "section": "Miscellaneous",
        "question": "Tflite_runtime unable to install"
      },
      {
        "text": "Error: ImageDataGenerator name 'scipy' is not defined.\nCheck that scipy is installed in your environment.\nRestart jupyter kernel and try again.\nMarcos MJD",
        "section": "Miscellaneous",
        "question": "Error when running ImageDataGenerator.flow_from_dataframe"
      },
      {
        "text": "Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\nhttps://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\nKonrad Muehlberg",
        "section": "Miscellaneous",
        "question": "How to pass BentoML content / docker container to Amazon Lambda"
      },
      {
        "text": "In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\nurl = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\nX = preprocessor.from_url(url)\nI got the error:\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\nSolution:\nAdd ?raw=true after .jpg in url. E.g. as below\nurl = \u2018https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true\u2019\nBhaskar Sarma",
        "section": "Miscellaneous",
        "question": "Error UnidentifiedImageError: cannot identify image file"
      },
      {
        "text": "Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\nSolution: Run: ` pipenv lock` for fix this problem and dependency files\nAlejandro Aponte",
        "section": "Miscellaneous",
        "question": "[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies"
      },
      {
        "text": "Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\nOld: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\nNew: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\nSolution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\nIbai Irastorza",
        "section": "Miscellaneous",
        "question": "Get_feature_names() not found"
      },
      {
        "text": "Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\nThe problem was the format input to the model wasn\u2019t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\nAhmed Okka",
        "section": "Miscellaneous",
        "question": "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)"
      },
      {
        "text": "Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\nI think .5GB RAM is not enough, is there any other free alternative available ?\nA: aws (amazon), gcp (google), saturn.\nBoth aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\nSaturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\n\u201cYou can sign up here: https://bit.ly/saturn-mlzoomcamp\nWhen you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)\u201d\nAdded by Andrii Larkin",
        "section": "Miscellaneous",
        "question": "Free cloud alternatives"
      },
      {
        "text": "Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\nSolution description:\nconvert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\nconvert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\nconvert day and month into a datetime object with:\ndf['date_formatted'] = pd.to_datetime(\ndict(\nyear='2055',\nmonth=df['month'],\nday=df['day']\n)\n)\nget day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\n(Bhaskar Sarma)",
        "section": "Miscellaneous",
        "question": "Getting day of the year from day and month column"
      },
      {
        "text": "How to visualize the predictions per classes after training a neural net\nSolution description\nclasses, predictions = zip(*dict(zip(classes, predictions)).items())\nplt.figure(figsize=(12, 3))\nplt.bar(classes, predictions)\nLuke",
        "section": "Miscellaneous",
        "question": "Chart for classes and predictions"
      },
      {
        "text": "You can convert the prediction output values to a datafarme using \ndf = pd.DataFrame.from_dict(dict, orient='index' , columns=[\"Prediction\"])\nEdidiong Esu",
        "section": "Miscellaneous",
        "question": "Convert dictionary values to Dataframe table"
      },
      {
        "text": "The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that\u2019s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\nIt can be found here: kitchenware-dataset-generator | Kaggle\nMartin Uribe",
        "section": "Miscellaneous",
        "question": "Kitchenware Classification Competition Dataset Generator"
      },
      {
        "text": "Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\nWindows:\nInstall Anaconda prompt https://www.anaconda.com/\nTwo options:\nInstall package \u2018tensorflow-gpu\u2019 in Anaconda\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\nWSL/Linux:\nWSL: Use the Windows Nvida drivers, do not touch that.\nTwo options:\nInstall the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\nMake sure to follow step 4 to install CUDA by environment\nAlso run:\necho \u2018export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\nInstall CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\nInstall https://developer.nvidia.com/rdp/cudnn-download\nNow you should be able to do training/inference with GPU in Tensorflow\n(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\nANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\nezehcp7482@gmail.com:\nPROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to \u2018google\u2019 my way out.\nANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)",
        "section": "Miscellaneous",
        "question": "CUDA toolkit and cuDNN Install for Tensorflow"
      },
      {
        "text": "When multiplying matrices, the order of multiplication is important.\nFor example:\nA (m x n) * B (n x p) = C (m x p)\nB (n x p) * A (m x n) = D (n x n)\nC and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\nBaran Ak\u0131n",
        "section": "Miscellaneous",
        "question": "About getting the wrong result when multiplying matrices"
      },
      {
        "text": "Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\n(added by Rileen Sinha)\nI made a medium article describing it:\nhttps://medium.com/p/eceb6e42e36e\nTill Meineke",
        "section": "Miscellaneous",
        "question": "None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?"
      },
      {
        "text": "Depends on whether the form will still be open. If you're lucky and it's open, you can submit your homework and it will be evaluated. if closed - it's too late.\n(Added by Rileen Sinha, based on answer by Alexey on Slack)",
        "section": "Miscellaneous",
        "question": "I may end up submitting the assignment late. Would it be evaluated?"
      },
      {
        "text": "Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\n(added by Tano Bugelli)",
        "section": "Miscellaneous",
        "question": "Does the github repository need to be public?"
      },
      {
        "text": "You don\u2019t install a conda environment. First you create it, then you activate it.\nStep 1: how to create a conda environment?\nIn a terminal write the command (ml-zoomcamp is the name of the environment):\nconda create -n ml-zoomcamp\nStep 2: how to activate a conda environment?\nconda activate ml-zoomcamp\nYou can verify that it worked if you see (ml-zoomcamp) prepended to your command prompt.\nNote:\nThe answer above assumes Anaconda has already been installed on your local machine. If this is not the case, you can download it from Anaconda\u2019s download page. After installing it, you can verify it succeeded with the following command in a terminal: conda --version.\n(added by Kemal Dahha)",
        "section": "Miscellaneous",
        "question": "How to install Conda environment in my local machine?"
      },
      {
        "text": "VSCode and Jupyter.\n(added by Kemal Dahha)",
        "section": "Miscellaneous",
        "question": "Which IDE is recommended for machine learning?"
      },
      {
        "text": "Install w get:\n!which wget\nDownload data:\n!wget -P /content/drive/My\\ Drive/Downloads/ URL\n(added by Paulina Hernandez)",
        "section": "Miscellaneous",
        "question": "How to use wget with Google Colab?"
      },
      {
        "text": "Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\nUse reshape to reshape a 1D array to a 2D.\n\t\t\t\t\t\t\t(-Aileah) :>\n(added by Tano\nfiltered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n# Select only the desired columns\nselected_columns = [\n'latitude',\n'longitude',\n'housing_median_age',\n'total_rooms',\n'total_bedrooms',\n'population',\n'households',\n'median_income',\n'median_house_value'\n]\nfiltered_df = filtered_df[selected_columns]\n# Display the first few rows of the filtered DataFrame\nprint(filtered_df.head())",
        "section": "Miscellaneous",
        "question": "Features in scikit-learn?"
      },
      {
        "text": "You can probably resolve this by installing the latest version of Pandas. You can do this conveniently from a Jupyter code cell by writing:\n!pip install --upgrade pandas\nAlternatively, if for whatever reason you don\u2019t want to change your Pandas version, you can suppress warnings:\nimport warnings\nimport pandas as pd\n# Suppress FutureWarning messages\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n(added by Kemal Dahha)",
        "section": "Miscellaneous",
        "question": "When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass? FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead"
      },
      {
        "text": "When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\n```\nWarning: Python 3.11 was not found on your system\u2026\nNeither \u2018pipenv\u2019 nor \u2018asdf\u2019 could be found to install Python.\nYou can specify specific versions of Python with:\n$ pipenv \u2013python path\\to\\python\n```\nThe solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\n(Added by Abhijit Chakraborty)",
        "section": "Miscellaneous",
        "question": "Reproducibility in different OS"
      },
      {
        "text": "You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\nSteps:\nRegister in DigitalOcean\nGo to Apps -> Create App.\nYou will need to choose GitHub as a service provider.\nEdit Source Directory (if your project is not in the repo root)\nIMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\nRemember to add model files if they are not built automatically during the container build process.\nBy Dmytro Durach",
        "section": "Miscellaneous",
        "question": "Deploying to Digital Ocean"
      },
      {
        "text": "I\u2019m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\nNot necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\nBy Rileen Sinha",
        "section": "Miscellaneous",
        "question": "Is it best to train your model only on the most important features?"
      },
      {
        "text": "You can consider several different approaches:\nSampling: In the exploratory phase, you can use random samples of the data.\nChunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\nOptimizing data types: Pandas\u2019 automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\nUsing Dask, an open-source python project which parallelizes Numpy and Pandas.\n(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\nBy Rileen Sinha",
        "section": "Miscellaneous",
        "question": "How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?"
      },
      {
        "text": "Technically, yes. Advisable? Not really. Reasons:\nSome homework(s) asks for specific python library versions.\nAnswers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\nAnd as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\nYou can create a separate repo using course\u2019s lessons but written in other languages for your own learnings, but not advisable for submissions.\ntx[source]",
        "section": "Miscellaneous",
        "question": "Can I do the course in other languages, like R or Scala?"
      },
      {
        "text": "Yes, it\u2019s allowed (as per Alexey).\nAdded By Rileen Sinha",
        "section": "Miscellaneous",
        "question": "Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?"
      },
      {
        "text": "The TF and TF Serving versions have to match (as per solution from the slack channel)\nAdded by Chiedu Elue\nFor Module 10.3, if you are on apple silicon:\nif you see this error when trying to run TF-Serving locally with docker:\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\nif found this hint\nDocker release 4.35.0 (172550) for Mac introduces Docker VMM Beta, a replacement for the Apple Virtualisation Framework using Rosetta. Good news is that I can run the native TF Serving image now on.\nTill Meineke (Dec 11 2024)",
        "section": "Miscellaneous",
        "question": "Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?"
      },
      {
        "text": "I\u2019ve seen LinkedIn users list DataTalksClub as Experience with titles as:\nMachine Learning Fellow\nMachine Learning Student\nMachine Learning Participant\nMachine Learning Trainee\nPlease note it is best advised that you do not list the experience as an official \u201cjob\u201d or \u201cinternship\u201d experience since DataTalksClub did not hire you, nor financially compensate you.\nOther ways you can incorporate the experience in the following sections:\nOrganizations\nProjects\nSkills\nFeatured\nOriginal posts\nCertifications\nCourses\nBy Annaliese Bronz\nInteresting question, I put the link of my project into my CV as showcase and make posts to show my progress.\nBy Ani Mkrtumyan",
        "section": "Miscellaneous",
        "question": "Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?"
      },
      {
        "text": "Create a cell and, for example, you can perform the next command using PIP:\n!pip install tensorflow[and-cuda]==2.14\n2.you can use Conda commands, for example:\n!conda install pandas \u2013yes\nThe option --yes is so that the installation can continue when the message \"Proceed ([y]/n)?\" appears\nBy Alexander Daniel Rios",
        "section": "Miscellaneous",
        "question": "How to install extras packages on Google Colab or Kaggle?"
      },
      {
        "text": "The following piece of code which involves shuffling is crucial to getting RMSE which is close to the ones in the answer options in the homework.\ndf = orig_df.copy()\nbase = ['ram','storage','screen','final_price']\nmy_df = df[base]\nidx = np.arange(n)\nnp.random.seed(s)\nnp.random.shuffle(idx)\ndf_shuffled = my_df.iloc[idx]\ndf_train = df_shuffled.iloc[idx[:n_train]].copy()\ndf_val = df_shuffled.iloc[idx[n_train:n_train+n_val]].copy()\ndf_test = df_shuffled.iloc[idx[n_train+n_val:]].copy()\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\ny_train = df_train.final_price.values\ny_val =  df_val.final_price.values\ny_test = df_test.final_price.values\ndel df_train['final_price']\ndel df_val['final_price']\ndel df_test['final_price']\n2.\tif we don't get this logic right, then all the RMSE gets messed up. Do double check in your codes.\nCUDA ran out of memory in google collab\nMost of the time when we are trying to run the models on collab or kaggle , we tend to face the  runtime error of CUDA out of memory.\nTips to overcome this :\nReduce the batch size .\nUse lower precision\nSomething the memory can be allocated for things which we aren\u2019t using it recently , so try to clear the cache\nImport torch \ntorch.cuda.empty_cache()  // free up the GPU memory space.\nDelete unnecessary variables\nFollow the links below to get a deep view of it .\nstackoverflow\nhttps://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2\n(added by Nikisha)",
        "section": "Miscellaneous",
        "question": "Getting Wrong RMSE that is not matching or close to answer options in HW 2(Regression) of 2024 Cohort."
      },
      {
        "text": "You need to add gunicorn and flask just to be safe to Pipfile with nano Pipfile.\n\nadd\n\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n[packages]\nscikit-learn = \"==1.5.2\"\ngunicorn = \"*\"\nflask \t    = \u201c*\u201d\n[dev-packages]\n[requires]\npython_version = \"3.11\"\nAfter that run pipenv lock then do the docker build -t [name] .  and docker run [name]\n(Added by Ico)",
        "section": "Miscellaneous",
        "question": "docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"gunicorn\": executable file not found in $PATH: unknown."
      },
      {
        "text": "Explorer.exe calls the process and . opens in the current directory.\nOr you can sync through VScode to github\n(Added by Ico)",
        "section": "Miscellaneous",
        "question": "If you are working in the terminal on your computer in wsl and you want to go to the directory in explorer to upload to github use type explorer.exe ."
      },
      {
        "text": "Yes, you can provided it does not violate any of the plagiarism rules and it is not one of the common datasets like titanic, iris or Boston housing.set.\nBy Victor Emenike",
        "section": "Miscellaneous",
        "question": "Can I work with this dataset for midterm project https://www.kaggle.com/datasets/kapoorprakhar/cardio-health-risk-assessment-dataset?"
      },
      {
        "text": "The review assignments will be available on CoursePlatform after the first phase of the project is over -i.e. after the submission deadline. After that you will have one week to complete the review.\nSaturn Cloud errors (in Jupyter Notebook):\nImportError: cannot import name 'runtime_version' from 'google.protobuf'\nCan be resolved by updating protobuf:\n!pip install -U protobuf\nWorks for tensorflow 2.17.0 and protobuf 4.25.5  (5.29.0 after updating)\nAdditionally, updating tensorflow to 2.18.0 (!pip install -U tensorflow at the moment) resolves the issue of the model.fit function hanging",
        "section": "Miscellaneous",
        "question": "Where can I find other people\u2019s projects to peer review them and where do I post mine for peer review? (cohort 2024)."
      }
    ]
  },
  {
    "course": "mlops-zoomcamp",
    "documents": [
      {
        "text": "MLOps Zoomcamp FAQ\n\u200b\u200bMLOps Zoomcamp FAQ\nThe purpose of this document is to capture frequently asked technical questions.\nEditing guidelines:\nWhen adding a new FAQ entry, make sure the question is \u201cHeading 2\u201d\nFeel free to improve if you see something is off\nDon\u2019t change the formatting in the document or add any visual \u201cimprovements\u201d\nDon\u2019t change the pages format (it should be \u201cpageless\u201d)\n[Problem description]\n[Solution description]\n(optional) Added by Name",
        "section": "General course questions",
        "question": "Format for questions: [Problem title]"
      },
      {
        "text": "Approximately 3 months.",
        "section": "General course questions",
        "question": "What is the expected duration of this course or that for each module?"
      },
      {
        "text": "Q: When can I expect to receive the confirmation email, after I have registered?\nQ: I forgot if I have registered, can I still join the zoomcamp?\nQ: Can I still take this course after the cohort has started (or after the course launch date)?\nYou don't need to register, registration is not mandatory. It is for gauging the level of interest and collecting data for analytics.\nYou can also just start learning and submitting homework without registering while a cohort is \u201clive\u201d. It is not checked against any registered list. Registration is just to gauge interest before the start date.",
        "section": "General course questions",
        "question": "Course - Registration for the Zoomcamp."
      },
      {
        "text": "The course videos are pre-recorded, you can start watching the course right now.\nThe zoomcamps are spread out throughout the year. See article https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\nYou can see the office hours (playlist with year 20xx) as well as the pre-recorded course videos in (playlist without year) in Course Channel\u2019s Bookmarks and/or DTC\u2019s youtube channel",
        "section": "General course questions",
        "question": "Is it going to be live? When?"
      },
      {
        "text": "Yes, even if you don't register, you're still eligible to submit the homeworks as long as the form is still open and accepting submissions.\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything to the last minute.",
        "section": "General course questions",
        "question": "Course - Can I still join the course after the start date?DataTalksClub \u2b1b - YouTube"
      },
      {
        "text": "No matter if you're with a 'live' cohort or follow on the self-paced mode, the way to start is similar:\nsee what things are where by reading  pins and  bookmarks on the course-channel\nreading the repo (bookmarked in channel) and watching the video lessons (playlist bookmarked in channel)\nif have questions, search the channel itself first; someone may have already asked and gotten a solution\nif don't want to comb through the search results, read  for the most Frequently Asked Questions (this document)\nif don't even want to read/skim/search (use ctrl+F) the questions in FAQ doc, tag the @ZoomcampQABot when you ask questions, and it will summarize it for you (if answers in its knowledge-base)\nFor generic, non-zoomcamp queries, you can also ask ChatGPT/BingCopilot/GoogleGemini/etc, especially for error messages\ncheck if you're on track by checking the deadlines (in Course Management form for Homework submissions)\nmain difference of not being in a \u201clive\u201d cohort is that the responses to your questions might be delayed as not many active students come online anymore. Which is not an issue if you do your own due diligence and search for answers first or reading the documentation of the library.\nIf you do need to ask questions when no answers supplied in resources above have helped, follow the asking-questions.md (bookmarked in channel) guidelines, also in Pins\u2026",
        "section": "General course questions",
        "question": "Course - How do I start?"
      },
      {
        "text": "Yes",
        "section": "General course questions",
        "question": "Course - Can I still graduate when I didn\u2019t complete homework for week x?"
      },
      {
        "text": "No, you can only get a certificate if you finish the course with a \u201clive\u201d cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.",
        "section": "General course questions",
        "question": "Certificate - Can I follow the course in a self-paced mode and get a certificate?"
      },
      {
        "text": "The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded to use Prefect. The rest should mostly be the same.\nAlso all of the homeworks will be changed for the 2023 cohort.",
        "section": "General course questions",
        "question": "What\u2019s the difference between the 2023 and 2022 course?"
      },
      {
        "text": "The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded to use Mage-AI. The rest should mostly be the same.\nAlso all of the homeworks will be changed for the 2024 cohort.",
        "section": "General course questions",
        "question": "Cohort - What\u2019s the difference between the 2024 and 2023 course?"
      },
      {
        "text": "Yes, it will start in May 2024",
        "section": "General course questions",
        "question": "Cohort - Will there be a 2024 Cohort? When will the 2024 cohort start?"
      },
      {
        "text": "Please see the summary of all zoomcamps and their respective schedule at https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html\nAlthough note that, there\u2019s no guarantee that the zoomcamps would be run indefinitely or the same zoomcamps would be conducted year-after-year-after-year.",
        "section": "General course questions",
        "question": "Cohort - I missed the current cohort, when is the next cohort scheduled for? Will there be a 202x cohort?"
      },
      {
        "text": "Please choose the closest one to your answer. Also do not post your answer in the course slack channel.",
        "section": "General course questions",
        "question": "Homework - What if my answer is not exactly the same as the choices presented?"
      },
      {
        "text": "You can find the deadlines for each homework assignment in the course schedule/ timeline provided at https://courses.datatalks.club/mlops-zoomcamp-2024/. The time is your own local time, it has been automatically converted.",
        "section": "General course questions",
        "question": "Homework - where can I find the schedule and/or deadlines of each homework assignment?"
      },
      {
        "text": "The due date differs due to the participants being in different time zones, it was the midnight of May 19th/20th in Berlin, and whatever corresponded to that in your particular time zone. You can find the deadline on the homework submission page, e.g.\nhttps://courses.datatalks.club/mlops-zoomcamp-2024/homework/hw1\nYou can find all cohort-specific info the 2025 cohort here:\nhttps://github.com/DataTalksClub/mlops-zoomcamp/tree/main/cohorts/2025",
        "section": "General course questions",
        "question": "Homework 1 , 2025 Cohort - Is the due date for homework 20th May? How do I check the updated playlist and homework for mlops course?"
      },
      {
        "text": "You might need to use the green taxi data rather than yellow taxi data - look at other recent questions similar to yours. The preprocessing code provided with the homework expects green taxi data (even though the question specifies yellow taxi data), and answers seem to confirm that using green taxi data is the way to go. Not that there's no official confirmation of this though (as of early morning, May 27th in Berlin).",
        "section": "General course questions",
        "question": "Homework 2 , 2025 Cohort - Why is the experiment in question 6 taking so long to run? Should we use yellow taxi data, or green taxi data?"
      },
      {
        "text": "Your port (5000) may be in use by some other process ID, run \u2018lsoft -i :5000\u2019 to find out and either kill the process or explicitly route to a different port (than the default) with \u2018mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\u2019",
        "section": "General course questions",
        "question": "Homework 2 , I am getting conflicts on server ports and cannot establish a connection to the MLflow server, why?"
      },
      {
        "text": "You do not have to use MAGE or any specific orchestrator, it is totally up to you.",
        "section": "General course questions",
        "question": "Homework 3, 2025 Cohort - Do I have to use MAGE as the orchestrator? Can I use any orchestrator I want?"
      },
      {
        "text": "To clarify on \"Late homework submissions\": we cannot submit after the homework is scored as the form is closed.\nOnce the form is closed (aka Scored), no further submissions are possible. So, then no \"late homework submissions\" allowed after the form is closed and not editable. You can check your code against the solution (check the homework.md) file.\nIf the due date has passed, but the form is still Open / Submitted, this is what I term \"late homework submissions\", the form is still editable. Don\u2019t forget to click the Update button with your changes!\nSo, we're on borrowed time and we have no way of knowing when Alexey would close the form (the process is still manual at this time).",
        "section": "General course questions",
        "question": "Q : just found this course, can I still submit homeworks?"
      },
      {
        "text": "Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository. More links documented in datasets.md. Please also read README.md in 07-project folder",
        "section": "General course questions",
        "question": "Project - Are we free to choose our own topics for the final project?"
      },
      {
        "text": "It is an individual project.",
        "section": "General course questions",
        "question": "Project - Is the capstone an individual or team project?"
      },
      {
        "text": "You can get a few cloud points by using kubernetes even if you deploy it only locally. Or you can use localstack too to mimic AWS. Be sure you\u2019re clear on the Evaluation Criteria.",
        "section": "General course questions",
        "question": "Project - For the final project, is it required to be put on the cloud?"
      },
      {
        "text": "After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points you\u2019ve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point. Hover over the ? for some explanations.",
        "section": "General course questions",
        "question": "Homework and Leaderboard - what is the system for points in the course management platform?"
      },
      {
        "text": "They are content that you created about what you have learned on that topic. Some DOs and DON\u2019Ts explained by Alexey in loom video:\nhttps://www.loom.com/share/710e3297487b409d94df0e8da1c984ce\nAnyone caught abusing and gaming the system would be publicly called out and get their points stripped so they don\u2019t appear so high up on the Leaderboard (as of 18 June 2024).",
        "section": "General course questions",
        "question": "What exactly is a learning-in-public post?"
      },
      {
        "text": "When you set up your account you are automatically assigned a random name such as \u201cLucid Elbakyan\u201d for example. Click on the Jump to your record on the leaderboard link to find your entry.\nIf you want to see what your Display name is, click on the Edit Course Profile button.\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname if you want to remain anonymous\nUnless you want \u201cLucid Elbakyan\u201d on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver\u2019s license, etc. This is the name that is going to appear on your Certificate!",
        "section": "General course questions",
        "question": "Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?"
      },
      {
        "text": "It might be a good idea not to wait till last minute and submit even partially complete project\nAnd keep adding improvements till last date (If needed)\nYou can simply update git commit sha.",
        "section": "Module 1: Introduction",
        "question": "You can submit and update your project attempt (1 or 2) multiple times before the final deadline."
      },
      {
        "text": "You can install the Jupyter extension to open notebooks in VSCode.\nAdded by Khubaib",
        "section": "Module 1: Introduction",
        "question": "Opening Jupyter in VSCode"
      },
      {
        "text": "When you are ready and installed Anaconda, in a new terminal, you can run a jupyter notebook with the command line \u201cjupyter notebook\u201d. Be careful not to make any typo, for example\u201cjupyter-notebook\u201d will throw an error \u201cJupyter command `jupyter-notebook` not found.\u201d.\nAdded by M\u00e9lanie Fouesnard",
        "section": "Module 1: Introduction",
        "question": "Launching Jupyter notebook from codespace VM"
      },
      {
        "text": "In case one would like to set a github repository (e.g. for Homeworks), one can follow 2 great tutorials that helped a lot\nSetting up github on AWS instance - this\nSetting up keys on AWS instance - this\nThen, one should be able to push to its repo\nAWS selected instance\nIt is not covered under free subscription due to large in size or other reason..Received below reply from them.(I am referring here the Linux setup video in intro)\nFor free version you can check the free tier link. EC2 only has the below in free :\nResizable compute capacity in the Cloud.\n750 hours per month of Linux, RHEL, or SLES t2.micro or t3.micro* instance dependent on region\n750 hours per month of Windows t2.micro or t3.micro* instance dependent on region\n750 hours per month of public IPv4 address regardless of instance type\n*launches in Unlimited mode (may incur additional charges)\nAdded by Giri (glk08909@gmail.com)\nAdded by Daniel Hen (daniel8hen@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Configuring Github to work from the remote VM"
      },
      {
        "text": "Faced issue while setting up JUPYTER NOTEBOOK on AWS. I was unable to access it from my desktop. (I am not using visual studio and hence faced problem)\nRun\njupyter notebook --generate-config\nEdit file /home/ubuntu/.jupyter/jupyter_notebook_config.py to add following line:\nNotebookApp.ip = '*'\nAdded by Atul Gupta (samatul@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Opening Jupyter in AWS"
      },
      {
        "text": "If you wish to use WSL on your windows machine, here are the setup instructions:\nCommand: Sudo apt install wget\nGet Anaconda download address here. wget <download address>\nTurn on Docker Desktop WFree Download | AnacondaSL2\nCommand: git clone <github repository address>\nVSCODE on WSL\nJupyter: pip3 install jupyter\nAdded by Gregory Morris (gwm1980@gmail.com)\nAll in all softwares at one shop:\nYou can use anaconda which has all built in services like pycharm, jupyter\nAdded by Khaja Zaffer (khajazaffer@aln.iseg.ulisboa.pt)\nAlternatively, you can download miniforge, which is a more lightweight open-source version of conda, which doesn\u2019t rely on the proprietary Anaconda repository and allows you to use mamba, as a default package manager, which greatly improves environment solving speed.\nFor a clear, step-by-step guide to installing miniforge, the Texas Tech University High Performance Computing Center has an excellent comprehensive guide:\nInstalling Miniforge3 Guide by TTU HPCC\nAdded by Jon Areas (areasjx@gmail.com)\nFor windows \u201cwsl --install\u201d in Powershell\nAdded by Vadim Surin (vdmsurin@gmai.com)\nIf python is still showing as 3.10 after installing anaconda with Python 3.9, try running \u2018source .bashrc\u2019 from ${HOME} folder, for any reason if its still not working, add \u2018export PATH=\u201d<anaconda install path>/bin:$PATH\u201d\u2019",
        "section": "Module 1: Introduction",
        "question": "WSL instructions"
      },
      {
        "text": "If you created a repo without .gitignore, follow this steps to add .gitignore:\nOpen Terminal.\nNavigate to the location of your Git repository.\nCreate a .gitignore file for your repository.\ntouch .gitignore\nLocate the .gitignore file: If you already have a .gitignore file, open it.\nEdit the .gitignore File: Add the following lines to the .gitignore file:\n# Python\n*.pyc\n__pycache__/\n*.py[cod]\n*$\nSave the Changes: Save the .gitignore file.\nCommit the Changes\nAdded by Chuks Okoli (chuks.o.okoli@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Created repo without .gitignore"
      },
      {
        "text": "If you create a folder data and download datasets or raw files in your local repository. Then to push all your code to remote repository without this files or folder please use gitignore file. The simple way to create it do the following steps\n1. Create empty .txt file (using text editor or command line)\n2. Safe as .gitignore (. must use the dot symbol)\n3. Add rules\n *.parquet - to ignore all parquet files\ndata/ - to ignore all files in folder data\n\nFor more pattern read GIT documentation\nhttps://git-scm.com/docs/gitignore\nAdded by Olga Rudakova (olgakurgan@gmail.com)",
        "section": "Module 1: Introduction",
        "question": ".gitignore how-to"
      },
      {
        "text": "Make sure when you stop an EC2 instance that it actually stops (there's a meme about it somewhere). There are green circles (running), orange (stopping), and red (stopped). Always refresh the page to make sure you see the red circle and status of stopped.\nEven when an EC2 instance is stopped, there WILL be other charges that are incurred (e.g. if you uploaded data to the EC2 instance, this data has to be stored somewhere, usually an EBS volume and this storage incurs a cost).\nYou can set up billing alerts. (I've never done this, so no advice on how to do this).\n(Question by: Akshit Miglani (akshit.miglani09@gmail.com) and Answer by Anna Vasylytsya)",
        "section": "Module 1: Introduction",
        "question": "AWS suggestions"
      },
      {
        "text": "You can get invitation code by coursera and use it in account to verify it it has different characteristics.\n\u0130BM CLOUD - Coursera Free Feature Code 395 Days",
        "section": "Module 1: Introduction",
        "question": "IBM Cloud an alternative for AWS"
      },
      {
        "text": "I am worried about the cost of keeping an AWS instance running during the course.\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finished your work for the day.  Using that strategy, in a day with about 5 hours of work you will pay around $0.40 USD which will account for $12 USD per month, which seems to be an affordable amount.\nYou must remember that you would have a different IP public address every time you Restart your instance, and you would need to edit your ssh Config file.  It's worth the time though.\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded.\nHere is a tutorial to set this up.\nAlso, you can estimate the cost yourself, using AWS pricing calculator (to use it you don\u2019t even need to be logged in).\nAt the time of writing (20.05.2023) t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week that should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD\nHere\u2019s a link to the estimate\nAdded by Alex Litvinov (aaalex.lit@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "AWS costs"
      },
      {
        "text": "For many parts - yes. Some things like kinesis are not in AWS free tier, but you can do it locally with localstack.",
        "section": "Module 1: Introduction",
        "question": "Is the AWS free tier enough for doing this course?"
      },
      {
        "text": "When I click an open IP-address in an AWS EC2 instance I get an error: \u201cThis site can\u2019t be reached\u201d. What should I do?\nThis ip-address is not required to be open in a browser. It is needed to connect to the running EC2 instance via terminal from your local machine or via terminal from a remote server with such command, for example if:\nip-address is 11.111.11.111\ndownloaded key name is razer.pem (the key should be moved to a hidden folder .ssh)\nyour user name is user_name\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111",
        "section": "Module 1: Introduction",
        "question": "AWS EC2: this site can\u2019t be reached"
      },
      {
        "text": "After this command `ssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX` I got this error: \"unprotected private key file\". This page (https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) explains how to fix this error. Basically you need to change the file permissions of the key file with this command: chmod 400 ~/.ssh/razer.pem",
        "section": "Module 1: Introduction",
        "question": "Unprotected private key file!"
      },
      {
        "text": "My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS code.\nMy config:\n# Copy Configuration in local nano editor, then Save it!\nHost mlops-zoomcamp                                         # ssh connection calling name\nUser ubuntu                                             # username AWS EC2\nHostName <instance-public-IPv4-addr>                    # Public IP, it changes when Source EC2 is turned off.\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem   # Private SSH key file path\nLocalForward 8888 localhost:8888                        # Connecting to a service on an internal network from the outside, static forward or set port user forward via on vscode\nStrictHostKeyChecking no\nAdded by Muhammed \u00c7elik\nThe disconnection will occur whether I SSH via WSL2 or via VS Code, and usually occurs after I run some code, i.e. \u201cimport mlflow\u201d, so not particularly intense computation.\nI cannot reconnect to the instance without stopping and restarting with a new IPv4 address.\nI\u2019ve gone through steps listed on this page: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\nInbound rule should allow all incoming IPs for SSH.\nWhat I expect to happen:\nSSH connection should remain while I\u2019m actively using the instance, and if it does disconnect, I should be able to reconnect back.\nSolution: sometimes the hang ups are caused by the instance running out of memory. In one instance, using EC2 feature to view screenshot of the instance as a means to troubleshoot, it was the OS out-of-memory feature which killed off some critical processes. In this case, if we can\u2019t use a higher compute VM with more RAM, try adding a swap file, which uses the disk as RAM substitute and prevents the OOM error. Follow Ubuntu\u2019s documentation here: https://help.ubuntu.com/community/SwapFaq.\nAlternatively follow AWS\u2019s own doc, which mirrors Ubuntu\u2019s: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/\nAdded by Claudia van Dijk: In addition, if your connection happens to be dropping because of timeouts, you can add this line to your local .ssh/config file, which makes it ping the connection every 50 seconds in case timeout is set to 60 seconds:\nServerAliveInterval 50",
        "section": "Module 1: Introduction",
        "question": "AWS EC2 instance constantly drops SSH connection"
      },
      {
        "text": "Everytime I restart my EC2 instance I keep getting different IP and need to update the config file manually.\n\nSolution: You can create a script like this to automatically update the IP address of your EC2 instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md",
        "section": "Module 1: Introduction",
        "question": "AWS EC2 IP Update"
      },
      {
        "text": "Make sure to use an instance with enough compute capabilities such as a t2.xlarge. You can check the monitoring tab in the EC2 dashboard to monitor your instance.",
        "section": "Module 1: Introduction",
        "question": "VS Code crashes when connecting to Jupyter"
      },
      {
        "text": "If you switched off the VM instance completely in GCP then when it switches back on the IP address changes. You need to update the ssh_config file with the new external IP address. This can be done in VS Code if you have the Remote-SSH extension installed. Open the command palette and type `Remote-SSH Open SSH Configuration File\u2026` then select the appropriate ssh_config file. And edit the HostName to the correct IP address.",
        "section": "Module 1: Introduction",
        "question": "My connection to my GCP VM instance keeps timing out when I try to connect"
      },
      {
        "text": "Error \u201cValueError: X has 526 features, but LinearRegression is expecting 525 features as input.\u201d when running your Linear Regression Model on the validation data set:\nSolution: The DictVectorizer creates an initial mapping for the features (columns). When calling the DictVecorizer again for the validation dataset transform should be used as it will ignore features that it did not see when fit_transform was last called. E.g.\nX_train = dv.fit_transform(train_dict)\nX_test = dv.transform(test_dict)",
        "section": "Module 1: Introduction",
        "question": "X has 526 features, but expecting 525 features"
      },
      {
        "text": "If some dependencies are missing\nInstall following packages\npandas\nmatplotlib\nscikit-learn\nfastparquet\npyarrow\nseaborn\npip install -r requirements.txt\nI have seen this error when using pandas.read_parquet(), the solution is to install pyarrow or fastparquet by doing !pip install pyarrow in the notebook\nNOTE: if you\u2019re using Conda instead of pip, install fastparquet rather than pyarrow, as it is much easier to install and it\u2019s functionally identical to pyarrow for our needs.",
        "section": "Module 1: Introduction",
        "question": "Missing dependencies"
      },
      {
        "text": "The mean_squared_error function in scikit-learn no longer includes the squared parameter. To compute the Root Mean Squared Error (RMSE), use the dedicated function root_mean_squared_error from sklearn.metrics instead.",
        "section": "Module 1: Introduction",
        "question": "squared Option Not Available in mean_squared_error"
      },
      {
        "text": "The evaluation RMSE I get doesn\u2019t figure within the options!\nIf you\u2019re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (0\u2264duration\u226460) and you\u2019ll get a RMSE which is (approximately) in the options. Also don\u2019t forget to convert the columns data types to str before using the DictVectorizer.\nAnother option: Along with filtering outliers, additionally filter on null values by replacing them with -1.  You will get a RMSE which is (almost same as) in the options. Use \u2018.round(2)\u2019 method to round it to 2 decimal points.\nWarning deprecation\nThe python interpreter warning of modules that have been deprecated  and will be removed in future releases as well as making suggestion how to go about your code.\nFor example\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619:\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\nwarnings.warn(msg, FutureWarning)\nTo suppress the warnings, you can include this code at the beginning of your notebook\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
        "section": "Module 1: Introduction",
        "question": "No RMSE value in the options"
      },
      {
        "text": "sns.distplot(df_train[\"duration\"])\nCan be replaced with\nsns.histplot(\ndf_train[\"duration\"] , kde=True,\nstat=\"density\", kde_kws=dict(cut=3), bins=50,\nalpha=.4, edgecolor=(1, 1, 1, 0.4),\n)\nTo get almost identical result",
        "section": "Module 1: Introduction",
        "question": "How to replace distplot with histplot"
      },
      {
        "text": "You need to replace the capital letter \u201cL\u201d with a small one \u201cl\u201d",
        "section": "Module 1: Introduction",
        "question": "KeyError: 'PULocationID'  or  'DOLocationID'"
      },
      {
        "text": "Run the following command:\n!pip install pyarrow\nAfter successfully downloading, you can delete the command.\n-AnnalieseTech",
        "section": "Module 1: Introduction",
        "question": "ImportError: Unable to find a usable engine; tried using: \u2018pyarrow\u2019, \u2018fastparquet\u2019."
      },
      {
        "text": "I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\nThe error message is:\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\nI solved it by performing the homework directly as a python script.\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\nYou can try using the Pyspark library\nAnswered by kamaldeen (kamaldeen32@gmail.com)\nParquet format can be read in chunks: link. (IK)",
        "section": "Module 1: Introduction",
        "question": "Reading large parquet files"
      },
      {
        "text": "If the jupyter notebook kernel gets killed repeatedly due to out of memory issues when converting pandas DF to dict or other memory intensive steps, try google colab as it offers larger memory.\nFor this,\nUpload the datasets to google drive [Folder Colab Notebooks]\nMount the drive on colab\nfrom google.colab import drive\ndrive.mount('/content/drive')\nPull the data from uploaded tables in colab\ndf_jan = pq.read_table('/content/drive/My Drive/Colab Notebooks/yellow_tripdata_2023-01.parquet').to_pandas()\nAll set for doing the assignment\nDownload the final assignment to your local and copy into the relevant repo",
        "section": "Module 1: Introduction",
        "question": "Kernel getting killed during assignment tasks on local"
      },
      {
        "text": "Two main encoding approaches are generally used to handle categorical data: label encoding and one-hot encoding. The first assigns each categorical value an integer value based on alphabetical order, while the second creates new variables (using 0s and 1s) depicting original categorical data. Simply, one may use label encoding with logical categorical data such as a rating system or a classification, and one-hot encoding is rather applicable to cases where there is no reasoning with the data. Sci-kit Learn dictionary vectorizer is an encoding class that will provide a means to handle categorical data and generate a corresponding array based on the unique number of instances encountered within your columns choice from a DataFrame (or else). The key point is to assign values to categories and thus enable fitting ML models. In case you want to apply one-hot encoding, sometimes you\u2019ll have to reset the dataset into objects, so any possible logic is deceived. Otherwise, you may fall into label encoding, which can be limiting for some applications. Besides the dictionary vectorizer, Sci-kit Learn also offers the OneHotEncoding() class. Pandas has a similar feature, named pd.get_dummies().\nAdded by Jonathan Lima (jtlimads@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "What is the difference between label and one-hot encoding?"
      },
      {
        "text": "First remove the outliers (trips with unusual duration) before plotting\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Distplot takes too long"
      },
      {
        "text": "Problem: RMSE on test set was too high when hot encoding the validation set with a previously fitted OneHotEncoder(handle_unknown=\u2019ignore\u2019) on the training set, while DictVectorizer would yield the correct RMSE.\nIn principle both transformers should behave identically when treating categorical features (at least in this week\u2019s homework where we don\u2019t have sequences of strings in each row):\nFeatures are put into binary columns encoding their presence (1) or absence (0)\nUnknown categories are imputed as zeroes in the hot-encoded matrix",
        "section": "Module 1: Introduction",
        "question": "RMSE on test set too high"
      },
      {
        "text": "In summary,\npd.get_dummies or OHE can come up with result in different orders and handle missing data differently, so train and val set would have different columns during train and validation\nDictVectorizer would ignore missing (in train) and new (in val) datasets\nOther sources:\nhttps://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\nhttps://scikit-learn.org/stable/modules/feature_extraction.html\nhttps://innovation.alteryx.com/encode-smarter/\n~ ellacharmed",
        "section": "Module 1: Introduction",
        "question": "ictVectorizerA: Alexey\u2019s answer https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s"
      },
      {
        "text": "Why didn't get_dummies in pandas library or OneHotEncoder in scikit-learn library be used for one-hot encoding? I know OneHotEncoder is the most common and useful. One-hot coding can also be done using the eye or identity components of the NumPy library.\nM.Sari\nOneHotEncoder has the option to output a row column tuple matrix. DictVectorizer is a one step method to encode and support row column tuple matrix output.\nHarinder(sudwalh@gmail.com)\nWe used DictVectorizer because it provides a simple one-step way to handle both categorical and numerical features from dictionaries, and directly outputs a sparse matrix\u2014making it ideal for ML pipelines without extra preprocessing.\nYann Pham-Van\nUse OneHotEncoder when you want full control, need to work with sklearn pipelines, or must handle unknown categories safely. Use DictVectorizer when your data is in dictionary format (e.g., JSON or from APIs) and you want to plug it into a pipeline quickly.\nGabi Fonseca",
        "section": "Module 1: Introduction",
        "question": "Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?"
      },
      {
        "text": "How to check that we removed the outliers?\nUse the pandas function describe() which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using boolean expression, the min and max can be verified using\ndf[\u2018duration\u2019].describe()",
        "section": "Module 1: Introduction",
        "question": "Clipping outliers"
      },
      {
        "text": "pd.get_dummies and DictVectorizer both create a one-hot encoding on string values. Therefore you need to convert the values in PUlocationID and DOlocationID to string.\nIf you convert the values in PUlocationID and DOlocationID from numeric to string, the NaN values get converted to the string \"nan\".  With DictVectorizer the RMSE is the same whether you use \"nan\" or \"-1\" as string representation for the NaN values. Therefore the representation doesn't have to be \"-1\" specifically, it could also be some other string.",
        "section": "Module 1: Introduction",
        "question": "Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding"
      },
      {
        "text": "Problem: My LinearRegression RMSE is very close to the answer but not exactly the same. Is this normal?\nAnswer: No, LinearRegression is an deterministic model, it should always output the same results when given the same inputs.\nAnswer:\nCheck if you have treated the outlier properly for both train and validation sets\nCheck if the one hot encoding has been done properly by looking at the shape of one hot encoded feature matrix. If it shows 2 features, there is something wrong with one hot encoding. Hint: the drop off and pick up codes need to be converted to proper data format and then DictVectorizer is fitted.\nHarshit Lamba (hlamba19@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Slightly different RMSE"
      },
      {
        "text": "Problem: I\u2019m facing an extremely low RMSE score (eg: 4.3451e-6) - what shall I do?\nAnswer: Recheck your code to see if your model is learning the target prior to making the prediction. If the target variable is passed in as a parameter while fitting the model, chances are the model would score extremely low. However, that\u2019s not what you would want and would much like to have your model predict that. A good way to check that is to make sure your X_train doesn\u2019t contain any part of your y_train. The same stands for validation too.\nSnehangsu De (desnehangsu@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Extremely low RSME"
      },
      {
        "text": "Problem: how to enable auto completion in jupyter notebook? Tab doesn\u2019t work for me\nSolution: !pip install --upgrade jedi==0.17.2\nChristopher R.J.(romanjaimesc@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Enabling Auto-completion in jupyter notebook"
      },
      {
        "text": "Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\nSolution: The links point to files on cloudfront.net, something like this:\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\nI\u2019m not download the dataset directly, i use dataset URL and run this in the file.\nUpdate(27-May-2023): Vikram\nI am able to download the data from the below link. This is from the official  NYC trip record page (TLC Trip Record Data). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\n(Asif)",
        "section": "Module 1: Introduction",
        "question": "Downloading the data from the NY Taxis datasets gives error : 403 Forbidden"
      },
      {
        "text": "Problem: PyCharm (remote) doesn\u2019t see conda execution path. So, I cannot use conda env (which is located on a remote server).\nSolution: In remote server in command line write \u201cconda activate envname\u201d, after write \u201cwhich python\u201d - it gives you python execution path. After you can use this path when you will add new interpreter in PyCharm: add local interpreter -> system interpreter -> and put the path with python.\nSalimov Ilnaz (salimovilnaz777@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Using PyCharm & Conda env in remote development"
      },
      {
        "text": "Problem: The output of DictVectorizer was taking up too much memory. So much so, that I couldn\u2019t even fit the linear regression model before running out of memory on my 16 GB machine.\nSolution: In the example for DictVectorizer in the scikit-learn website, they set the parameter \u201csparse\u201d as False. Although this helps with viewing the results, this results in a lot of memory usage. The solution is to either use \u201csparse=True\u201d instead, or leave it at the default which is also True.\nAhmed Fahim (afahim03@yahoo.com)",
        "section": "Module 1: Introduction",
        "question": "Running out of memory"
      },
      {
        "text": "Problem: For me, Installing anaconda didn\u2019t modify the .bashrc profile. That means Anaconda env was not activated even after exiting and relaunching the unix shell.\nSolution:\nFor bash : Initiate conda again, which will add entries for anaconda in .bashrc file.\n$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\nThat will automatically edit your .bashrc.\nReload:\n$ source ~/.bashrc\nAhamed Irshad (daisyfuentesahamed@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Activating Anaconda env in .bashrc"
      },
      {
        "text": "While working through the HW1, you will realize that the training and the validation data set feature sizes are different. I was trying to figure out why and went down the entire rabbit hole only to see that I wasn\u2019t doing ```transform``` on the premade dictionary vectorizer instead of ```fit_transform```. You already have the dictionary vectorizer made so no need to execute the fit pipeline on the model.\nSam Lim(changhyeonlim@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "The feature size is different for training set and validation set"
      },
      {
        "text": "I found a good guide how to get acces to your machine again when you removed your public key.\nUsing the following link you can go to Session Manager and log in to your instance and create public key again. https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\nThe main problem for me here was to get my old public key, so for doing this you should run the following command: ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\nFor more information: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\nHanna Zhukavets (a.zhukovec1901@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Permission denied (publickey) Error (when you remove your public key on the AWS machine)"
      },
      {
        "text": "Problem: The February dataset has been used as a validation/test dataset and been stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\nAnswer: The sparsematrix result from DictVectorizer shouldn\u2019t be turned into an ndarray. After removing that part of the code, I ended up receiving a correct result .\nTahina Mahatoky (tahinadanny@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Overfitting: Absurdly high RMSE on the validation dataset"
      },
      {
        "text": "more specific error line:\nfrom sklearn.feature_extraction import DictVectorizer\nI had this issue and to solve it I did\n!pip install scikit-learn\nJoel Auccapuclla (auccapuclla 2013@gmail.com)",
        "section": "Module 1: Introduction",
        "question": "Can\u2019t import sklearn"
      },
      {
        "text": "If you don\u2019t want to install docker desktop and run docker in WSL2 on Windows you can try the following:\nInstall docker and docker compose and give the user the right privileges (you do not need\n# Install Docker, you can ignore the warnings\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n# Add your user to the Docker group\nsudo usermod -aG docker $USER\nThen you need to start the service:\nsudo systemctl enable docker.service\nThen you can test both are installed:\n# Sanity check that both tools were installed successfully\ndocker --version\ndocker compose version\ndocker run hello-world\nif after restarting WSL the service is not started automatically, you will need to change your .profile o .zprofile file and include something like this:\nif grep -q \"microsoft\" /proc/version > /dev/null 2>&1; then\nif service docker status 2>&1 | grep -q \"is not running\"; then\nwsl.exe --distribution \"${WSL_DISTRO_NAME}\" --user root \\\n--exec /usr/sbin/service docker start > /dev/null 2>&1\nfi\nfi\nAdded by Eduardo Munoz",
        "section": "Module 1: Introduction",
        "question": "Install docker in WSL2 without installing Docker Desktop"
      },
      {
        "text": "Seeing <2855951x515 sparse matrix of type '<class 'numpy.float64'>'\nwith 0 stored elements in Compressed Sparse Row format>? It could be that your (soon to be vectorized) variables imported as floating point rather than integer. This will result in nonsensical models. Convert with (for dg being your dataframe, and categorical storing names of your variables to be vectorized):\ndg[categorical] = dg[categorical].round(0).astype(int).astype(str)",
        "section": "Module 1: Introduction",
        "question": "Zero elements in sparse matrix (AKA when dictionary vectorizer / categorical X transformation fails )"
      },
      {
        "text": "If you don\u2019t want to install anaconda on your machine and don\u2019t want to use codespace or a VPS, you could create a docker image and run it locally.\nFor this, can use the following Dockerfile:\nFROM docker.io/bitnami/minideb:bookworm\nRUN install_packages wget ca-certificates vim less silversearcher-ag\n# Uncomment the `COPY` and comment the `RUN` line if you have downloaded anaconda manually\n# I did this to save bandwith when experimenting with the image creation\nRUN wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh && bash Anaconda3-2022.05-Linux-x86_64.sh -b -p /opt/anaconda3\n#COPY  Anaconda3-2022.05-Linux-x86_64.sh /tmp/Anaconda3-2022.05-Linux-x86_64.sh\nRUN   bash /tmp/Anaconda3-2022.05-Linux-x86_64.sh -b -p /opt/anaconda3 && \\\nrm /tmp/Anaconda3-2022.05-Linux-x86_64.sh\nENV PATH=\"/opt/anaconda3/bin:$PATH\" \\\nHOME=\"/app\"\nEXPOSE 8888\nWORKDIR /app\nUSER 1001\nENTRYPOINT [ \"jupyter\", \"notebook\", \"--ip\", \"0.0.0.0\" ]\nBuild the image using:\ndocker build -f Dockerfile -t mlops:v0 .\nThen you could run it with:\nmkdir app\nchmod -R 777 app\ndocker run --name jupyter -p 8888:8888 -v ./app:/app mlops:v0\nIn the logs you could see the jupyter URL that you need to use to enter the jupyter environment. The files you create in the environment will be written under app directory.",
        "section": "Module 1: Introduction",
        "question": "Using a docker image as development environment (Linux)"
      },
      {
        "text": "There is an option to run the project without anaconda and not much pain with maintaining multiple pythons on your machine. The new package manager uv is a speedy and powerful one written in Rust. It\u2019s good to use in your python projects overall. Install guide\nuv venv --python 3.9.7 # install python 3.9.7 that is used in the course\nsource .venv/bin/activate # activate the environment\npython -V # should be 3.9.7\nuv pip install pandas scikit-learn notebook seaborn pyarrow # install required packages\njupyter notebook # run jupyter notebook\nAnd cleanup has never been easy. Deactivate the environment and delete the folder\ndeactivate\nrm -rf .venv\nAdded by Masha Loianych",
        "section": "Module 1: Introduction",
        "question": "Use uv as a package manager"
      },
      {
        "text": "A: While calculating the RMSE, I initially used mean_squared_error(..., squared=False), but it failed with a TypeError. It turns out that the squared parameter was only added in scikit-learn 0.22, and in earlier versions it's not recognized. For older versions, RMSE can be computed manually using np.sqrt(mean_squared_error(...)). Alternatively, from version 1.0 onward, there's a dedicated function: root_mean_squared_error(...), which is more explicit and convenient.\nfrom sklearn.metrics import root_mean_squared_error as rmse\nrmse = root_mean_squared_error(y_train, y_pred)\nprint('RMSE:', rmse)\nAdded by Jos\u00e9 Luis Mart\u00ednez (Maxkaizo)",
        "section": "Module 1: Introduction",
        "question": "Q: I get TypeError: got an unexpected keyword argument 'squared' when using mean_squared_error(..., squared=False). Why?"
      },
      {
        "text": "seaborn.boxplot is generally faster because it uses a smaller set of summary statistics (min, Q1, median, Q3, max) to represent the data, which requires less computational effort, especially for large datasets.\nseaborn.histplot can be slower, particularly with large datasets, because it needs to bin the data and compute frequency counts for each bin, which involves more processing.\nSo, if speed is a concern, especially with large datasets, boxplots are typically faster than histograms.\nAdded by Alexander Daniel Rios",
        "section": "Module 1: Introduction",
        "question": "Visualizing outliers in large datasets with Seaborn: Boxplot vs Histplot"
      },
      {
        "text": "Error\nA module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.4 as it may crash.\nOR\nAttributeError: module 'pyarrow' has no attribute '__version__'\nSolution: Down grade the version of your numpy\npip uninstall numpy -y\nconda remove numpy --force\nconda clean --all -y\nconda install numpy=1.26 -y\nAdded by Uchechukwu Fortune Njoku",
        "section": "Module 1: Introduction",
        "question": "Reading parquet files with Pandas (pyarrow dependency)"
      },
      {
        "text": "While training the model in Jupyter Notebook ono Github Codespaces . You may get an error where the jupyter kernel dies. Simply upgrade the machine type in Codespaces from 8 cores to 14 cores. It is free to upgrade but be aware that you use up more hours .\nAdded by : Abiodun Gbadamosi",
        "section": "Module 2: Experiment tracking",
        "question": "Kernel died during Model Training  on Github Codespaces"
      },
      {
        "text": "Sure, the path may be an URL :\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html\nYann Pham-Van",
        "section": "Module 2: Experiment tracking",
        "question": "Do we absolutely need to save data to disk ?\nCan we use it directly from download ?"
      },
      {
        "text": "Problem: Localhost:5000 Unavailable // Access to Localhost Denied // You don\u2019t have authorization to view this page (127.0.0.1:5000)\n\nSolution: If you are on an chrome browser you need to head to `chrome://net-internals/#sockets` and press \u201cFlush Socket Pools\u201d",
        "section": "Module 2: Experiment tracking",
        "question": "Access Denied at Localhost:5000 - Authorization Issue"
      },
      {
        "text": "You have something running on the 5000 port. You need to stop it.\nAnswer: On terminal in mac .\nRun ps -A | grep gunicorn\nLook for the number process id which is the 1st number after running the command\nkill 13580\nwhere 13580  represents the process number.\nSource\nwarrie.warrieus@gmail.com\nOr by executing the following command it will kill all the processes using port 5000:\n>> sudo fuser -k 5000/tcp\nAnswered by Vaibhav Khandelwal\nJust execute in the command below in he command line to kill the running port\n->> kill -9 $(ps -A | grep python | awk '{print $1}')\nAnswered by kamaldeen (kamaldeen32@gmail.com)\nChange to different port (5001 in this case)\n>> mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\nAnswered by krishna (nellaikrishna@gmail.com)",
        "section": "Module 2: Experiment tracking",
        "question": "Connection in use: ('127.0.0.1', 5000)"
      },
      {
        "text": "Running python register_model.py results in the following error:\nValueError: could not convert string to float: '0 int\\n1   float\\n2     hyperopt_param\\n3       Literal{n_estimators}\\n4       quniform\\n5         Literal{10}\\n6         Literal{50}\\n7         Literal{1}'\nFull Traceback:\nTraceback (most recent call last):\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\nrun(args.data_path, args.top_n)\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\ntrain_and_log_model(data_path=data_path, params=run.data.params)\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/xfsub/scripts/register_model.py\", line 41, in train_and_log_model\nparams = space_eval(SPACE, params)\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\nrval = pyll.rec_eval(space, memo=memo)\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\nrval = scope._impls[node.name](*args, **kwargs)\nValueError: could not convert string to float: '0 int\\n1   float\\n2     hyperopt_param\\n3       Literal{n_estimators}\\n4       quniform\\n5         Literal{10}\\n6         Literal{50}\\n7         Literal{1}'\nSolution: There are two plausible errors to this. Both are in the hpo.py file where the hyper-parameter tuning is run. The objective function should look like this.\n\n   def objective(params):\n# It's important to set the \"with\" statement and the \"log_params\" function here\n# in order to properly log all the runs and parameters.\nwith mlflow.start_run():\n# Log the parameters\nmlflow.log_params(params)\nrf = RandomForestRegressor(**params)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_valid)\n# Calculate and log rmse\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\nmlflow.log_metric('rmse', rmse)\nIf you add the with statement before this function, and just after the following line\nX_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\nand you log the parameters just after the search_space dictionary is defined, like this\nsearch_space = {....}\n# Log the parameters\nmlflow.log_params(search_space)\nThen there is a risk that the parameters will be logged in group. As a result, the\nparams = space_eval(SPACE, params)\nregister_model.py file will receive the parameters in group, while in fact it expects to receive them one by one. Thus, make sure that the objective function looks as above.\nAdded by Jakob Salomonsson",
        "section": "Module 2: Experiment tracking",
        "question": "Could not convert string to float - ValueError"
      },
      {
        "text": "Make sure you launch the mlflow UI from the same directory as the code that is running the experiments (same directory that contains the mlruns directory and the database that stores the experiments).\nOr navigate to the correct directory when specifying the tracking_uri.\nFor example:\nIf the mlflow.db is in a subdirectory called database, the tracking uri would be \u2018sqllite:///database/mlflow.db\u2019\nIf the mlflow.db is a directory above your current directory: the tracking uri would be\n\u2018sqlite:///../mlflow.db\u2019\nAnswered by Anna Vasylytsya\nAnother alternative is to use an absolute path to mlflow.db rather than relative path\nAnd yet another alternative is to launch the UI from the same notebook by executing the following code cell\nimport subprocess\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\nAnd then using the same MLFLOW_TRACKING_URI when initializing mlflow or the client\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)",
        "section": "Module 2: Experiment tracking",
        "question": "Experiment not visible in MLflow UI"
      },
      {
        "text": "I encountered the following issue: I was able to run experiments and the different model parameters were visible. However, the metrics, including the \u201chandmade\u201d metric rmse in the training script, were not visible (empty field).\nI solved my problem by making sure to specify the \u201ckey\u201d and \u201cvalue\u201d explicitly when using mlflow.log_metric:\nmlflow.log_metric(key=\"rmse\",value=rmse)\nAdded by M\u00e9lanie Fouesnard",
        "section": "Module 2: Experiment tracking",
        "question": "Metrics not visible in mlflow UI"
      },
      {
        "text": "Following the instructions as per the video as below did not work though the jupyter notebook says it is successfully created.\nSet the URI to the listener directly. It worked for me. This could be because the video was made with a lower version of \u201cmlflow\u201d package and we are working on the latest version. The documentation of the latest  \u201cmlflow\u201d package is asking to set as below\nmlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n(optional) Arun Gansi",
        "section": "Module 2: Experiment tracking",
        "question": "Unable to create new Experiment"
      },
      {
        "text": "Problem:\nGetting\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE\nduring MLFlow's installation process, particularly while installing the Numpy package using pip\nWhen I installed mlflow using \u2018pip install mlflow\u2019 on 27th May 2022, I got the following error while numpy was getting installed through mlflow:\n\nCollecting numpy\nDownloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              \t| 6.3 MB 107 kB/s eta 0:01:19\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\nnumpy from https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 (from mlflow):\nExpected sha256 3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\nGot    \t15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\nSolution:\nThen when I install numpy separately (and not as part of mlflow), numpy gets installed (same version), and then when I do 'pip install mlflow', it also goes through.\nPlease note that the above may not be consistently simulatable, but please be aware of this issue that could occur during pip install of mlflow.\nAdded by Venkat Ramakrishnan",
        "section": "Module 2: Experiment tracking",
        "question": "Hash Mismatch Error with Package Installation"
      },
      {
        "text": "After deleting an experiment from UI, the deleted experiment still persists in the database.\nSolution: To delete this experiment permanently, follow these steps.\nAssuming you are using sqlite database;\nInstall ipython sql using the following command: pip install ipython-sql\nIn your jupyter notebook, load the SQL magic scripts with this: %load_ext sql\nLoad the database with this: %sql sqlite:///nameofdatabase.db\nRun the following SQL script to delete the experiment permanently: check link",
        "section": "Module 2: Experiment tracking",
        "question": "How to Delete an Experiment Permanently from MLFlow UI"
      },
      {
        "text": "Problem: I cloned the public repo, made edits, committed and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\nThis is what my config looks like (in case this might be useful):\n[core]\nrepositoryformatversion = 0\nfilemode = true\nbare = false\nlogallrefupdates = true\nignorecase = true\nprecomposeunicode = true\n[remote \"origin\"]\nurl = git@github.com:my_username/mlops-zoomcamp.git\nfetch = +refs/heads/*:refs/remotes/origin/*\n[branch \"main\"]\nremote = origin\nmerge = refs/heads/main\nSolution: You should fork DataClubsTak\u2019s repo instead of cloning it. On GitHub, click \u201cFetch and Merge\u201d under the menu \u201cFetch upstream\u201d at the main page of your own",
        "section": "Module 2: Experiment tracking",
        "question": "How to Update Git Public Repo Without Overwriting Changes"
      },
      {
        "text": "This is caused by ```mlflow.xgboost.autolog()``` when version 1.6.1 of xgboost\nDowngrade to 1.6.0\n```pip install xgboost==1.6.0``` or update requirements file with xgboost==1.6.0 instead of xgboost\nAdded by Nakul Bajaj",
        "section": "Module 2: Experiment tracking",
        "question": "Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction."
      },
      {
        "text": "Since the version 1.29 the list_experiments method was deprecated and then removed in the later version\nYou should use # Register the best model model_uri = f\"runs:/{best_run.info.run_id}/model\" mlflow.register_model(model_uri=model_uri, name=\"RandomForestBestModel\") instead\nAdded by Alex Litvinov",
        "section": "Module 2: Experiment tracking",
        "question": "MlflowClient object has no attribute 'list_experiments'"
      },
      {
        "text": "Make sure `mlflow.autolog()` ( or framework-specific autolog ) written BEFORE `with mlflow.start_run()` not after.\nAlso make sure that all dependencies for the autologger are installed, including matplotlib. A warning about uninstalled dependencies will be raised.\nMohammed Ayoub Chettouh",
        "section": "Module 2: Experiment tracking",
        "question": "MLflow Autolog not working"
      },
      {
        "text": "If you\u2019re running MLflow on a remote VM, you need to forward the port too like we did in Module 1 for Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT like in the screenshot:\nAdded by Sharon Ibejih\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page navigate to localhost:5000 instead.",
        "section": "Module 2: Experiment tracking",
        "question": "MLflow URL (http://127.0.0.1:5000), doesn\u2019t open."
      },
      {
        "text": "Got the same warning message as Warrie Warrie when using \u201cmlflow.xgboost.autolog()\u201d\nIt turned out that this was just a warning message and upon checking MLflow UI (making sure that no \u201ctag\u201d filters were included), the model was actually automatically tracked in the MLflow.\nAdded by Bengsoon Chuah, Asked by Warrie Warrie, Answered by Anna Vasylytsya & Ivan Starovit",
        "section": "Module 2: Experiment tracking",
        "question": "MLflow.xgboost Autolog Model Signature Failure"
      },
      {
        "text": "raise MlflowException(\nmlflow.exceptions.MlflowException: Cannot set a deleted experiment 'random-forest-hyperopt' as the active experiment. You can restore the experiment, or permanently delete the experiment to create a new one.\nThere are many options to solve in this link: https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow\n\u2705Had deleted the experiment from the mlflow ui, and this command in CLI works mlflow gc --backend-store-uri sqlite:///backend.db (use the filename.db that you had used, obviously)\n\u26d4 Below suggestion didn\u2019t work, as .trash/ was already empty\nrm -rf mlruns/.trash/*\nzsh: sure you want to delete all the files in /home/ellacharmed/github/mlops-zoomcamp/cohorts/2024/02-experiment-tracking/homework/mlruns/.trash [yn]? y\nzsh: no matches found: mlruns/.trash/*",
        "section": "Module 2: Experiment tracking",
        "question": "MlflowException: Unable to Set a Deleted Experiment"
      },
      {
        "text": "If you\u2019re using a  postgres  backend locally or remotely and you don\u2019t want to delete the entire backend, you can run this script to permanently delete an experiment. I had a separate env.py file to retrieve my environment variables from.\n```\nimport os\nimport sys\nimport psycopg2\nsys.path.insert(0, os.getcwd())\nfrom env import DB_NAME, DB_PASSWORD, DB_PORT, DB_USER\ndef perm_delete_exp():\nconnection = psycopg2.connect(database=DB_NAME,\nuser=DB_USER,\npassword=DB_PASSWORD,\nhost=\"localhost\",\nport=int(DB_PORT))\nwith connection.cursor() as cursor:\nqueries = \"\"\"\nDELETE FROM experiment_tags WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted');\nDELETE FROM latest_metrics WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\nDELETE FROM metrics WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\nDELETE FROM tags WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\nDELETE FROM params WHERE run_uuid=ANY(SELECT run_uuid FROM runs where experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\nDELETE FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted');\nDELETE FROM datasets WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted');\nDELETE FROM experiments where lifecycle_stage='deleted';\n\"\"\"\nfor query in queries.splitlines()[1:-1]:\ncursor.execute(query.strip())\nconnection.commit()\nconnection.close()\nif __name__ == \"__main__\":\nperm_delete_exp()\n```\nAdded by Joses Omojola",
        "section": "Module 2: Experiment tracking",
        "question": "MlflowException: Unable to Set a Deleted Experiment with Postgres backend"
      },
      {
        "text": "You do not have enough disk space to install the requirements. You can either increase the base EBS volume by following this link or add an external disk to your instance and configure conda installation to happen on the external disk.\nAbinaya Mahendiran\nOn GCP: I added another disk to my vm and followed this guide to mount the disk. Confirm the mount by running df -H (disk free) command in bash shell. I also deleted Anaconda and instead used miniconda. I downloaded miniconda in the additional disk that I mounted and when installing miniconda, enter the path to the extra disk instead of the default disk, this way conda is installed on the extra disk.\nYang Cao",
        "section": "Module 2: Experiment tracking",
        "question": "No Space Left on Device - OSError[Errno 28]"
      },
      {
        "text": "I was using an old version of sklearn due to which I got the wrong number of parameters because in the latest version min_impurity_split for randomforrestRegressor was deprecated. Had to upgrade to the latest version to get the correct number of params.",
        "section": "Module 2: Experiment tracking",
        "question": "Parameters Mismatch in Homework Q3"
      },
      {
        "text": "Error: I installed all the libraries from the requirements.txt document in a new environment as follows:\npip install -r requirementes.txt\nThen when I run mlflow from my terminal like this:\nmlflow\nI get this error:\nSOLUTION: You need to downgrade the version of 'protobuf' module to 3.20.x or lower. Initially, it was version=4.21, I installed protobuf==3.20\npip install protobuf==3.20\nAfter which I was able to run mlflow from my terminal.\n-Submitted by Aashnna Soni",
        "section": "Module 2: Experiment tracking",
        "question": "Protobuf error when installing MLflow"
      },
      {
        "text": "If the ssh connection from your local machine\u2019s WSL to AWS EC2 instance is frequently getting terminated with very short span of inactivity with the following message displayed at prompt:\nYou can fix the same by adding the following lines to your config file at your .ssh directory in your WSL environment:\nServerAliveInterval 60\nServerAliveCountMax 3\nFor eg (for clarity). after adding these lines your ssh connection should look somewhat like this below:\nHost mlops-zoomcamp\nHostName 45.80.32.7\nUser ubuntu\nIdentityFile ~/.ssh/siddMLOps.pem\nStrictHostKeyChecking no\nServerAliveInterval 60\nServerAliveCountMax 3\nAdded by Siddhartha Gogoi",
        "section": "Module 2: Experiment tracking",
        "question": "SSH Connection to AWS EC2 instance from local machine WSL getting terminated frequently within a minute of inactivity."
      },
      {
        "text": "Please check your current directory while running the mlflow ui command. You need to run mlflow ui or mlflow server command in the right directory.",
        "section": "Module 2: Experiment tracking",
        "question": "Setting up Artifacts folders"
      },
      {
        "text": "If you have problem with setting up MLflow for experiment tracking on GCP, you can check these two links:\nhttps://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\nhttps://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html",
        "section": "Module 2: Experiment tracking",
        "question": "Setting up MLflow experiment tracker on GCP"
      },
      {
        "text": "Solution: Downgrade setuptools (I downgraded 62.3.2 -> 49.1.0)",
        "section": "Module 2: Experiment tracking",
        "question": "Setuptools Replacing Distutils - MLflow Autolog Warning"
      },
      {
        "text": "I can\u2019t sort runs in MLFlow\nMake sure you are in table view (not list view) in the MLflow UI.\nAdded and Answered by Anna Vasylytsya",
        "section": "Module 2: Experiment tracking",
        "question": "Sorting runs in MLflow UI"
      },
      {
        "text": "Problem: When I ran `$ mlflow ui` on a remote server and try to open it in my local browser I got an exception  and the page with mlflow ui wasn\u2019t loaded.\nSolution: You should `pip uninstall flask` on your remote server on conda env and after it install Flask `pip install Flask`. It is because the base conda env has ~flask<1.2, and when you clone it to your new work env, you are stuck with this old version.\nAdded by Salimov Ilnaz",
        "section": "Module 2: Experiment tracking",
        "question": "TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch"
      },
      {
        "text": "Problem: After successfully installing mlflow using pip install mlflow on my Windows system, I am trying to run the mlflow ui command but it throws the following error:\nFileNotFoundError: [WinError 2] The system cannot find the file specified\nSolution: Add C:\\Users\\{User_Name}\\AppData\\Roaming\\Python\\Python39\\Scripts to the PATH\nAdded by Alex Litvinov",
        "section": "Module 2: Experiment tracking",
        "question": "mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified"
      },
      {
        "text": "Running \u201cpython hpo.py --data_path=./your-path --max_evals=50\u201d for the homework leads to the following error: TypeError: unsupported operand type(s) for -: 'str' and 'int'\nFull Traceback:\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\nrun(args.data_path, args.max_evals)\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\nfmin(\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\nreturn trials.fmin(\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\nreturn fmin(\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\nrval.exhaust()\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\nself.run(self.max_evals - n_done, block_until_done=self.asynchronous)\nTypeError: unsupported operand type(s) for -: 'str' and 'int'\nSolution:\nThe --max_evals argument in hpo.py has no defined datatype and will therefore implicitly be treated as string. It should be an integer, so that the script can work correctly. Add type=int to the argument definition:\nparser.add_argument(\n\"--max_evals\",\ntype=int,\ndefault=50,\nhelp=\"the number of parameter evaluations for the optimizer to explore.\"\n)\nexport",
        "section": "Module 2: Experiment tracking",
        "question": "Unsupported Operand Type Error in hpo.py"
      },
      {
        "text": "Getting the following warning when running mlflow.sklearn:\n\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow. [\u2026]\nSolution: use 0.24.1 <= scikit-learn <= 1.4.2 (Updated: May 26, 2024)\nReference: https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html",
        "section": "Module 2: Experiment tracking",
        "question": "Unsupported Scikit-Learn version"
      },
      {
        "text": "Problem: CLI commands (mlflow experiments list) do not return experiments\nSolution description: need to set environment variable for the Tracking URI:\n$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\nAdded and Answered by Dino Vitale",
        "section": "Module 2: Experiment tracking",
        "question": "Mlflow CLI does not return experiments"
      },
      {
        "text": "Problem: After starting the tracking server, when we try to use the mlflow cli commands as listed here, most of them can\u2019t seem to find the experiments that have been run with the tracking server\nSolution: We need to set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. This is something like \u201cexport MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}\u201d . After this, we can view the experiments from the command line using commands like \u201cmlflow experiments search\u201d\nEven after this commands like \u201cmlflow gc\u201d doesn\u2019t seem to get the tracking uri, and they have to be passed explicitly as an argument every time the command is run.\nAhmed Fahim (afahim03@yahoo.com)",
        "section": "Module 2: Experiment tracking",
        "question": "Viewing MLflow Experiments using MLflow CLI"
      },
      {
        "text": "All the experiment and other tracking information in mlflow are stored in sqllite database provided while initiating the mlflow ui command. This database can be inspected using Pycharm\u2019s Database tab by using the SQLLite database type. Once the connection is created as below, the tables can be queried and inspected using regular SQL. The same applies for any SQL backed database such as postgres as well.\nThis is very useful to understand the entity structure of the data being stored within mlflow and useful for any kind of systematic archiving of model tracking for longer periods.\nAdded by Senthilkumar Gopal",
        "section": "Module 2: Experiment tracking",
        "question": "Viewing SQLlite Data Raw & Deleting Experiments Manually"
      },
      {
        "text": "Solution : It is another way to start it for remote hosting a mlflow server. For example, if you are multiple colleagues working together on something you most likely would not run mlflow on one laptop but rather everyone would connect to the same server running mlflow\nAnswer by Christoffer Added by Akshit Miglani (akshit.miglani09@gmail.com)",
        "section": "Module 2: Experiment tracking",
        "question": "What does launching the tracking server locally mean?"
      },
      {
        "text": "Problem: parameter was not recognized during the model registry\nSolution: parameters should be added in previous to the model registry. The parameters can be added by mlflow.log_params(params) so that the dictionary can be directly appended to the data.run.params.\nAdded and Answered by Sam Lim",
        "section": "Module 2: Experiment tracking",
        "question": "Parameter adding in case of max_depth not recognized"
      },
      {
        "text": "Problem: Max_depth is not recognize even when I add the mlflow.log_params\nSolution: the mlflow.log_params(params) should be added to the hpo.py script, but if you run it it will append the new model to the previous run that doesn\u2019t contain the parameters, you should either remove the previous experiment or change it\nPastor Soto",
        "section": "Module 2: Experiment tracking",
        "question": "Max_depth is not recognize even when I add the mlflow.log_params"
      },
      {
        "text": "Problem: About week_2 homework: The register_model.py  script, when I copy it into a jupyter notebook fails and spits out the following error. AttributeError: 'tuple' object has no attribute 'tb_frame'\nSolution: remove click decorators",
        "section": "Module 2: Experiment tracking",
        "question": "AttributeError: 'tuple' object has no attribute 'tb_frame'"
      },
      {
        "text": "Problem: when running the preprocess_data.py file you get the following error:\n\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\nSolution: Go to your WandB profile (top RHS) \u2192 user settings \u2192 scroll down to \u201cDanger Zone\u201d and copy your API key. \n\nThen before running preprocess_data.py, add and run the following cell in your notebook:\n\n%%bash\n\nWandb login <YOUR_API_KEY_HERE>.\nAdded and Answered by James Gammerman (jgammerman@gmail.com)",
        "section": "Module 2: Experiment tracking",
        "question": "WandB API error"
      },
      {
        "text": "Please make sure you following the order below nd enabling the autologging before constructing the dataset. If you still have this issue check that your data is in format compatible with XGBoost.\n# Enable MLflow autologging for XGBoost\nmlflow.xgboost.autolog()\n# Construct your dataset\nX_train, y_train = ...\n# Train your XGBoost model\nmodel = xgb.XGBRegressor(...)\nmodel.fit(X_train, y_train)\nAdded by Olga Rudakova",
        "section": "Module 2: Experiment tracking",
        "question": "WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset."
      },
      {
        "text": "Starting from version 2.1.0, XGBoost distributes its Python package in two variants:\nmanylinux_2_28: For recent Linux distributions with glibc 2.28 or newer. This variant includes all features, such as GPU algorithms and federated learning/\nmanylinux2014: For older Linux distributions with glibc versions older than 2.28. This variant lacks support for GPU algorithms and federated learning.\nIf you're installing XGBoost via pip, the package manager automatically selects the appropriate variant based on your system's glibc version. Starting May 31, 2025, the manylinux2014 variant will no longer be distributed.\nThis means that systems with glibc versions older than 2.28 will not be able to install future versions of XGBoost via pip unless they upgrade their glibc version or build XGBoost from source. This behavior is disabled for conda.\nAdded by Jon Areas (areasjx@gmail.com)",
        "section": "Module 2: Experiment tracking",
        "question": "Old version of glibc when running XGBoost"
      },
      {
        "text": "Problem\nUsing wget command to download either data or python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a python virtual env, it did not recognize the pip command.\nSolution: Use python -m pip, this same for any other command. Ie. python -m wget\nAdded by Erick Calderin",
        "section": "Module 2: Experiment tracking",
        "question": "wget not working"
      },
      {
        "text": "Problem: Open/run github notebook(.ipynb) directly in Google Colab\nSolution: Change the domain from 'github.com' to 'githubtocolab.com'. The notebook will open in Google Colab.\nOnly works with Public repo.\nAdded by Ming Jun\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\nSolution: Go to the official doc.\nAdded by Erick Calderin",
        "section": "Module 2: Experiment tracking",
        "question": "Open/run github notebook(.ipynb) directly in Google Colab"
      },
      {
        "text": "Problem: Someone asked why we are using this type of split approach instead of just a random split.\nSolution: For example, I have some models at work that train on Jan 1 2020 \u2014 Aug 1 2021 time period, and then test on Aug 1 - Dec 31 2021, and finally validate on Jan - March or something\nWe do these \u201cout of time\u201d  validations to do a few things:\nCheck for seasonality of our data\nWe know if the RMSE for Test is 5 say, and then RMSE for validation is 20, then there\u2019s serious seasonality to the data we are looking at, and now we might change to Time Series approaches\nIf I\u2019m predicting on Mar 30 2023 the outcomes for the next 3 months, the \u201crandom sample\u201d in our train/test would have caused data leakage, overfitting, and poor model performance in production. We mustn\u2019t take information about the future and apply it to the present when we are predicting in a model context.\nThese are two of, I think, the biggest points for why we are doing jan/feb/march. I wouldn\u2019t do it any other way.\nTrain: Jan\nTest: Feb\nValidate: March\nThe point of validation is to report out model metrics to leadership, regulators, auditors, and record the models performance to then later analyze target drift\nAdded by Sam LaFell",
        "section": "Module 2: Experiment tracking",
        "question": "Why do we use Jan/Feb/March for Train/Test/Validation Purposes?"
      },
      {
        "text": "Problem: When using MLflow\u2019s autolog function, I get this warning: \"WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'numpy.ndarray' object has no attribute 'toarray'\". Why is this happening?\nSolution:\nYou're getting this warning because autolog is attempting to log your dataset. Mlflow expects the dataset to be in a pd.DataFrame format, but if you\u2019re following the course\u2019s code, we\u2019re providing a numpy.ndarray. So, when Mlflow tries to do the execute the toarray method, it fails because the numpy.ndarray is already an array.\nSince we're not doing anything (yet) with the datasets in this zoomcamp, I just went ahead and put log_datasets = False as a parameter in the autolog function.\nAdded by Fustincho\nProblem: If you get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database:\nReproducible Command:\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\nError:\n\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'OpenSSL 1.0.2k-fips  26 Jan 2017'. See: https://github.com/urllib3/urllib3/issues/2168\nSolution: Upgrade mlflow using\nCode: pip3 install --upgrade mlflow\nResolution: It downgrades urllib3 2.0.3 to 1.26.16 which is compatible with mlflow and ssl 1.0.2\nInstalling collected packages: urllib3\nAttempting uninstall: urllib3\nFound existing installation: urllib3 2.0.3\nUninstalling urllib3-2.0.3:\nSuccessfully uninstalled urllib3-2.0.3\nSuccessfully installed urllib3-1.26.16\nAdded by Sarvesh Thakur",
        "section": "Module 2: Experiment tracking",
        "question": "WARNING: mlflow.sklearn: Failed to log training dataset information to MLflow Tracking."
      },
      {
        "text": "If encountering an error while running s3 buckets make sure to resolve dependencies issue by downgrading urllib3 to a compatible version: pip3 install \"urllib3<1.27\"\nAdded by Maximilien Eyengue",
        "section": "Module 2: Experiment tracking",
        "question": "ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+"
      },
      {
        "text": "Problem: In scenario 2 notebook, an error AttributeError: 'MlflowClient' object has no attribute 'list_run_infos',\nIs thrown when one runs \u201crun_id = client.list_run_infos(experiment_id='1')[0].run_id\u201d\nSolution: Use \u201crun_id = client.search_runs(experiment_ids='1')[0].info.run_id\u201d instead.\nScenario: For reference, this works for mflow version 2.12.2, but might work for other recent versions as of May, 2024\nAdded by Oluwadara Adedeji",
        "section": "Module 2: Experiment tracking",
        "question": "AttributeError: 'MlflowClient' object has no attribute 'list_run_infos'"
      },
      {
        "text": "No, in the official documentation it\u2019s mentioned that autologging keep track of the parameters even when you do not explicitly set them when calling .fit.\nYou can run the training, only setting the parameters you want, but you can check all the parameters in mlflow UI.\nAdded by Eduardo Munoz",
        "section": "Module 2: Experiment tracking",
        "question": "When using Autologging, do I need to set a training parameter to track it on Mlflow UI?"
      },
      {
        "text": "Description\nWhen setting up your venv with\n$conda install --file requirements.txt\nYou may encounter the following error\n\u200b\u200b\n```\nPackagesNotFoundError: The following packages are not available from current channels:\n- hyperopt\n```\nSolution\nIt is probably because your conda is out of date. You can update Conda with\n$conda update -n base -c defaults conda\nIf that doesn\u2019t work you can always install it via, which is the advice from the conda page\n$conda install intel::hyperopt\nAdded by Marcus Leiwe",
        "section": "Module 2: Experiment tracking",
        "question": "Hyperopt is not installable with Conda"
      },
      {
        "text": "I fix this error by running `brew install libomp`",
        "section": "Module 2: Experiment tracking",
        "question": "Error importing xgboost in python with OS mac: library not loaded: @rpath/libomp.dylib"
      },
      {
        "text": "Basically add the mlruns and artifacts to the .gitignore, like this:\n02-experiment-tracking/mlruns\n02-experiment-tracking/runnin-mflow-examples/mlruns\n02-experiment-tracking/homework/mlruns\n02-experiment-tracking/homework/artifacts\nAdded by Ibai Irastorza",
        "section": "Module 2: Experiment tracking",
        "question": "Size limit when uploading to github"
      },
      {
        "text": "Older versions of MLflow used client.list_experiments(), but in recent versions this method was replaced.\nUse client.search_experiments() instead.\nAdded by Jos\u00e9 Luis Mart\u00ednez",
        "section": "Module 3: Orchestration",
        "question": "Why does MlflowClient no longer support list_experiments()?"
      },
      {
        "text": "Instructor (Tommy) uses MacOS (I presume), so shortcut key is CMD+period\nOn Windows, the equivalent is CTRL+WIN+period",
        "section": "Module 3: Orchestration",
        "question": "Mage shortcut key to open Text Editor is not working on Windows"
      },
      {
        "text": "Try to export the pipeline as zip file, create a new Mage project and import the pipeline zip to new project\nStart by thoroughly checking the logs of the upstream block that was supposed to generate object.joblib. Ensure it completed successfully and that its expected output (often named output_1) was actually created and saved. You might also want to quickly verify in the Mage UI or file system (if accessible) whether the file exists in the .variables directory for that upstream block.",
        "section": "Module 3: Orchestration",
        "question": "Mage pipeline breaks with [Errno 2] No such file or directory: '/home/src/mage_data/{\u2026} /.variables/{...}/output_1/object.joblib'\""
      },
      {
        "text": "When running ./scripts/start.sh - it returns below error\nERROR: The Compose file './docker-compose.yml' is invalid because:\nUnsupported config option for networks: 'app-network'\nUnsupported config option for services: 'magic-platform'\nSolution description\n# Download the latest version of Docker Compose\nsudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n# Apply executable permissions to the binary\nsudo chmod +x /usr/local/bin/docker-compose\n(optional) Artur G",
        "section": "Module 3: Orchestration",
        "question": "Update docker-compose to initiate Mage"
      },
      {
        "text": "Issue (1) you get errors like:\n[+] Running 1/1\n\u2718 magic-database Error too many requests: You have reached your pull rate limit. You may increase the limit by authenticating and upgra...                       1.2s\nError response from daemon: too many requests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit\nIssue (2) you get these popups with different % values but all saying space is in single digits.\nSolution: It is not recommended to setup Mage as a subfolder of mlops-zoomcamp. See findings in this thread.",
        "section": "Module 3: Orchestration",
        "question": "Mage in Codespaces in a subfolder under mlops-zoomcamp repository"
      },
      {
        "text": "The below errors seem to occur only when using mage in Codespaces.\nErrors (1)\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\nErrors (2)\nError response from daemon: invalid volume specification: '/workspaces/mage-mlops:/:rw': invalid mount config for type \"bind\": invalid specification: destination can't be '/'\nSolution for (1) & (2): stay tuned\u2026still testing\ndocker info and docker \u2013version runs fine. \nBut when I do docker compose down, stop codespaces, and reconnect, the errors went away. Not sure if it is reproducible for everyone, though.\nErrors (3)\nwarning: unable to access '/home/codespace/.gitconfig': Is a directory\nSolution (3) via Office Hours:\nthis is targeted for 3.5.x Deploying with Mage\nIf not deploying,\nComment line#20 in docker-compose.yml\nplace a dummy empty file named .gitconfig in your repo\u2019s root folder and copy it in the Dockerfile with this line, place it below line#9\nCOPY .gitconfig /root/.gitconfig\nThe reason this happens is that when the file is missing, Docker auto-creates it as a Directory instead of a file. So creating a dummy file prevents this",
        "section": "Module 3: Orchestration",
        "question": "Mage in Codespaces"
      },
      {
        "text": "When you see the mage version change in the UI after you\u2019ve started the container, and you want to update, follow these steps. Read the release notes first to see if there\u2019s a fix that affected your work and would benefit from an update.\nIf you want to remain in the previous version is also fine; unless the fixes were specifically for our zoomcamp course-work (check slack and/or the repository for any new instructions or PRs added).\nClose the browser page\nIn the terminal console, bring down the container `docker compose down`\nRebuild the container with new mage image `docker compose build --no-cache`\nVerify that you see `[magic-platform 1/4] FROM docker.io/mageai/mageai:alpha` meaning that the container is being rebuild with a new version\nIf the image is not updated, ctrl+c to cancel the process and pull the image manually with `docker pull mageai/mageai:alpha` then rebuild\nThen restart the docker container with `./scripts/start.sh` as before\nps: this is the same sequence of steps if you want to switch to the latest tagged image instead of using the alpha image.\nWhat does alpha and latest mean?\nLatest is the fully released version ready for production use, and it has gone through verification, testing, QA and whatever else the release cycle entails.\nAlpha is the potentially buggy version with fresh new fixes and newly added features; but not yet put through the full beta test (if there\u2019s one), integration testing and other QA steps. So expect issues to occur.",
        "section": "Module 3: Orchestration",
        "question": "Mage updated in UI"
      },
      {
        "text": "import requests\nfrom io import BytesIO\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nif 'data_loader' not in globals():\nfrom mage_ai.data_preparation.decorators import data_loader\n@data_loader\ndef ingest_files(**kwargs) -> pd.DataFrame:\ndfs: List[pd.DataFrame] = []\nfor year, months in [(2024, (1, 3))]:\nfor i in range(*months):\nresponse = requests.get(\n'https://github.com/mage-ai/datasets/raw/master/taxi/green'f'/{year}/{i:02d}.parquet'\n)\nif response.status_code != 200:\nraise Exception(response.text)\ndf = pd.read_parquet(BytesIO(response.content))\n# if time series chart on mage error, add code below\ndf['lpep_pickup_datetime_cleaned'] = df['lpep_pickup_datetime'].astype(np.int64) // 10**9\ndfs.append(df)\nreturn pd.concat(dfs)\nAdded by Rohmat S",
        "section": "Module 3: Orchestration",
        "question": "Mage Time Series Bar Chart Not Showing"
      },
      {
        "text": "I encountered this issue while trying to run the data_export block that saves the dict vectorizer and the logs of the linear regression model into mlflow. My two distinct outputs, while they were clearly created by the previous transformer block where the linear regression model is trained and the dict vectorizer fitted to the training dataset.\nThus, I had this error while trying to run my export code:\nException: Block mlflow_model_registry may be missing upstream dependencies. It expected to have 2 arguments, but only received 1. Confirm that the @data_exporter method declaration has the correct number of arguments.\nThe outputs are stored in a list and this list is the input with the two outputs as the two elements. I had to modify my code in the data_exporter function to take only one argument and to define the two variables after that:\nDv = output[0]\nLr = output[1]\nAdded by M\u00e9lanie Fouesnard",
        "section": "Module 3: Orchestration",
        "question": "Mage data_exporter block not taking all outputs from previous transformer block"
      },
      {
        "text": "Error: Cannot cast DatetimeArray to dtype float64\nHave the runs completed successfully? We need to have successfully running Pipelines in order to populate the mage and mlflow databases.\nIf all pipelines are successfully completed and still getting this error, then please provide this info.",
        "section": "Module 3: Orchestration",
        "question": "Mage Dashboard on unit_3 is not showing charts"
      },
      {
        "text": "There\u2019s no need to add the utilities functions in each sub-project when you watch the videos as there only need to be one set. Just verify the code is still the same as in Mage\u2019s mlops repository.\nAs from the import statements\nfrom mlops.utils.[...] import [...]\nall refer to the same path in the main mlops \u201cparent\u201d project:\n/[mage-mlops-repository-name]/mlops/utils/...",
        "section": "Module 3: Orchestration",
        "question": "Creating Helper functions in Mage"
      },
      {
        "text": "https://docs.mage.ai/platform/projects/management\nhttps://docs.mage.ai/design/abstractions/project-structure\nhttps://docs.mage.ai/orchestration/global-data-products/overview\nRunning the GDP block takes forever.\nException: Pipeline run xx for global data product training_set: failed\nAttributeError: 'NoneType' object has no attribute 'to_dict'\nWe need to replicate the pipelines and codes into each sub-project as the Settings indicate that only one project can be active at any one time, which means the sub-projects do not communicate with each other. (Needs confirmation during office hours.)\nThings you can try:\nMake sure the following lines in the GDP block are for the actual project and pipeline you\u2019re running\n\"project\": \"unit_2_training\",\n\"repo_path\": \"/home/src/mlops/unit_2_training\",\nInterrupt and Restart Kernel from the Run menu\nBring docker down and restarting it via the script\nIf both of the above does not resolve, recreate everything from scratch:\nRemove the connections from the hyperparameter_tuning/sklearn block in the Tree panel to its upstream blocks. Click on the connector \u2192 Remove Connection\nRemove the Global Data Product block from the Tree panel, right click \u2192 Delete Block (ignore dependencies)\nClick on All blocks and select the Global Data Products, drag+drop this block to be the first in the pipeline\nRename the block to what is used in the video\nRun the block to test it (Play button or Ctrl+Enter)\nIf it helps, do the same for the file in path \u201cunit_3_observability\u201d (ella\u2019s full disclosure: unit_2 works after I removed all things GDP and recreate, now I cannot replicate the same success for unit_3. Still trying\u2026let\u2019s discuss here)\nError with creating Global Data Product on Mage: AttributeError: 'NoneType' object has no attribute 'to_dict'\nSolution: Global product is currently not cross product. You will have to create the data preparation pipeline in unit_2_training and configure to build.\nAdded by Oluwadara Adedeji",
        "section": "Module 3: Orchestration",
        "question": "Video 3.2.1 - Various issues with Global Data Products"
      },
      {
        "text": "There is no way to remove this through the UI, you need to manually edit the global_data_products.yaml  which is stored in your project\u2019s utils function. You can do this through the Text Editor.\nAdded by Marcus Leiwe",
        "section": "Module 3: Orchestration",
        "question": "How do you remove a global data product"
      },
      {
        "text": "If you had remove and re-add blocks, especially from the above issue with Global Data Products, remove the connections from the hyperparameter_tuning/sklearn block in the Tree panel to its upstream blocks and re-add them. Don\u2019t forget to [Ctrl+S] to save Pipeline.\nVideo 3.2.8 Error with Xgboost pipeline: ValueError: not enough values to unpack (expected 3, got 1)\nSolution: Ensure that you have created the variables as in the video and you have this order in your code.\ndata \u2192 training_set\ndata_2 \u2192 hyperparameter_tuning/xgboost\nIf not, remove the connections for the xgboost and reconnect starting with the training set, followed by hyperparameter_tuning/xgboost.\nAdded by Oluwadara Adedeji",
        "section": "Module 3: Orchestration",
        "question": "Video 3.2.5 -  TypeError: string indices must be integers"
      },
      {
        "text": "This means your MLflow container tries to access a db file which was a backend for a different MLflow version than the one you have in the container. Most likely, the MLflow version in the container does not match the MLflow version of the MLflow server you ran in module 2.\nThe easiest solution is to check which version you worked with before, and change the docker image accordingly.\nYou can check your version by opening a terminal on your host, conda activate into the env you worked in, and run:\nmlflow --version\nNow edit the mlflow.dockerfile line to your version:\nRUN pip install mlflow==2.??.??\nSave the file and rebuild the docker service by running:\ndocker-compose build\nNow you can start up the containers again and your MLflow container should be able to successfully read your mounted DB file.",
        "section": "Module 3: Orchestration",
        "question": "MLflow container error: Can't locate revision identified by \u2026"
      },
      {
        "text": "When you use github codespaces and you get permission denied when trying to set up the server, you need to follow this guide:\nhttps://askubuntu.com/questions/409025/permission-denied-when-running-sh-scripts",
        "section": "Module 3: Orchestration",
        "question": "Permission denied in github codespace"
      },
      {
        "text": "Here",
        "section": "Module 3: Orchestration",
        "question": "Where is the FAQ for Prefect questions?"
      },
      {
        "text": "This error means that you are not writing below server on Docker Compose file. To solve the issue",
        "section": "Module 3: Orchestration",
        "question": "(root) Additional property mlflow is not allowed"
      },
      {
        "text": "By default, the logged model and artifacts are stored in a local folder in the mlflow container but not in /home/src/mlflow, so when the container is restarted (after a compose down or container remove) the artifacts are deleted and you can not see them in mlflow UI.\nA simple solution to avoid this issue: You can include a new volume in the docker compose service mlflow to map a folder in the local machine to the folder /mlartifacts in the mlflow container:\n- \"${PWD}/mlartifacts:/mlartifacts/\"\nThen, every data logged to the experiment will be available when the mlflow container is recreated.\nAdded by edumunozsala",
        "section": "Module 3: Orchestration",
        "question": "Q6: Logged model artifacts lost when mlflow container is down or removed"
      },
      {
        "text": "When using localstore, try to start mlflow where mlflow.db is present. For example, mlflow.db is present in mlops/mlflow, cd to that folder, and run ../scrtips/start.sh (assuming you followed the instructions in homework.md file of week3 and setup mlops folder)\nAdded by Vijay",
        "section": "Module 3: Orchestration",
        "question": "Q6: mlflow not showing artifacts"
      },
      {
        "text": "For the correct mlflow tracking uri, use : mlflow.set_tracking_uri(uri=\"http://mlflow:5000\") provided that you used the suggested docker file snippet in the homework question 6.\nAdded by Victor",
        "section": "Module 3: Orchestration",
        "question": "Q6: Correct mlflow tracking uri"
      },
      {
        "text": "You should not run docker compose up for mage repo, should always use bash ./scripts/start.sh\nA\n>>> Update from another student of mlops zoomcamp: The start.sh script also runs docker compose up. And depending on how you start your mage project (like starting a fresh one in the capstone project), you may not have a start.sh or scripts directory. The most important thing about start.sh is that it sets the PROJECT_NAME and MAGE_CODE_PATH before executing docker compose up. These ENV variables can and probably should be set in your .env file.\nUpdate added by Claudia van Dijk",
        "section": "Module 3: Orchestration",
        "question": "I get below error invalid mount config for type \"bind\": invalid specification: destination can't be '/' when running docker compose up when running mage"
      },
      {
        "text": "In a mage block, the python statement mlflow.set_tracking_uri() was returning an attribute error. This problem was observed when running mage in one container and mlflow in another. If you encounter this, consider that there may be something else in your project with the name \u201cmlflow\u201d.\nInsert a statement before the python statement that produces the attribute error: print(mlflow.__file__) to see what the mlflow module points to. It should return a site-packages location, something like '/usr/local/lib/python3.10/site-packages/mlflow/__init__.py'.\nIf not, you may have another file or folder called \u201cmlflow\u201d that is confusing the python import statement.\nLook at the folder name where the mlflow.db is being created via this command (either in command line or in the dockerfile for the mlflow service):\nmlflow server --backend-store-uri sqlite:///home/mlflow/mlflow.db --host\", \"0.0.0.0 --port 5000\nIf the folder name for the backend store is mlflow, as above, python may be trying to import that instead of the mlflow package you installed. You will need to change the backend-store folder name to something else, like mlflow_data.\nRename the folder it in your local drive (since it gets mounted in docker-compose.yml);\nChange the folder name in the dockerfile for the mlflow service (where you specify the backend-store-uri in the mlflow server command)\nChange the folder name in docker-compose.yml (when mounting the folder for the mlflow service), e.g. \nVolumes:\n  - \"${PWD}/mlflow_data:/home/mlflow_data/\"\nThis should resolve the issue of confusing python with which mlflow to import.\nIf the import mlflow statement now gives a \u201cmodule not found\u201d error, check the PYTHONPATH variable in the container by ssh-ing into the running container, as follows:\ndocker ps   (copy the mage container ID)\ndocker exec -it <container-ID> /bin/bash\necho $PYTHONPATH\nIf you do not see the path to the site-packages directory for your python version, you will need to add it to the PYTHONPATH environment variable.\nTo find out what path you should use, execute this from the running container that you have ssh\u2019d into:\nPython\n>>> import sys\n>>> print(sys.path)\nYou will hopefully see a path for the site-packages directory for your current python version.\nAdd this to the PYTHONPATH in the Dockerfile for the Mage service with this line:\nENV PYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python3.10/site-packages\"\nAdded by Claudia van Dijk",
        "section": "Module 3: Orchestration",
        "question": "AttributeError: module 'mlflow' has no attribute 'set_tracking_url'"
      },
      {
        "text": "The newest  version of Prefect does not have the module project. To initiate a project, use command `project init`",
        "section": "Module 3: Orchestration",
        "question": "prefect project init Error: No such command 'project'."
      },
      {
        "text": "Solution: Check the difference between xgboost and sklearn pipelines. In xgboost pipeline there is a track_experiment callback while in sklearn pipeline is not.\nPlease add those lines:\nYou can refer them in my similar commit\nLines to be added\nAdded by Nilesh Arte",
        "section": "Module 3: Orchestration",
        "question": "Video 3.3.4 Training Metrics RMSE chart does not show due to the error: KeyError: \u2018rmse_LinearRegression\u2019"
      },
      {
        "text": "A: Use the docker.Run plugin in your Kestra task to run containers. This plugin supports advanced Docker options like custom networks.\nFor local development, you can use networkMode: host to allow containers to access services on your host (e.g., MLflow running on localhost).\nExample:\nnetworkMode: host\n\u26a0\ufe0f Note: host mode is only supported on Linux. For Docker Desktop on Windows/macOS, use host.docker.internal or create a shared Docker network.\nBest practice:\nIn production setups, tools like MLflow should run outside Kestra and be accessed over a stable URI (e.g., a cloud endpoint or a container with a known hostname in a shared network).\nAdded by Jos\u00e9 Luis Mart\u00ednez",
        "section": "Module 3: Orchestration",
        "question": "Q: How can I enable communication between Docker containers when invoked from a Kestra task?"
      },
      {
        "text": "We come across situations in data transformation & pre-processing as well as model training in a ML pipeline where we need to handle dataset of high dimensionality or/and high cardinality (usually millions). And we often end up with Out of Memory (OOM) errors like below when the flow is running:\nIf you do not have the option of increasing your RAM, the following 3 approaches can be effective in mitigating this error:\nIf at all possible, during the data loading step, read only the required features/columns from the dataset, eg.\nBefore encoding/vectorizing, when we get our X_train & y_train, we can remove the dataframe, eg.\nIf you do not have a swap file or have a small one, create a swap file (size as per memory requirement) or replace the existing one with a proper sized one.Eg.\nTo remove existing swapfile, issue commands:\nsudo swapoff /swapfile\nsudo rm /swapfile\nTo create a new proper sized (I\u2019m setting 16 GB in my case) swapfile, issue commands:\nsudo fallocate -l 16G /swapfile\nsudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\nTo check the swap file created, issue command:\nfree -h\nAdded by Siddhartha Gogoi",
        "section": "Module 4: Deployment",
        "question": "Fix Out of Memory error while orchestrating the workflow on a ML Pipeline for a high volume dataset."
      },
      {
        "text": "Windows with AWS CLI already installed\nAWS CLI version:\naws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\nExecuting\n$(aws ecr get-login --no-include-email)\nshows error\naws.exe: error: argument operation: Invalid choice, valid choices are\u2026\nUse this command instead. More info here:\nhttps://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\naws ecr get-login-password \\\n--region <region> \\\n| docker login \\\n--username AWS \\\n--password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\nAdded by MarcosMJD",
        "section": "Module 4: Deployment",
        "question": "aws.exe: error: argument operation: Invalid choice \u2014 Docker can not login to ECR."
      },
      {
        "text": "Use ` at the end of each line except the last. Note that multiline string does not need `.\nEscape \u201c to \u201c\\ .\nUse $env: to create env vars (non-persistent). E.g.:\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\naws kinesis put-record --cli-binary-format raw-in-base64-out `\n--stream-name $env:KINESIS_STREAM_INPUT `\n--partition-key 1 `\n--data '{\n\\\"ride\\\": {\n\\\"PULocationID\\\": 130,\n\\\"DOLocationID\\\": 205,\n\\\"trip_distance\\\": 3.66\n},\n\\\"ride_id\\\": 156\n}'\nAdded by MarcosMJD",
        "section": "Module 4: Deployment",
        "question": "Multiline commands in Windows Powershell"
      },
      {
        "text": "If one gets pipenv failures for pipenv install command -\nAttributeError: module 'collections' has no attribute 'MutableMapping'\nIt happens because you use the system Python (3.10) for pipenv.\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it\u2019s just this: pip install pipenv\nNow re-run pipenv install XXXX (relevant dependencies) - should work\nTested and worked on AWS instance, similar to the config Alexey presented in class.\nAdded by Daniel HenSSL",
        "section": "Module 4: Deployment",
        "question": "Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')"
      },
      {
        "text": "First check if SSL module configured with following command:\nPython -m ssl\n\nIf the output of this is empty there is no problem with SSL configuration.\n\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\nAdded by Kenan Arslanbay",
        "section": "Module 4: Deployment",
        "question": "module is not available (Can't connect to HTTPS URL)"
      },
      {
        "text": "During scikit-learn installation via the command:\npipenv install scikit-learn==1.0.2\nThe following error is raised:\nModuleNotFoundError: No module named 'pip._vendor.six'\nThen, one should:\nsudo apt install python-six\npipenv --rm\npipenv install scikit-learn==1.0.2\nAdded by Giovanni Pecoraro",
        "section": "Module 4: Deployment",
        "question": "No module named 'pip._vendor.six'"
      },
      {
        "text": "Problem description. How can we use Jupyter notebooks with the Pipenv environment?\nSolution: Refer to this stackoverflow question. Basically install jupyter and ipykernel using pipenv. And then register the kernel with `python -m ipykernel install --user --name=my-virtualenv-name` inside the Pipenv shell. If you are using Jupyter notebooks in VS Code, doing this will also add the virtual environment in the list of kernels.\nAdded by Ron Medina",
        "section": "Module 4: Deployment",
        "question": "Pipenv with Jupyter"
      },
      {
        "text": "Problem: I tried to run starter notebook on pipenv environment but had issues with no output on prints. \nI used scikit-learn==1.2.2 and python==3.10\nTornado version was 6.3.2\n\nSolution: The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\nDowngrading to tornado==6.1 fixed the issue\nhttps://stackoverflow.com/questions/54971836/no-output-jupyter-notebook",
        "section": "Module 4: Deployment",
        "question": "Pipenv with Jupyter no output"
      },
      {
        "text": "Problem description:  You might get an error \u2018Invalid base64\u2019 after running the \u2018aws kinesis put-record\u2019 command on your local machine. This might be the case if you are using the AWS CLI version 2 (note that in the video 4.4, around 57:42, you can see a warning since the instructor is using v1 of the CLI.\nSolution description: To get around this, pass the argument \u2018--cli-binary-format raw-in-base64-out\u2019. This will encode your data string into base64 before passing it to kinesis\nAdded by M",
        "section": "Module 4: Deployment",
        "question": "\u2018Invalid base64\u2019 error after running `aws kinesis put-record`"
      },
      {
        "text": "Problem description:   Running starter.ipynb in homework\u2019s Q1 will show up this error.\nSolution description: Update pandas (actually pandas version was the latest, but several dependencies are updated).\nAdded by Marcos Jimenez",
        "section": "Module 4: Deployment",
        "question": "Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file."
      },
      {
        "text": "Use command $pipenv lock to force the creation of Pipfile.lock\nAdded by Bijay P.",
        "section": "Module 4: Deployment",
        "question": "Pipfile.lock was not created along with Pipfile"
      },
      {
        "text": "This issue is usually due to the pythonfinder module in pipenv.\nThe solution to this involves manually changing the scripts as describe here python_finder_fix\nAdded by Ridwan Amure",
        "section": "Module 4: Deployment",
        "question": "Permission Denied using Pipenv"
      },
      {
        "text": "There is a possibility to load and store the data in a Google Cloud Storage bucket. To do that, we will need to authenticate through the IDE we are using (for example github Codespaces) and allow the read and write from/to a GCS bucket:\nAuthenticate gsutil with your GCP account: gsutil config\nUpload the data to your GCS bucket: gsutil cp path/to/local/data gs://your-bucket-name\nIn the GCP Console, go to the \"IAM & Admin\" section, then \"Service accounts.\"\nCreate a new service account, grant it the necessary permissions (e.g., \"Storage Object Admin\" for GCS access), and generate a JSON key file.\nInstall the Google Cloud SDK: https://cloud.google.com/sdk/docs/install\nAuthenticate the SDK with your GCP account: gcloud auth login\nSet the project: gcloud config set project YOUR_GCP_PROJECT_ID\nInstall google cloud storage library (you can do it with a pip install directly in your notebook): !pip install google-cloud-storage\nExample script on how to do it to load a file from a csv to a pandas df:\nfrom google.cloud import storage\nimport pandas as pd\n# Set up the storage client with the service account key\nstorage_client = storage.Client.from_service_account_json('path/to/service-account-key.json')\n# Get the GCS bucket\nbucket = storage_client.get_bucket('your-bucket-name')\n# List the contents of the bucket\nblobs = bucket.list_blobs()\nfor blob in blobs:\nprint(blob.name)\n# Load a CSV file from the bucket into a pandas DataFrame\ncsv_blob = bucket.blob('path/to/csv/in/bucket.csv')\ndf = pd.read_csv(csv_blob.download_as_string())\nYou can directly save output data by setting the output file name to your desired file gsutil uri.\nAdded by M\u00e9lanie Fouesnard",
        "section": "Module 4: Deployment",
        "question": "Going further with Google Cloud Platform (load and save data to GCS)"
      },
      {
        "text": "When passing arguments to a script via command line and converting it to a 4 digit number using f\u2019{year:04d}\u2019, this error showed up.\nThis happens because all inputs from the command line are read as string by the script. They need to be converted to numeric/integer before transformation in fstring.\nyear = int(sys.argv[1])\nf\u2019{year:04d}\u2019\nIf you use click library just edit a decorator\n@click.command()\n@click.option( \"--year\",  help=\"Year for evaluation\",   type=int)\ndef  your_function(year):\n<<Your code>>\nAdded by Taras Sh",
        "section": "Module 4: Deployment",
        "question": "Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']"
      },
      {
        "text": "Ensure the correct image is being used to derive from.\nCopy the data from local to the docker image using the COPY command to a relative path. Using absolute paths within the image might be troublesome.\nUse paths starting from /app and don\u2019t forget to do WORKDIR /app before actually performing the code execution.\nMost common commands\nBuild container using docker build -t mlops-learn .\nExecute the script using docker run -it --rm mlops-learn\n<mlops-learn> is just a name used for the image and does not have any significance.",
        "section": "Module 4: Deployment",
        "question": "Dockerizing tips"
      },
      {
        "text": "If you are trying to run Flask gunicorn & MLFlow server from the same container, defining both in Dockerfile with CMD will only run MLFlow & not Flask.\nSolution: Create separate shell script with server run commands, for eg:\n> \tscript1.sh\n#!/bin/bash\ngunicorn --bind=0.0.0.0:9696 predict:app\nAnother script with e.g. MLFlow server:\n>\tscript2.sh\n#!/bin/bash\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\nCreate a wrapper script to run above 2 scripts:\n>\twrapper_script.sh\n#!/bin/bash\n# Start the first process\n./script1.sh &\n# Start the second process\n./script2.sh &\n# Wait for any process to exit\nwait -n\n# Exit with status of process that exited first\nexit $?\nGive executable permissions to all scripts:\nchmod +x *.sh\nNow we can define last line of Dockerfile as:\n> \tDockerfile\nCMD ./wrapper_script.sh\nDont forget to expose all ports defined by services!",
        "section": "Module 4: Deployment",
        "question": "Running multiple services in a Docker container"
      },
      {
        "text": "Problem description cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1\nSolution: you need to force and upgrade wheel and pipenv\nJust run the command line :\npip install --user --upgrade --upgrade-strategy eager pipenv wheel",
        "section": "Module 4: Deployment",
        "question": "Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)"
      },
      {
        "text": "Problem description. How can we connect s3 bucket to MLFLOW?\nSolution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\nRead more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\nAdded by Akshit Miglani",
        "section": "Module 4: Deployment",
        "question": "Connecting s3 bucket to MLFLOW"
      },
      {
        "text": "Even though the upload works using aws cli and boto3 in Jupyter notebook.\nSolution set the AWS_PROFILE environment variable (the default profile is called default)",
        "section": "Module 4: Deployment",
        "question": "Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\""
      },
      {
        "text": "Problem description: lib_lightgbm.so Reason: image not found\nSolution description: Add \u201cRUN apt-get install libgomp1\u201d to your docker. (change installer command based on OS)\nAdded by Kazeem Hakeem",
        "section": "Module 4: Deployment",
        "question": "Dockerizing lightgbm"
      },
      {
        "text": "When the request is processed in lambda function, mlflow library raises:\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module 'dataclasses' has no attribute '__version__'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\nSolution: Increase the memory of the lambda function.\nAdded by MarcosMJD",
        "section": "Module 4: Deployment",
        "question": "Error raised when executing mlflow\u2019s pyfunc.load_model in lambda function."
      },
      {
        "text": "Just a note if you are following the video but also using the repo\u2019s notebook The notebook is the end state of the video which eventually uses mlflow pipelines.\nJust watch the video and be patient. Everything will work :)\nAdded by Quinn Avila",
        "section": "Module 4: Deployment",
        "question": "4.3 FYI Notebook is end state of Video -"
      },
      {
        "text": "Added by Oluwadara Adedeji",
        "section": "Module 4: Deployment",
        "question": "Solution: The notebook in the repo is missing some code, Include the code to log the dict_vectorizer. If the error is after using pipelines, update the predict function as seen in the video."
      },
      {
        "text": "Problem description: I was having issues because my python script was not reading AWS credentials from env vars, after building the image I was running it like this:\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\nSolution 1:\n\nEnvironment Variables: \nYou can set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you are using AWS STS) environment variables. You can set these in your shell, or you can include them in your Docker run command like this:\nI found out by myself that those variables must be passed before specifying the name of the image, as follow:\ndocker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\nSolution 2:\nIf you want to pass an env file, you can also do so by adding, for an env file called .env:\ndocker run -it homework-04 --env-file .env\nSolution 3 (if AWS credentials were not found):\nAWS Configuration Files: \nThe AWS SDKs and CLI will check the ~/.aws/credentials and ~/.aws/config files for credentials if they exist. You can map these files into your Docker container using volumes:\n\ndocker run -it --rm -v ~/.aws:/root/.aws homework:v1\nAdded by Erick Cal\nLast edited by: Fustincho",
        "section": "Module 4: Deployment",
        "question": "Passing envs to my docker image"
      },
      {
        "text": "If anyone is troubleshooting or just interested in seeing the model listed on the image svizor/zoomcamp-model:mlops-3.10.0-slim.\nCreate a dockerfile. (yep thats all) and build \u201cdocker build -t zoomcamp_test .\u201d\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\nRun \u201cdocker run -it zoomcamp_test ls /app\u201d output -> model.bin\nThis will list the contents of the app directory and \u201cmodel.bin\u201d should output. With this you could just copy your files, for example \u201ccopy myfile .\u201d maybe a requirements file and this can be run for example \u201cdocker run -it myimage myscript arg1 arg2 \u201d. Of course keep in mind a build is needed everytime you change the Dockerfile.\nAnother variation is to have it run when you run the docker file.\n\u201c\u201d\u201d\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\nWORKDIR /app\nCMD ls\n\u201c\u201d\u201d\nJust keep in mind CMD is needed because the RUN commands are used for building the image and the CMD is used at container runtime. And in your example you probably want to run a script or should we say CMD a script.\nQuinn Avila",
        "section": "Module 4: Deployment",
        "question": "How to see the model in the docker container in app/?"
      },
      {
        "text": "To resolve this make sure to build the docker image with the platform tag, like this:\n\u201cdocker build -t homework:v1 --platform=linux/arm64 .\u201d",
        "section": "Module 4: Deployment",
        "question": "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested"
      },
      {
        "text": "Solution: instead of input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  use input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\nIlnaz Salimov\nsalimovilnaz777@gmail.com",
        "section": "Module 4: Deployment",
        "question": "HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb"
      },
      {
        "text": "i'm getting this error ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\nand Resolved from this command pip install pipenv --force-reinstall\ngetting this errror site-packages\\pipenv\\patched\\pip\\_vendor\\urllib3\\connectionpool.py\"\nResolved from this command pip install -U pip and pip install requests\nAsif",
        "section": "Module 4: Deployment",
        "question": "ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'"
      },
      {
        "text": "When installing pipenv using --user option, not to all users, you would need to update the PATH environment variable to run pipenv commands. \nYou can update the env variable but its much better to update your .bashrc or .profile, depends on you OS, to persist the change. Go to your .bashrc file and include or update a line like this:\n`PATH=\"<path_to_your_pipenv_install_dir>:$PATH\u201d`\nOr you can try to reinstall pipenv as root, for all users:\nsudo -H pip install -U pipenv\nAdded by Eduardo Mu\u00f1oz",
        "section": "Module 4: Deployment",
        "question": "Error pipenv command not found after pipenv installation"
      },
      {
        "text": "For question 2 which requires you to prepare the dataframe with the output, you need to first define the year and month as integers.\nAdded by Victor E.",
        "section": "Module 4: Deployment",
        "question": "Homework/Question 2: Namerror: name \u2018year\u2019 is not defined"
      },
      {
        "text": "When returning an object from a block, you may encounter an error like\nError loading custom_object at /home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0: [Errno 2] No such file or directory: '/home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0/object.joblib'\nError loading custom_object at /home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0: [Errno 2] No such file or directory: '/home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0/object.joblib'\nThis happened to me when returning a numpy.ndarray, namely the y_pred variable containing the predictions for the taxi dataset. I believe Mage struggles returning some type of objects and expects things like DataFrames instead of numpy.ndarrays. What I did was to return a df that had both the y_pred and the ride ids.\nAdded by Fustincho",
        "section": "Module 4: Deployment",
        "question": "Mage error: Error loading custom object at\u2026"
      },
      {
        "text": "You may get a warning similar to the one below when trying to run the docker\nWARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\nPython 3.10.13 (main, Mar 12 2024, 12:22:40) [GCC 12.2.0] on linux\nAdd the tag --platform linux/amd64 when running and it should work. For example\ndocker run -it --platform linux/amd64 --rm -p 9696:9696 homework:v2",
        "section": "Module 4: Deployment",
        "question": "The arm64 chip doesn\u2019t match with Alexey\u2019s docker image"
      },
      {
        "text": "Make sure you have python and pip\npython --version\npip --version\nPreferred Installation of Pipenv\npip install pipenv --user",
        "section": "Module 4: Deployment",
        "question": "Pipenv installation"
      },
      {
        "text": "If you encounter an error when converting your notebook.ipnb into a Python script using the command:\njupyter nbconvert --to script your_notebook.ipynb\nand you see the error message:\nJupyter command `jupyter-nbconvert` not found.\nfollow these steps:\nVerify that you're in the directory containing your Jupyter notebook.( I\u2019ve got this error message that confused me).\nInstall the necessary package: If the issue persists, you may need to install the nbconvert package. Run the following command:\npip install nbconvert\n:After installing nbconvert, use the following command to convert your notebook to a Python script:\njupyter nbconvert your_notebook.ipynb --to python\nNote: The correct command is slightly different (--to python instead of --to script).\nAdded by Anatolii Kryvko",
        "section": "Module 4: Deployment",
        "question": "Jupyter nbconvert error"
      },
      {
        "text": "For question 6 which requires you to put your script in a docker file,  you can specify that the folder \u2018output/yellow\u2019 should be created in the working directory of your docker container by specifying \u2018RUN mkdir -p output/yellow\u2019 in your docker file.\nAdded by Victor E.",
        "section": "Module 4: Deployment",
        "question": "Homework/Question 6: Do not forget to specify that the folder output/yellow should be created in the working directory of your docker file"
      },
      {
        "text": "For question 6, if you are using the script as instructed in the homework and not flask, your endpoint should be \u2018bash\u2019. This can be set by specifying ENTRYPOINT = [\u201cbash\u201d].\nAdded by Victor E.",
        "section": "Module 4: Deployment",
        "question": "Homework/Question 6: Entry point for running scoring script in Docker container"
      },
      {
        "text": "This error appeared when I was running the jupyter notebooks inside visual code in codespace. I fixed just running the jupyter notebooks outside codespaces.\nAdded by Ibai Irastorza",
        "section": "Module 5: Monitoring",
        "question": "Unable to locate credentials"
      },
      {
        "text": "Problem description: When running docker-compose up as shown in the video 5.2 if you go to http://localhost:3000/ you get asked for a username and a password.\nSolution: for both of them the default is \u201cadmin\u201d. Then you can enter your new password. \nSee also here\nAdded by JaimeRV",
        "section": "Module 5: Monitoring",
        "question": "Login window in Grafana"
      },
      {
        "text": "Problem Description : In Linux, when starting services using docker compose up --build  as shown in video 5.2, the services won\u2019t start and instead we get message unknown flag: --build in command prompt.\nSolution : Since we install docker-compose separately in Linux, we have to run docker-compose up --build instead of docker compose up --build\nAdded by Ashish Lalchandani",
        "section": "Module 5: Monitoring",
        "question": "Error in starting monitoring services in Linux"
      },
      {
        "text": "Problem: When running prepare.py getting KeyError: \u2018content-length\u2019\nSolution: From Emeli Dral:\nIt seems to me that the link we used in prepare.py to download taxi data does not work anymore. I substituted the instruction:\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\nby the\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\nin the prepare.py and it worked for me. Hopefully, if you do the same you will be able to get those data.",
        "section": "Module 5: Monitoring",
        "question": "KeyError \u2018content-length\u2019 when running prepare.py"
      },
      {
        "text": "Problem description\nWhen I run the command \u201cdocker-compose up \u2013build\u201d and send the data to the real-time prediction service. The service will return \u201cMax retries exceeded with url: /api\u201d.\nIn my case it because of my evidently service exit with code 2 due to the \u201capp.py\u201d in evidently service cannot import \u201cfrom pyarrow import parquet as pq\u201d.\nSolution description\nThe first solution is just install the pyarrow module \u201cpip install pyarrow\u201d\nThe second solution is restart your machine.\nThe third solution is if the first and second one didn\u2019t work with your machine. I found that \u201capp.py\u201d of evidently service didn\u2019t use that module. So comment the pyarrow module out and the problem was solved for me.\nAdded by Surawut Jirasaktavee",
        "section": "Module 5: Monitoring",
        "question": "Evidently service exit with code 2"
      },
      {
        "text": "When using evidently if you get this error.\nYou probably forgot to and parentheses () just and opening and closing and you are good to go.\nQuinn Avila",
        "section": "Module 5: Monitoring",
        "question": "ValueError: Incorrect item instead of a metric or metric preset was passed to Report"
      },
      {
        "text": "You will get an error if you didn\u2019t add a target=\u2019duration_min\u2019\nIf you want to use RegressionQualityMetric() you need a target=\u2019duration_min and you need this added to you current_data[\u2018duration_min\u2019]\nQuinn Avila",
        "section": "Module 5: Monitoring",
        "question": "For the report RegressionQualityMetric()"
      },
      {
        "text": "Problem description\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\nSolution description\nThis happens because the generated data is based on an early date therefore the training dataset would be empty.\nAdjust the following\nbegin = datetime.datetime(202X, X, X, 0, 0)\nAdded by Luke",
        "section": "Module 5: Monitoring",
        "question": "Found array with 0 sample(s)"
      },
      {
        "text": "Problem description\nGetting \u201ctarget columns\u201d \u201cprediction columns\u201d not present errors after adding a metric\nSolution description\nMake sure to read through the documentation on what is required or optional when adding the metric. I added DatasetCorrelationsMetric which doesn\u2019t require any parameters because the metric evaluates for correlations among the features.\nSam Lim",
        "section": "Module 5: Monitoring",
        "question": "Adding additional metric"
      },
      {
        "text": "When you try to login in Grafana with standard requisites (admin/admin) it throw up an error.\nAfter run grafana-cli admin reset-admin-password admin in Grafana container the problem will be fixed\nAdded by Artem Glazkov\nCommand above is deprecated:\nDeprecation warning: The standalone 'grafana-cli' program is deprecated and will be removed in the future. Please update all uses of 'grafana-cli' to 'grafana cli'\nTo enter the docker container with grafana, find the Container ID by running:\ndocker ps\nThen use the Container ID from grafana and prepend it to the command above like this:\nlpep_pickup_datetime<container_ID> grafana cli admin reset-admin-password admin\nAdded by Svetlana Ulianova",
        "section": "Module 5: Monitoring",
        "question": "Standard login in Grafana does not work"
      },
      {
        "text": "Problem description. While my metric generation script was still running, I noticed that the charts in Grafana don\u2019t get updated.\nSolution description. There are two things to pay attention to:\nRefresh interval: set it to a small value: 5-10-30 seconds\nUse your local timezone in a call to `pytz.timezone` \u2013 I couldn\u2019t get updates before changing this from the original value \u201cEurope/London\u201d to my own zone",
        "section": "Module 5: Monitoring",
        "question": "The chart in Grafana doesn\u2019t get updates"
      },
      {
        "text": "Problem description. Prefect server was not running locally, I ran `prefect server start` command but it stopped immediately..\nSolution description. I used Prefect cloud to run the script, however I created an issue on the Prefect github.\nBy Erick Calderin",
        "section": "Module 5: Monitoring",
        "question": "Prefect server was not running locally"
      },
      {
        "text": "Solution. Using docker CLI run docker system prune to remove unused things (build cache, containers, images etc)\nAlso, to see what\u2019s taking space before pruning you can run docker system df\nBy Alex Litvinov",
        "section": "Module 5: Monitoring",
        "question": "no disk space left error when doing docker compose up"
      },
      {
        "text": "Problem: when run docker-compose up \u2013build, you may see this error. To solve, add `command: php -S 0.0.0.0:8080 -t /var/www/html` in adminer block in yml file like:\nadminer:\ncommand: php -S 0.0.0.0:8080 -t /var/www/html\nimage: adminer\n\u2026\nIlnaz Salimov\nsalimovilnaz777@gmail.com",
        "section": "Module 5: Monitoring",
        "question": "Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)"
      },
      {
        "text": "Problem: Can we generate charts like Evidently inside Grafana?\nSolution: In Grafana that would be a stat panel (just a number) and scatter plot panel (I believe it requires a plug-in). However, there is no native way to quickly recreate this exact Evidently dashboard. You'd need to make sure you have all the relevant information logged to your Grafana data source, and then design your own plots in Grafana.\nIf you want to recreate the Evidently visualizations externally, you can export the Evidently output in JSON with include_render=True\n(more details here https://docs.evidentlyai.com/user-guide/customization/json-dict-output) and then parse information from it for your external visualization layer. To include everything you need for non-aggregated visuals, you should also add \"raw_data\": True  option (more details here https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\nOverall, this specific plot with under- and over-performance segments is more useful during debugging, so might be easier to access it ad hoc using Evidently.\nAdded by Ming Jun, Asked by Luke, Answered by Elena Samuylova",
        "section": "Module 5: Monitoring",
        "question": "Generate Evidently Chart in Grafana"
      },
      {
        "text": "A new version of Numpy has just been released v 2.0.0 (on Jun 16, 2024), and this causes an import error of the package.\n\"`np.chararray` is deprecated and will be removed from \"\n419     \t\"the main namespace in the future. Use an array with a string \"\n420     \t\"or bytes dtype instead.\", DeprecationWarning, stacklevel=2): `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\nOr\nAttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\nYou can solve it by reinstalling numpy to previous version 1.26.4. Just run:\npython -m pip install numpy==1.26.4\nOr modify the requirements.txt to freeze the version:\nnumpy==1.26.4\nAdded by Eduardo Mu\u00f1oz\nProblem: Grafarna and Adminer not showing at their designated URIs after Docker Compose\nSolution: If you are using a VM, you will have to forward the ports (on VS Code or any other editor)\nAdded by Oluwadara Adedeji",
        "section": "Module 5: Monitoring",
        "question": "Error when importing evidently package because of numpy version upgraded"
      },
      {
        "text": "Problem: When trying to start the postgres services through docker-compose up, this error occurs\nNote: This issue occurs since port 5432 is already used by some other service.\nSolution: update the port mapping for postgres service to 5433:5432 in docker compose yaml\nAdded by Sumeet Lalla",
        "section": "Module 5: Monitoring",
        "question": "Bind for 0.0.0.0:5432 failed: port is already allocated"
      },
      {
        "text": "Problem: for 5.4, when trying to create a new dashboard grafana does not list the \u2018dummy_metrics\u2019 table in the query tab.\nNote: also change the datasource name other than PostgreSQL\nSolution1: update the config/grafana_datasources.yaml to the following:\n# list of datasources to insert/update\n# available in the database\ndatasources:\n- name: NewPostgreSQL\ntype: postgres\nurl: db:5432\nuser: postgres\nsecureJsonData:\npassword: 'example'\njsonData:\nsslmode: 'disable'\ndatabase: test\nSolution 2: Utilise the Code option rather than the Builder option and load the data in using your own SQL queries. See screenshot below (box highlight in red). Tip, if you write your FROM statement first the SELECT options are able to be done through auto-complete too.\nAnswered by  Anuj Panthri, added by Andrea Nicolas, edit by Marcus Leiwe",
        "section": "Module 5: Monitoring",
        "question": "Table/database not showing on grafana dashboard"
      },
      {
        "text": "Problem: After running docker compose, adminer cannot be accessed on http://127.0.0.1:8080/\nSolution: Just add index.php after URL, so the URL will be http://127.0.0.1:8080/index.php\nAdded by Rohmat S",
        "section": "Module 5: Monitoring",
        "question": "Adminer Not Loaded"
      },
      {
        "text": "Problem: When selecting column from table db has no time column: no time column found\nSolution: Add timestamp column in query builder\nAdded by Rohmat S",
        "section": "Module 5: Monitoring",
        "question": "Grafana UI Changes"
      },
      {
        "text": "Problem: When to running evidently_metrics_calculation.py show RuntimeError: Cannot create flow run. Failed to reach API at https://api.prefect.cloud/api/accounts/ee976605-4ca7-4a27-b5e3-0a37da3c7678/workspaces/78b23cf5-38bb-4d8b-9888-5bf8070d6d62/.\nSolution: Register or Signup on https://app.prefect.cloud/account/\nAdded by Sofyan Akbar",
        "section": "Module 5: Monitoring",
        "question": "Runtime Error : Failed to Reach API on Prefect"
      },
      {
        "text": "Problem: You\u2019ve already loaded your data, created a dashboard and even saved it according to the exercise. But suddenly when you run docker-compose up after saving the dashboard file you get this error: db query error: pq: database \u201ctest\u201d does not exist\nSolution:\nThis error indicates you haven\u2019t run the DB initialization code. But if you did run it before and even saw results, the most likely reason for the error is that you restarted the docker-compose services.\nThe default docker-compose.yml file doesn\u2019t have a volume for the Postgres DB, so every restart will also delete the DB data with it.\nIf you\u2019re not planning to restart the services again, the easiest solution would be to run the DB initialization and filling code of your exercise again and forget about this.\nOn the other hand, if you\u2019ll run more services restarts, consider adding a volume to your postgres service in the docker-compose.yml file, e.g.:\nvolumes:\n- ./data/postgres:/var/lib/postgresql/data\n* Notice i added a new directory to the project ./data directory!\nYou can run the following in order for the volume to be attached:\ndocker-compose down\ndocker-compose up --build\nAdded by Igal Chernov",
        "section": "Module 5: Monitoring",
        "question": "Grafana dashboard error after reset: db query error: pq: database \u201ctest\u201d does not exist"
      },
      {
        "text": "There are several alternatives to Evidently for monitoring machine learning models in the cloud. Here are a few options on popular cloud platforms:\nGoogle Cloud Platform (GCP): AI Platform Predictions with Cloud Monitoring & Logging\nMicrosoft Azure: Azure Machine Learning\nAmazon Web Services (AWS): Amazon SageMaker Model Monitor\nThese services provide model monitoring capabilities, allowing you to track the performance and data quality of your machine learning models within the cloud environment.\nAdded by M\u00e9lanie Fouesnard",
        "section": "Module 5: Monitoring",
        "question": "Are there any alternative to Evidently on cloud platforms ?"
      },
      {
        "text": "Instead of:  docker-compose up \u2013build\nUse: docker compose up \u2013build\nAdded Ibai Irastorza",
        "section": "Module 5: Monitoring",
        "question": "docker.errors.DockerException: Error while fetching server API version: HTTPConnection.request() got an unexpected keyword argument 'chunked'"
      },
      {
        "text": "Related to the question above. Docker compose v1 is depreciated (from April 2023 onwards). More information on why v2 is better is in this blog post here\nhttps://www.docker.com/blog/new-docker-compose-v2-and-v1-deprecation/\nAdded by Marcus Leiwe",
        "section": "Module 5: Monitoring",
        "question": "Docker-Compose depreciated"
      },
      {
        "text": "It could be that there is already a another docker container running (for example, from previous week).\nCheck with docker ps, and stop it with docker stop container_name_or_ID\nAdded Ibai Irastorza",
        "section": "Module 5: Monitoring",
        "question": "psycopg.OperationalError: connection failed: connection to server at \"127.0.0.1\", port 5432 failed: FATAL:  password authentication failed for user \"postgres\""
      },
      {
        "text": "Problem: In adminer ui Adminer access is not working even after right DB user and password\n(It wasn't for me) Adminer simply does not respond and does not show DB details\nSolution: Try accessing DB from command line via psql\nYou can quickly install psql via sudo apt etc\nHere is the example\n(base) cpl@inpne-ed-lab003:~$ psql -h localhost -p 5432 -U postgres\nPassword for user postgres:\npsql (14.12 (Ubuntu 14.12-0ubuntu0.22.04.1), server 16.4 (Debian 16.4-1.pgdg120+1))\nWARNING: psql major version 14, server major version 16.\nSome psql features might not work.\nType \"help\" for help.\npostgres=# \\l\nList of databases\nName    |  Owner   | Encoding |  Collate   |   Ctype    |   Access privileges\n-----------+----------+----------+------------+------------+-----------------------\npostgres  | postgres | UTF8     | en_US.utf8 | en_US.utf8 |\ntemplate0 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +\n|          |          |            |            | postgres=CTc/postgres\ntemplate1 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +\n|          |          |            |            | postgres=CTc/postgres\ntest      | postgres | UTF8     | en_US.utf8 | en_US.utf8 |\n(4 rows)\nAdded by Nilesh Arte",
        "section": "Module 5: Monitoring",
        "question": "Login to DB not working in Adminer UI even after right DB, user and password."
      },
      {
        "text": "In the video (current cohort: 2025) the Evidently used is 0.4.17, but any version up to 0.6.7 will be able to run the code in the video and the repo.\nThe newer versions have changed the APIs so the code in the video will not run.\nAdded by Thanh Trung Mai",
        "section": "Module 6: Best practices",
        "question": "What version of Evidently AI is used in the course?"
      },
      {
        "text": "When following the video instructions and running the Dockerfile I get an error that the Dockerfile build failed in line 8, because there is no matching distribution for mlflow=1.27.0. Below is the code output:\n4.900 ERROR: No matching distribution found for mlflow==1.27.0\n4.901 ERROR: Couldn't install package: {}\n4.901  Package installation failed...\n------\nDockerfile:8\n--------------------\n6 |     COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n7 |\n8 | >>> RUN pipenv install --system --deploy\n9 |\n10 |     COPY [ \"lambda_function.py\", \"model.py\", \"./\" ]\n--------------------\nERROR: failed to solve: process \"/bin/sh -c pipenv install --system --deploy\" did not complete successfully: exit code: 1",
        "section": "Module 6: Best practices",
        "question": "Error following video 6.2: mlflow=1.27.0"
      },
      {
        "text": "You may get an error \u2018{'errorMessage': 'Unable to locate credentials', \u2026\u2019 from the print statement in test_docker.py after running localstack with kinesis.\nTo fix this, in the docker-compose.yaml file, in addition to the environment variables like AWS_DEFAULT_REGION, add two other variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their value is not important; anything like abc will suffice\nAdded by M\nOther possibility is just to run\naws --endpoint-url http://localhost:4566 configure\nAnd providing random values for AWS Access Key ID , AWS Secret Access Key, Default region name, and Default output format.\nAdded by M.A. Monjas",
        "section": "Module 6: Best practices",
        "question": "Get an error \u2018Unable to locate credentials\u2019 after running localstack with kinesis"
      },
      {
        "text": "You may get an error while creating a bucket with localstack and the boto3 client:\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\nTo fix this, instead of creating a bucket via\ns3_client.create_bucket(Bucket='nyc-duration')\nCreate it with\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={\n'LocationConstraint': AWS_DEFAULT_REGION})\nyam\nAdded by M",
        "section": "Module 6: Best practices",
        "question": "Get an error \u2018 unspecified location constraint is incompatible \u2019"
      },
      {
        "text": "When executing an AWS CLI command (e.g., aws s3 ls), you can get the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>.\nTo fix it, simply set the AWS CLI environment variables:\nexport AWS_DEFAULT_REGION=eu-west-1\nexport AWS_ACCESS_KEY_ID=foobar\nexport AWS_SECRET_ACCESS_KEY=foobar\nTheir value is not important; anything would be ok.\nAdded by Giovanni Pecoraro",
        "section": "Module 6: Best practices",
        "question": "Get an error \u201c<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>\u201d after running an AWS CLI command"
      },
      {
        "text": "At every commit the above error is thrown and no pre-commit hooks are ran.\nMake sure the indentation in .pre-commit-config.yaml is correct. Especially the 4 spaces ahead of every `repo` statement\nAdded by M. Ayoub C.",
        "section": "Module 6: Best practices",
        "question": "Pre-commit triggers an error at every commit: \u201cmapping values are not allowed in this context\u201d"
      },
      {
        "text": "No option to remove pytest test\nRemove .vscode folder located on the folder you previously used for testing, e.g. folder code (from week6-best-practices) was chosen to test, so you may remove .vscode inside the folder.\nAdded by Rizdi Aprilian",
        "section": "Module 6: Best practices",
        "question": "Could not reconfigure pytest from zero after getting done with previous folder"
      },
      {
        "text": "Problem description\nFollowing video 6.3, at minute 11:23, get records command returns empty Records.\nSolution description\nAdd --no-sign-request to Kinesis get records call:\n aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator [\u2026] --no-sign-request",
        "section": "Module 6: Best practices",
        "question": "Empty Records in Kinesis Get Records with LocalStack"
      },
      {
        "text": "Problem description\ngit commit -m 'Updated xxxxxx'\nAn error has occurred: InvalidConfigError:\n==> File .pre-commit-config.yaml\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\nSolution description\nSet uft-8 encoding when creating the pre-commit yaml file:\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\nAdded by MarcosMJD",
        "section": "Module 6: Best practices",
        "question": "In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file"
      },
      {
        "text": "Problem description\ngit commit -m 'Updated xxxxxx'\n[INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\n[INFO] Once installed this environment will be reused.\nAn unexpected error has occurred: CalledProcessError: command:\n\u2026\nreturn code: 1\nexpected return code: 0\nstdout:\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\nSolution description\nClear app-data of the virtualenv\npython -m virtualenv api -vvv --reset-app-data\nAdded by MarcosMJD",
        "section": "Module 6: Best practices",
        "question": "Git commit with pre-commit hook raises error \u2018'PythonInfo' object has no attribute 'version_nodot'"
      },
      {
        "text": "Problem description\nProject structure:\n/sources/production/model_service.py\n/sources/tests/unit_tests/test_model_service.py (\u201cfrom production.model_service import ModelService)\nWhen running python test_model_service.py from the sources directory, it works.\nWhen running pytest ./test/unit_tests fails. \u2018No module named \u2018production\u2019\u2019\nSolution description\nUse python -m pytest ./test/unit_tests\nExplanation: pytest does not add to the sys.path the path where pytest is run.\nYou can run python -m pytest, or alternatively export PYTHONPATH=. Before executing pytest\nAdded by MarcosMJD",
        "section": "Module 6: Best practices",
        "question": "Pytest error \u2018module not found\u2019 when if using custom packages in the source code"
      },
      {
        "text": "Problem description\nProject structure:\n/sources/production/model_service.py\n/sources/tests/unit_tests/test_model_service.py (\u201cfrom production.model_service import ModelService)\ngit commit -t \u2018test\u2019 raises \u2018No module named \u2018production\u2019\u2019 when calling pytest hook\n- repo: local\nhooks:\n- id: pytest-check\nname: pytest-check\nentry: pytest\nlanguage: system\npass_filenames: false\nalways_run: true\nargs: [\n\"tests/\"\n]\nSolution description\nUse this hook instead:\n- repo: local\nhooks:\n- id: pytest-check\nname: pytest-check\nentry: \"./sources/tests/unit_tests/run.sh\"\nlanguage: system\ntypes: [python]\npass_filenames: false\nalways_run: true\nAnd make sure that run.sh sets the right directory and run pytest:\ncd \"$(dirname \"$0\")\"\ncd ../..\nexport PYTHONPATH=.\npipenv run pytest ./tests/unit_tests\nAdded by MarcosMJD",
        "section": "Module 6: Best practices",
        "question": "Pytest error \u2018module not found\u2019 when using pre-commit hooks if using custom packages in the source code"
      },
      {
        "text": "Problem description\nThis is the step in the ci yml file definition:\n- name: Run Unit Tests\nworking-directory: \"sources\"\nrun: ./tests/unit_tests/run.sh\nWhen executing github ci action, error raises:\n\u2026/tests/unit_test/run.sh Permission error\nError: Process completed with error code 126\nSolution description\nAdd execution  permission to the script and commit+push:\ngit update-index --chmod=+x .\\sources\\tests\\unit_tests\\run.sh\nAdded by MarcosMJD",
        "section": "Module 6: Best practices",
        "question": "Github actions: Permission denied error when executing script file"
      },
      {
        "text": "Problem description\nWhen a docker-compose file contains a lot of containers, running the containers may take too much resource. There is a need to easily select only a group of containers while ignoring irrelevant containers during testing.\nSolution description\nAdd profiles: [\u201cprofile_name\u201d] in the service definition.\nWhen starting up the service, add `--profile profile_name` in the command.\nAdded by Ammar Chalifah",
        "section": "Module 6: Best practices",
        "question": "Managing Multiple Docker Containers with docker-compose profile"
      },
      {
        "text": "If you encounter such messages when you try to list your aws s3 buckets for example (aws --endpoint-url=http://localhost:4566 s3 ls), you can try to configure AWS by setting up the same region, access key and secret key as the ones that appear in your docker-compose file.\nAfter installing the aws cli, make sure you configure it in your terminal by entering this command line : aws configure\nIt will ask for:\nAWS Access Key ID [None]: abc (example)\nAWS Secret Access Key [None]: xyz (example)\nDefault region name [None]: eu-west-1 (example)\nAdded by M\u00e9lanie Fouesnard",
        "section": "Module 6: Best practices",
        "question": "Why do aws cli commands throw <botocore.awsrequest.AWSRequest object at 0x74c89c3562d0> type messages when listing or creating aws s3 buckets with localstack ?"
      },
      {
        "text": "Problem description\nIf you are having problems with the integration tests and kinesis double check that your aws regions match on the docker-compose and local config. Otherwise you will be creating a stream in the wrong region\nSolution description\nFor example set ~/.aws/config region = us-east-1 and the docker-compose.yaml - AWS_DEFAULT_REGION=us-east-1\nAdded by Quinn Avila",
        "section": "Module 6: Best practices",
        "question": "AWS regions need to match docker-compose"
      },
      {
        "text": "Problem description\nPre-commit command was failing with isort repo.\nSolution description\nSet version to 5.12.0\nAdded by Erick Calderin",
        "section": "Module 6: Best practices",
        "question": "Isort Pre-commit"
      },
      {
        "text": "Problem description\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\nSolution description\nFrom local:\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\nterraform destroy --var-file vars/prod.tfvars\nAdded by Erick Calderin",
        "section": "Module 6: Best practices",
        "question": "How to destroy infrastructure created via GitHub Actions"
      },
      {
        "text": "After installing awscliv2 in linux you can get this error every time you try to run an aws command that needs to use the credentials. For example, if you run aws configure, you can insert the key and secret but finally you receive the error message.\nThe user ubuntu does not have permission to read/write files in .aws folder and neither credentials and config files exists. What I have done to solve:\nGo to .aws folder, usually /home/ubuntu/.aws\nCreate an empty credentials and config files:\ntouch credentials\ntouch config\nModify the permissions:\nsudo chmod -R 777 credentials\nsudo chmod -R 777 config\nNow, you can run aws configure\nRun aws configure, modify the keys and secret and save them to the credentials file. And then you can execute your aws commands from your python scripts or in the command line.\nAdded by Eduardo Mu\u00f1oz",
        "section": "Module 6: Best practices",
        "question": "Error \u201c[Errno 13] Permission denied: '/home/ubuntu/.aws/credentials\u2019\u201d when running any aws command"
      },
      {
        "text": "Answer: Boto3 does not support underscores (_) in service URLs. Naming your Docker Compose services with underscores will cause Boto3 to throw an error when connecting to the endpoint. (Source: https://github.com/boto/boto3/issues/703)\n# Incorrect Docker Compose configuration with underscores\nversion: '3.8'\nservices:\nbackend_service:\nimage: my_backend_image\n...\ns3_service:\nimage: localstack/localstack\n\u2026\nRename your services to avoid using underscores. s3_service \u2192 s3service\nThat way, when you run client = boto3.client('s3', endpoint_url=\"http://s3service:4566\") you won\u2019t get any error.\nAdded by Fustincho\nProblem: Pre-commit fails with error RuntimeError: The Poetry configuration is invalid:\n- data.extras.pipfile_deprecated_finder[2] must match pattern ^[a-zA-Z-_.0-9]+$\nSolution: This is caused by version mixmatch between the pre-commit-config.yaml designated version for your package and the actual versions. Check the versions in Pipfile.lock and update as appropriate.\nAdded by Oluwadara Adedeji",
        "section": "Module 6: Best practices",
        "question": "Why do I get a ValueError: Invalid endpoint error when using Boto3 with Docker Compose services?"
      },
      {
        "text": "Solution: Follow the tip: When you compare two Pandas DataFrames, the result is also a DataFrame. The same is true for Pandas Series. Also, a DataFrame could be turned into a list of dictionaries.\nTherefore, do not compare data frames directly, but convert the actual and expected dataframes into list of dictionaries and then use assert to compare the resulting list of dictionaries.\nFor example:\n\u2026\nactual_df_list_dicts = actual_df.to_dict('records')\n\u2026\nexpected_df_list_dicts = expected_df.to_dict('records')\n\u2026\nassert actual_df_list_dicts == expected_df_list_dicts\nAdded by Victor Emenike",
        "section": "Capstone Project",
        "question": "Why do I get a \u201cValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()\u201d error when doing unit test that involves comparing two data frames?"
      },
      {
        "text": "No, the capstone is a solo project.",
        "section": "Capstone Project",
        "question": "Is it a group project?"
      },
      {
        "text": "You only need to submit 1 project. \nIf the submission at the first attempt fails, you can improve it and re-submit during attempt#2 submission window.\nIf you want to submit 2 projects for the experience and exposure, you must use different datasets and problem statements.\nIf you can\u2019t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of 3 peers in your cohort",
        "section": "Capstone Project",
        "question": "Do we submit 2 projects, what does attempt 1 and 2 mean?"
      },
      {
        "text": "Each submitted project will be evaluated by 3 (three) randomly assigned students who have also submitted the project.\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here (TBA for link).",
        "section": "Capstone Project",
        "question": "How is my capstone project going to be evaluated?"
      },
      {
        "text": "What is the scoring given for homework\nEach Homework has 6 question on correct answering you can get 6 points\nYou can also add 7 public learning which you can get 7 points\nAdding 1 Question valid in FAQ gives 1 point\nSo in total you can get 14 point per homework which will help you in leaderboard\n(optional) Added by Name",
        "section": "Capstone Project",
        "question": "What is the criteria of scoring home work?"
      },
      {
        "text": "Finish the Capstone project\nAdded by Shashank Kumar",
        "section": "Certificates:",
        "question": "Criteria for getting a certificate?"
      },
      {
        "text": "No\nAdded by Shashank Kumar",
        "section": "Certificates:",
        "question": "necessary for a certificate?"
      }
    ]
  }
]