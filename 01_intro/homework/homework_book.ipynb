{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf05177-991c-4a4a-92c6-e7fc7d7c2b12",
   "metadata": {},
   "source": [
    "## Homework: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024bcab-2672-46c4-b4f7-8eb0166fe3f1",
   "metadata": {},
   "source": [
    "In this homework, we'll learn more about search and use Elastic Search for practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e4653-64b7-4423-940c-111130866919",
   "metadata": {},
   "source": [
    "## Q1. Running Elastic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f6178-8c84-421f-b5b8-9ee83f461abc",
   "metadata": {},
   "source": [
    "Run Elastic Search 8.4.3, and get the cluster information. If you run it on localhost, this is how you do it:\n",
    "\n",
    "- curl localhost:9200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090ab31-64b6-4fe2-a8ec-6d24ae876cd4",
   "metadata": {},
   "source": [
    "What's the version.build_hash value?\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "831c0c58-d93c-4ac7-bbd0-543ef83e3d78.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "cd556f95-18e0-45a1-aeb3-da2773a5a1d3",
   "metadata": {},
   "source": [
    "![image.png](attachment:831c0c58-d93c-4ac7-bbd0-543ef83e3d78.png)m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec06c7b-9369-4715-8cdf-be5ac4be5be8",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "- 42f05b9372a9a4a470db3b52817899b99a76ee73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cced9ad5-2e0d-47ef-9532-951b3d6278c8",
   "metadata": {},
   "source": [
    "## Getting the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ddcc8-9c31-4c61-9f6c-506c30052cad",
   "metadata": {},
   "source": [
    "Now let's get the FAQ data. You can run this snippet:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25ce7103-dfa8-4acc-9d92-462cdbd30d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03ef8563-ab0e-4a3f-ade8-adc3bb04d991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'course': 'data-engineering-zoomcamp',\n",
       " 'documents': [{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDont forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - When will the course start?'},\n",
       "  {'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - What are the prerequisites for this course?'},\n",
       "  {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - Can I still join the course after the start date?'},\n",
       "  {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?'},\n",
       "  {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - What can I do before the course starts?'},\n",
       "  {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp live cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If youre not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any live cohort.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - how many Zoomcamps in a year?'},\n",
       "  {'text': 'Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - Is the current cohort going to be different from the previous cohort?'},\n",
       "  {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - Can I follow the course after it finishes?'},\n",
       "  {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but dont rely on its answers 100%, it is pretty good though.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - Can I get support if I take the course in the self-paced mode?'},\n",
       "  {'text': 'All the main videos are stored in the Main DATA ENGINEERING playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - Which playlist on YouTube should I refer to?'},\n",
       "  {'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?'},\n",
       "  {'text': \"No, you can only get a certificate if you finish the course with a live cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?'},\n",
       "  {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDont post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Office Hours - What is the video/zoom link to the stream for the Office Hour or workshop sessions?'},\n",
       "  {'text': 'Yes! Every Office Hours will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Office Hours - I cant attend the Office hours / workshop, will it be recorded?'},\n",
       "  {'text': 'You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Homework - What are homework and project deadlines?'},\n",
       "  {'text': 'No, late submissions are not allowed. But if the form is still not closed and its after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\nOlder news:[source1] [source2]',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Homework - Are late submissions of homework allowed?'},\n",
       "  {'text': 'Answer: In short, its your repository on github, gitlab, bitbucket, etc\\nIn long, your repository or any other location you have your code where a reasonable person would look at it and think yes, you went through the week and exercises.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Homework - What is the homework URL in the homework link?'},\n",
       "  {'text': 'After you submit your homework it will be graded based on the amount of questions in a particular homework. You can see how many points you have right on the page of the homework up top. Additionally in the leaderboard you will find the sum of all points youve earned - points for Homeworks, FAQs and Learning in Public. If homework is clear, others work as follows: if you submit something to FAQ, you get one point, for each learning in a public link you get one point.\\n(https://datatalks-club.slack.com/archives/C01FABYF2RG/p1706846846359379?thread_ts=1706825019.546229&cid=C01FABYF2RG)',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Homework and Leaderboard - what is the system for points in the course management platform?'},\n",
       "  {'text': 'When you set up your account you are automatically assigned a random name such as Lucid Elbakyan for example. If you want to see what your Display name is.\\nGo to the Homework submission link   https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on Data Engineering Zoom Camp 2024 > click on Edit Course Profile - your display name is here, you can also change it should you wish:',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?'},\n",
       "  {'text': 'Yes, for simplicity (of troubleshooting against the recorded videos) and stability. [source]\\nBut Python 3.10 and 3.11 should work fine.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Is Python 3.9 still the recommended version to use in 2024?'},\n",
       "  {'text': 'You can set it up on your laptop or PC if you prefer to work locally from your laptop or PC.\\nYou might face some challenges, especially for Windows users. If you face cnd2\\nIf you prefer to work on the local machine, you may start with the week 1 Introduction to Docker and follow through.\\nHowever, if you prefer to set up a virtual machine, you may start with these first:\\nUsing GitHub Codespaces\\nSetting up the environment on a cloudV Mcodespace\\nI decided to work on a virtual machine because I have different laptops & PCs for my home & office, so I can work on this boot camp virtually anywhere.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?'},\n",
       "  {'text': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\nYou can also open any GitHub repository in a GitHub Codespace.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?'},\n",
       "  {'text': \"It's up to you which platform and environment you use for the course.\\nGithub codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.'},\n",
       "  {'text': 'Choose the approach that aligns the most with your idea for the end project\\nOne of those should suffice. However, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Or you can set up a local environment for most of this course.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Do I need both GitHub Codespaces and GCP?'},\n",
       "  {'text': '1. To open Run command window, you can either:\\n(1-1) Use the shortcut keys: \\'Windows + R\\', or\\n(1-2) Right Click \"Start\", and click \"Run\" to open.\\n2. Registry Values Located in Registry Editor, to open it: Type \\'regedit\\' in the Run command window, and then press Enter.\\' 3. Now you can change the registry values \"Autorun\" in \"HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor\" from \"if exists\" to a blank.\\nAlternatively, You can simplify the solution by deleting the fingerprint saved within the known_hosts file. In Windows, this file is placed at  C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_host',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor'},\n",
       "  {'text': 'For uniformity at least, but youre not restricted to GCP, you can use other cloud platforms like AWS if youre comfortable with other cloud platforms, since you get every service thats been provided by GCP in Azure and AWS or others..\\nBecause everyone has a google account, GCP has a free trial period and gives $300 in credits  to new users. Also, we are working with BigQuery, which is a part of GCP.\\nNote that to sign up for a free GCP account, you must have a valid credit card.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Why are we using GCP and not other cloud providers?'},\n",
       "  {'text': 'No, if you use GCP and take advantage of their free trial.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Should I pay for cloud services?'},\n",
       "  {'text': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We wont be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\nFor everything in the course, theres a local alternative. You could even do the whole course locally.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?'},\n",
       "  {'text': 'Yes, you can. Just remember to adapt all the information on the videos to AWS. Besides, the final capstone will be evaluated based on the task: Create a data pipeline! Develop a visualisation!\\nThe problem would be when you need help. Youd need to rely on  fellow coursemates who also use AWS (or have experience using it before), which might be in smaller numbers than those learning the course with GCP.\\nAlso see Is it possible to use x tool instead of the one tool you use?',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - I want to use AWS. May I do that?'},\n",
       "  {'text': 'We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Besides the Office Hour which are the live zoom calls?'},\n",
       "  {'text': 'We will use the same data, as the project will essentially remain the same as last years. The data is available here',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Are we still using the NYC Trip data for January 2021? Or are we using the 2022 data?'},\n",
       "  {'text': 'No, but we moved the 2022 stuff here',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Is the 2022 repo deleted?'},\n",
       "  {'text': 'Yes, you can use any tool you want for your project.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Can I use Airflow instead for my final project?'},\n",
       "  {'text': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we cant support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Is it possible to use tool X instead of the one tool you use in the course?'},\n",
       "  {'text': 'Star the repo! Share it with friends if you find it useful \\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'How can we contribute to the course?'},\n",
       "  {'text': 'Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Is the course [Windows/mac/Linux/...] friendly?'},\n",
       "  {'text': \"Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\\nLater modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Environment - Roadblock for Windows users in modules with *.sh (shell scripts).'},\n",
       "  {'text': 'Yes to both! check out this document: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Any books or additional resources you recommend?'},\n",
       "  {'text': 'You will have two attempts for a project. If the first project deadline is over and youre late or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Project - What is Project Attemp #1 and Project Attempt #2 exactly?'},\n",
       "  {'text': \"The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\\nWhat does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\\nRestart app or server/pc.\\nGoogle it, use ChatGPT, Bing AI etc.\\nIt is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\\nSearch using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\\nThere are often different solutions for the same problem due to variation in environments.\\nCheck the techs documentation. Use its search if available or use the browsers search function.\\nTry uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\\nSometimes reinstalling fails to resolve the issue but works if you uninstall first.\\nPost your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\\nhttps://stackoverflow.com/help/how-to-ask\\nThis will be your real life. Ask an expert in the future (in addition to coworkers).\\nAsk in Slack\\nBefore asking a question,\\nCheck Pins (where the shortcut to the repo and this FAQ is located)\\nUse the slack apps search function\\nUse the bot @ZoomcampQABot to do the search for you\\ncheck the FAQ (this document), use search [ctrl+f]\\nWhen asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the offending code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\\nDO NOT use screenshots, especially dont take pictures from a phone.\\nDO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if its long, just post it in a reply to your thread.\\nUse ``` for formatting your code.\\nUse the same thread for the conversation (that means reply to your own thread).\\nDO NOT create multiple posts to discuss the issue.\\nlearYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\\nProvide additional information in the same thread of the steps you have taken for resolution.\\nTake a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\\nRemember technology issues in real life sometimes take days or even weeks to resolve.\\nIf somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'How to troubleshoot issues'},\n",
       "  {'text': 'When the troubleshooting guide above does not help resolve it and you need another pair of eyeballs to spot mistakes. When asking a question, include as much information as possible:\\nWhat are you coding on? What OS?\\nWhat command did you run, which video did you follow? Etc etc\\nWhat error did you get? Does it have a line number to the offending code and have you check it for typos?\\nWhat have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'How to ask questions'},\n",
       "  {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'How do I use Git / GitHub for this course?'},\n",
       "  {'text': 'Error: Makefile:2: *** missing separator.  Stop.\\nSolution: Tabs in document should be converted to Tab instead of spaces. Follow this stack.',\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'VS Code: Tab using spaces'},\n",
       "  {'text': \"If youre running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with whatever Internet Browser you have installed on the host (Windows). Just install wslu and open the page with wslview <file>, for example:\\nwslview index.html\\nYou can customise which browser to use by setting the BROWSER environment variable first. For example:\\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\",\n",
       "   'section': 'General course-related questions',\n",
       "   'question': 'Opening an HTML file with a Windows browser from Linux running on WSL'},\n",
       "  {'text': 'This tutorial shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.\\nTaxi Data - Yellow Taxi Trip Records downloading error, Error no or XML error webpage\\nWhen you try to download the 2021 data from TLC website, you get this error:\\nIf you click on the link, and ERROR 403: Forbidden on the terminal.\\nWe have a backup, so use it instead: https://github.com/DataTalksClub/nyc-tlc-data\\nSo the link should be https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\\nNote: Make sure to unzip the gz file (no, the unzip command wont work for this.)\\ngzip -d file.gzg',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine'},\n",
       "  {'text': 'In this video, we store the data file as output.csv. The data file wont store correctly if the file extension is csv.gz instead of csv. One alternative is to replace csv_name = output.cs -v with the file name given at the end of the URL. Notice that the URL for the yellow taxi data is: https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz where the highlighted part is the name of the file. We can parse this file name from the URL and use it as csv_name. That is, we can replace csv_name = output.csv with\\ncsv_name = url.split(/)[-1] . Then when we use csv_name to using pd.read_csv, there wont be an issue even though the file name really has the extension csv.gz instead of csv since the pandas read_csv function can read csv.gz files directly.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?'},\n",
       "  {'text': 'Yellow Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\\nGreen Trips: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Taxi Data - Data Dictionary for NY Taxi data?'},\n",
       "  {'text': 'You can unzip this downloaded parquet file, in the command line. The result is a csv file which can be imported with pandas using the pd.read_csv() shown in the videos.\\ngunzip green_tripdata_2019-09.csv.gz\\nSOLUTION TO USING PARQUET FILES DIRECTLY IN PYTHON SCRIPT ingest_data.py\\nIn the def main(params) add this line\\nparquet_name= \\'output.parquet\\'\\nThen edit the code which downloads the files\\nos.system(f\"wget {url} -O {parquet_name}\")\\nConvert the download .parquet file to csv and rename as csv_name to keep it relevant to the rest of the code\\ndf = pd.read_parquet(parquet_name)\\ndf.to_csv(csv_name, index=False)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Taxi Data - Unzip Parquet file'},\n",
       "  {'text': 'wget is not recognized as an internal or external command, you need to install it.\\nOn Ubuntu, run:\\n$ sudo apt-get install wget\\nOn MacOS, the easiest way to install wget is to use Brew:\\n$ brew install wget\\nOn Windows, the easiest way to install wget is to use Chocolatey:\\n$ choco install wget\\nOr you can download a binary (https://gnuwin32.sourceforge.net/packages/wget.htm) and put it to any location in your PATH (e.g. C:/tools/)\\nAlso, you can following this step to install Wget on MS Windows\\n* Download the latest wget binary for windows from [eternallybored] (https://eternallybored.org/misc/wget/) (they are available as a zip with documentation, or just an exe)\\n* If you downloaded the zip, extract all (if windows built in zip utility gives an error, use [7-zip] (https://7-zip.org/)).\\n* Rename the file `wget64.exe` to `wget.exe` if necessary.\\n* Move wget.exe to your `Git\\\\mingw64\\\\bin\\\\`.\\nAlternatively, you can use a Python wget library, but instead of simply using wget youll need to use\\npython -m wget\\nYou need to install it with pip first:\\npip install wget\\nAlternatively, you can just paste the file URL into your web browser and download the file normally that way. Youll want to move the resulting file into your working directory.\\nAlso recommended a look at the python library requests for the loading gz file  https://pypi.org/project/requests',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'lwget is not recognized as an internal or external command'},\n",
       "  {'text': 'Firstly, make sure that you add ! before wget if youre running your command in a Jupyter Notebook or CLI. Then, you can check one of this 2 things (from CLI):\\nUsing the Python library wget you installed with pip, try python -m wget <url>\\nWrite the usual command and add --no-check-certificate at the end. So it should be:\\n!wget <website_url> --no-check-certificate',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)'},\n",
       "  {'text': 'For those who wish to use the backslash as an escape character in Git Bash for Windows (as Alexey normally does), type in the terminal: bash.escapeChar=\\\\ (no need to include in .bashrc)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Git Bash - Backslash as an escape character in Git Bash for Windows'},\n",
       "  {'text': 'Instruction on how to store secrets that will be avialable in GitHub  Codespaces.\\nManaging your account-specific secrets for GitHub Codespaces - GitHub Docs',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GitHub Codespaces - How to store secrets'},\n",
       "  {'text': \"Make sure you're able to start the Docker daemon, and check the issue immediately down below:\\nAnd dont forget to update the wsl in powershell the  command is wsl update\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?'},\n",
       "  {'text': \"As the official Docker for Windows documentation says, the Docker engine can either use the\\nHyper-V or WSL2 as its backend. However, a few constraints might apply\\nWindows 10 Pro / 11 Pro Users: \\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nWindows 10 Home / 11 Home Users: \\nOn the other hand, Users of the 'Home' version do NOT have the option Hyper-V option enabled, which means, you can only get Docker up and running using the WSL2 credentials(Windows Subsystem for Linux). Url\\nYou can find the detailed instructions to do so here: rt ghttps://pureinfotech.com/install-wsl-windows-11/\\nIn case, you run into another issue while trying to install WSL2 (WslRegisterDistribution failed with error: 0x800701bc), Make sure you update the WSL2 Linux Kernel, following the guidelines here: \\n\\nhttps://github.com/microsoft/WSL/issues/5393\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified'},\n",
       "  {'text': 'Whenever a `docker pull is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name (pgadmin4, for the example above) from a repository (dbpage).\\nIF the repository is public, the fetch and download happens without any issue whatsoever.\\nFor instance:\\ndocker pull postgres:13\\ndocker pull dpage/pgadmin4\\nBE ADVISED:\\n\\nThe Docker Images we\\'ll be using throughout the Data Engineering Zoomcamp are all public (except when or if explicitly said otherwise by the instructors or co-instructors).\\n\\nMeaning: you are NOT required to perform a docker login to fetch them. \\n\\nSo if you get the message above saying \"docker login\\': denied: requested access to the resource is denied. That is most likely due to a typo in your image name:\\n\\nFor instance:\\n$ docker pull dbpage/pgadmin4\\nWill throw that exception telling you \"repository does not exist or may require \\'docker login\\'\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or \\nmay require \\'docker login\\': denied: requested access to the resource is denied\\nBut that actually happened because the actual image is dpage/pgadmin4 and NOT dbpage/pgadmin4\\nHow to fix it:\\n$ docker pull dpage/pgadmin4\\nEXTRA NOTES:\\nIn the real world, occasionally, when you\\'re working for a company or closed organisation, the Docker image you\\'re trying to fetch might be under a private repo that your DockerHub Username was granted access to.\\nFor which cases, you must first execute:\\n$ docker login\\nFill in the details of your username and password.\\nAnd only then perform the `docker pull` against that private repository\\nWhy am I encountering a \"permission denied\" error when creating a PostgreSQL Docker container for the New York Taxi Database with a mounted volume on macOS M1?\\nIssue Description:\\nWhen attempting to run a Docker command similar to the one below:\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\mount\\npostgres:13\\nYou encounter the error message:\\ndocker: Error response from daemon: error while creating mount source path \\'/path/to/ny_taxi_postgres_data\\': chown /path/to/ny_taxi_postgres_data: permission denied.\\nSolution:\\n1- Stop Rancher Desktop:\\nIf you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\\n2- Install Docker Desktop:\\nInstall Docker Desktop, ensuring that it is properly configured and has the required permissions.\\n2-Retry Docker Command:\\nRun the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\\nNote: The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - docker pull dbpage'},\n",
       "  {'text': 'When I runned command to create postgre in docker container it created folder on my local machine to mount it to volume inside container. It has write and read protection and owned by user 999, so I could not delete it by simply drag to trash.  My obsidian could not started due to access error, so I had to change placement of this folder and delete old folder by this command:\\nsudo rm -r -f docker_test/\\n- where `rm` - remove, `-r` - recursively, `-f` - force, `docker_test/` - folder.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - cant delete local folder that mounted to docker volume'},\n",
       "  {'text': 'First off, make sure you\\'re running the latest version of Docker for Windows, which you can download from here. Sometimes using the menu to \"Upgrade\" doesn\\'t work (which is another clear indicator for you to uninstall, and reinstall with the latest version)\\nIf docker is stuck on starting, first try to switch containers by right clicking the docker symbol from the running programs and switch the containers from windows to linux or vice versa\\n[Windows 10 / 11 Pro Edition] The Pro Edition of Windows can run Docker either by using Hyper-V or WSL2 as its backend (Docker Engine)\\nIn order to use Hyper-V as its back-end, you MUST have it enabled first, which you can do by following the tutorial: Enable Hyper-V Option on Windows 10 / 11\\nIf you opt-in for WSL2, you can follow the same steps as detailed in the tutorial here',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\"},\n",
       "  {'text': \"It is recommended by the Docker do\\n[Windows 10 / 11 Home Edition] If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the tutorial here\\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the option to Reset to Factory Defaults or do a fresh install.\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?'},\n",
       "  {'text': 'More info in the Docker Docs on Best Practises',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).'},\n",
       "  {'text': 'You may have this error:\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\nerror:\\nSolution:\\nUse winpty before docker command (source)\\n$ winpty docker run -it ubuntu bash\\nYou also can make an alias:\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\nOR\\necho \"alias docker=\\'winpty docker\\'\" >> ~/.bash_profile',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - The input device is not a TTY (Docker run for Windows)'},\n",
       "  {'text': \"You may have this error:\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.u\\nrllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')':\\n/simple/pandas/\\nPossible solution might be:\\n$ winpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Cannot pip install on Docker container (Windows)'},\n",
       "  {'text': 'Even after properly running the docker script the folder is empty in the vs code  then try this (For Windows)\\nwinpty docker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nHere quoting the absolute path in  the -v parameter is solving the issue and all the files are visible in the Vs-code ny_taxi folder as shown in the video',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - ny_taxi_postgres_data is empty'},\n",
       "  {'text': 'Check this article for details - Setting up docker in macOS\\nFrom researching it seems this method might be out of date, it seems that since docker changed their licensing model, the above is a bit hit and miss. What worked for me was to just go to the docker website and download their dmg. Havent had an issue with that method.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'dasDocker - Setting up Docker on Mac'},\n",
       "  {'text': '$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"admin\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \"postgres\".\\nThis use The database cluster will be initialized with locale \"en_US.utf8\".\\nThe default databerrorase encoding has accordingly been set to \"UTF8\".\\nxt search configuration will be set to \"english\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \"/var/lib/postgresql/data\" or run initdb\\nwitls',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': '1Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted'},\n",
       "  {'text': 'Mapping volumes on Windows could be tricky. The way it was done in the course video doesnt work for everyone.\\nFirst, if yo\\nmove your data to some folder without spaces. E.g. if your code is in C:/Users/Alexey Grigorev/git/, move it to C:/git/\\nTry replacing the -v part with one of the following options:\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\nwinpty docker run -it\\n-e POSTGRES_USER=\"root\"\\n-e POSTGRES_PASSWORD=\"root\"\\n-e POSTGRES_DB=\"ny_taxi\"\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-p 5432:5432\\npostgres:1\\nTry adding winpty before the whole command\\n3\\nwin\\nTry adding quotes:\\n-v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n-v \"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\nNote:  (Window) if it automatically creates a folder called ny_taxi_postgres_data;C suggests you have problems with volume mapping, try deleting both folders and replacing -v part with other options. For me //c/ works instead of /c/. And it will work by automatically creating a correct folder called ny_taxi_postgres_data.\\nA possible solution to this error would be to use /$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes position varying as in the above list).\\nYes for windows use the command it works perfectly fine\\n-v /$(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data\\nImportant: note how the quotes are placed.\\nIf none of these options work, you can use a volume name instead of the path:\\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\nPostgres:13\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)'},\n",
       "  {'text': 'Change the mounting path. Replace it with one of following:\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\\\ (leading slash in front of c:)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.'},\n",
       "  {'text': 'When you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-v <your path>:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nThe error message above could happen. That means you should not mount on the second run. This command helped me:\\nWhen you run this command second time\\ndocker run -it \\\\\\n-e POSTGRES_USER=\"root\" \\\\\\n-e POSTGRES_PASSWORD=\"root\" \\\\\\n-e POSTGRES_DB=\"ny_taxi\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\"},\n",
       "  {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldnt access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\"},\n",
       "  {'text': 'You might have installed docker via snap. Run sudo snap status docker to verify.\\nIf you have error: unknown command \"status\", see \\'snap help\\'. as a response than deinstall docker and install via the official website\\nBind for 0.0.0.0:5432 failed: port is a',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - ERRO[0000] error waiting for container: context canceled'},\n",
       "  {'text': 'Found the issue in the PopOS linux. It happened because our user didnt have authorization rights to the host folder ( which also caused folder seems empty, but it didnt!).\\nSolution:\\nJust add permission for everyone to the corresponding folder\\nsudo chmod -R 777 <path_to_folder>\\nExample:\\nsudo chmod -R 777 ny_taxi_postgres_data/',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - build error checking context: cant stat /home/fhrzn/Projects/./ny_taxi_postgres_data'},\n",
       "  {'text': 'This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\\n$ docker build -t taxi_ingest:v001 .\\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\nOr use 777 if you still see problems. 755 grants write access to only the owner.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.'},\n",
       "  {'text': 'Get the network name via: $ docker network ls.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Docker network name'},\n",
       "  {'text': 'Sometimes, when you try to restart a docker image configured with a network name, the above message appears. In this case, use the following command with the appropriate container name:\\n>>> If the container is running state, use docker stop <container_name>\\n>>> then, docker rm pg-database\\nOr use docker start instead of docker run in order to restart the docker image without removing it.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container xxx.  You have to remove (or rename) that container to be able to reuse that name.'},\n",
       "  {'text': 'Typical error: sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\\nWhen running docker-compose up -d see which network is created and use this for the ingestions script instead of pg-network and see the name of the database to use instead of pgdatabase\\nE.g.:\\npg-network becomes 2docker_default\\nPgdatabase becomes 2docker-pgdatabase-1',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - ingestion when using docker-compose could not translate host name'},\n",
       "  {'text': 'terraformRun this command before starting your VM:\\nOn Intel CPU:\\nmodprobe -r kvm_intel\\nmodprobe kvm_intel nested=1\\nOn AMD CPU:\\nmodprobe -r kvm_amd\\nmodprobe kvm_amd nested=1',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).'},\n",
       "  {'text': 'Its very easy to manage your docker container, images, network and compose projects from VS Code.\\nJust install the official extension and launch it from the left side icon.\\nIt will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.\\nDocker - How to stop a container?\\nUse the following command:\\n$ docker stop <container_id>',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - Connecting from VS Code'},\n",
       "  {'text': \"When you see this in logs, your container with postgres is not accepting any requests, so if you attempt to connect, you'll get this error:\\nconnection failed: server closed the connection unexpectedly\\nThis probably means the server terminated abnormally before or while processing the request.\\nIn this case, you need to delete the directory with data (the one you map to the container with the -v flag) and restart the container.\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down'},\n",
       "  {'text': 'On few versions of Ubuntu, snap command can be used to install Docker.\\nsudo snap install docker',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker not installable on Ubuntu'},\n",
       "  {'text': 'error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named docker_sql_dtc_postgres_volume_local whereas it should have used the already existing dtc_postgres_volume_local\\nAll i did to fix this is that I renamed the existing dtc_postgres_volume_local to docker_sql_dtc_postgres_volume_local and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - mounting error'},\n",
       "  {'text': 'Couldnt translate host name to address\\nMake sure postgres database is running.\\n\\n\\u200b\\u200bUse the command to start containers in detached mode: docker-compose up -d\\n(data-engineering-zoomcamp) hw % docker compose up -d\\n[+] Running 2/2\\n Container pg-admin     Started                                                                                                                                                                      0.6s\\n Container pg-database  Started\\nTo view the containers use: docker ps.\\n(data-engineering-zoomcamp) hw % docker ps\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"docker-entrypoint.s\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-admin\\nhw\\nTo view logs for a container: docker logs <containerid>\\n(data-engineering-zoomcamp) hw % docker logs faf05090972e\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in\\nprogress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28\\n] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\\nIf docker ps doesnt show pgdatabase running, run: docker ps -a\\nThis should show all containers, either running or stopped.\\nGet the container id for pgdatabase-1, and run',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Error translating host name to address'},\n",
       "  {'text': 'After executing `docker-compose up` - if you lose database data and are unable to successfully execute your Ingestion script (to re-populate your database) but receive the following error:\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\\nDocker compose is creating its own default network since it is no longer specified in a docker execution command or file. Docker Compose will emit to logs the new network name. See the logs after executing `docker compose up` to find the network name and change the network name argument in your Ingestion script.\\nIf problems persist with pgcli, we can use HeidiSQL,usql\\nKrishna Anand',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)'},\n",
       "  {'text': 'It returns --> Error response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\\nTry:\\ndocker ps -a to see all the stopped & running containers\\nd to nuke all the containers\\nTry: docker-compose up -d again ports\\nOn localhost:8080 server  Unable to connect to server: could not translate host name \\'pg-database\\' to address: Name does not resolve\\nTry: new host name, best without  -  e.g. pgdatabase\\nAnd on docker-compose.yml, should specify docker network & specify the same network in both  containers\\nservices:\\npgdatabase:\\nimage: postgres:13\\nenvironment:\\n- POSTGRES_USER=root\\n- POSTGRES_PASSWORD=root\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\nports:\\n- \"5431:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Hostname does not resolve'},\n",
       "  {'text': 'So one common issue is when you run docker-compose on GCP, postgres wont persist its data to mentioned path for example:\\nservices:\\n\\n\\npgadmin:\\n\\n\\nVolumes:\\n./pgadmin:/var/lib/pgadmin:wr\\nMight not work so in this use you can use Docker Volume to make it persist, by simply changing\\nservices:\\n\\n.\\npgadmin:\\n\\n\\nVolumes:\\npgadmin:/var/lib/pgadmin\\nvolumes:\\nPgadmin:',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Persist PGAdmin docker contents on GCP'},\n",
       "  {'text': 'The docker will keep on crashing continuously\\nNot working after restart\\ndocker engine stopped\\nAnd failed to fetch extensions pop ups will on screen non-stop\\nSolution :\\nTry checking if latest version of docker is installed / Try updating the docker\\nIf Problem still persist then final solution is to reinstall docker\\n(Just have to fetch images again else no issues)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker engine stopped_failed to fetch extensions'},\n",
       "  {'text': 'As per the lessons,\\nPersisting pgAdmin configuration (i.e. server name) is done by adding a volumes section:\\nservices:\\npgdatabase:\\n[...]\\npgadmin:\\nimage: dpage/pgadmin4\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=root\\nvolumes:\\n- \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\nports:\\n- \"8080:80\"\\nIn the example above, pgAdmin_data is a folder on the host machine, and /var/lib/pgadmin/sessions is the session settings folder in the pgAdmin container.\\nBefore running docker-compose up on the YAML file, we also need to give the pgAdmin container access to write to the pgAdmin_data folder. The container runs with a username called 5050 and user group 5050. The bash command to give access over the mounted volume is:\\nsudo chown -R 5050:5050 pgAdmin_data',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Persist PGAdmin configuration'},\n",
       "  {'text': 'This happens if you did not create the docker group and added your user. Follow these steps from the link:\\nguides/docker-without-sudo.md at main  sindresorhus/guides  GitHub\\nAnd then press ctrl+D to log-out and log-in again. pgAdmin: Maintain state so that it remembers your previous connection\\nIf you are tired of having to setup your database connection each time that you fire up the containers, all you have to do is create a volume for pgAdmin:\\nIn your docker-compose.yaml file, enter the following into your pgAdmin declaration:\\nvolumes:\\n- type: volume\\nsource: pgadmin_data\\ntarget: /var/lib/pgadmin\\nAlso add the following to the end of the file:ls\\nvolumes:\\nPgadmin_data:',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied'},\n",
       "  {'text': 'This is happen to me after following 1.4.1 video where we are installing docker compose in our Google Cloud VM. In my case, the docker-compose file downloaded from github named docker-compose-linux-x86_64 while it is more convenient to use docker-compose command instead. So just change the docker-compose-linux-x86_64 into docker-compose.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - docker-compose still not available after changing .bashrc'},\n",
       "  {'text': 'Installing pass via sudo apt install pass helped to solve the issue. More about this can be found here: https://github.com/moby/buildkit/issues/1078',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Error getting credentials after running docker-compose up -d'},\n",
       "  {'text': \"For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\\ncreate a new volume on docker (either using the command line or docker desktop app)\\nmake the following changes to your docker-compose.yml file (see attachment)\\nset low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\\nuse the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\\nOrder of execution:\\n(1) open terminal in 2_docker_sql folder and run docker compose up\\n(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\\n(3) open jupyter notebook and begin the data ingestion\\n(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup'},\n",
       "  {'text': 'Locate config.json file for docker (check your home directory; Users/username/.docker).\\nModify credsStore to credStore\\nSave and re-run',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``'},\n",
       "  {'text': 'To figure out which docker-compose you need to download from https://github.com/docker/compose/releases you can check your system with these commands:\\nuname -s  -> return Linux most likely\\nuname -m -> return \"flavor\"\\nOr try this command -\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Which docker-compose binary to use for WSL?'},\n",
       "  {'text': 'If you wrote the docker-compose.yaml file exactly like the video, you might run into an error like this:dev\\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\\nIn order to make it work, you need to include the volume in your docker-compose file. Just add the following:\\nvolumes:\\ndtc_postgres_volume_local:\\n(Make sure volumes are at the same level as services.)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker-Compose - Error undefined volume in Windows/WSL'},\n",
       "  {'text': 'Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the user: is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \"0:0\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \"pg-data:/var/lib/postgresql/data\"\\nports:\\n- \"5432:5432\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \"${UID}:${GID}\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \"pg-admin:/var/lib/pgadmin\"\\nports:\\n- \"8080:80\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'WSL Docker directory permissions error'},\n",
       "  {'text': 'Cause : If Running on git bash or vm in windows pgadmin doesnt work easily LIbraries like psycopg2 and libpq ar required still the error persists.\\nSolution- I use psql instead of pgadmin totally same\\nPip install psycopg2\\ndock',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Docker - If pgadmin is not working for Querying in Postgres Use PSQL'},\n",
       "  {'text': 'Cause:\\nIt happens because the apps are not updated. To be specific, search for any pending updates for Windows Terminal, WSL and Windows Security updates.\\nSolution\\nfor updating Windows terminal which worked for me:\\nGo to Microsoft Store.\\nGo to the library of apps installed in your system.\\nSearch for Windows terminal.\\nUpdate the app and restart your system to  see the changes.\\nFor updating the Windows security updates:\\nGo to Windows updates and check if there are any pending updates from Windows, especially security updates.\\nDo restart your system once the updates are downloaded and installed successfully.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'WSL - Insufficient system resources exist to complete the requested service.'},\n",
       "  {'text': 'Up restardoting the same issue appears. Happens out of the blue on windows.\\nSolution 1: Fixing DNS Issue (credit: reddit) this worked for me personally\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\\nRestart your computer and then enable it with the following\\nreg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\\nRestart your OS again. It should work.\\nSolution 2: right click on running Docker icon (next to clock) and chose \"Switch to Linux containers\"\\nbash: conda: command not found\\nDatabase is uninitialized and superuser password is not specified.\\nDatabase is uninitialized and superuser password is not specified.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.'},\n",
       "  {'text': 'Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isnt looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesnt work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'WSL - Permissions too open at Windows'},\n",
       "  {'text': 'Such as the issue above, WSL2 may not be referencing the correct .ssh/config path from Windows. You can create a config file at the home directory of WSL2.\\ncd ~\\nmkdir .ssh\\nCreate a config file in this new .ssh/ folder referencing this folder:\\nHostName [GPC VM external IP]\\nUser [username]\\nIdentityFile ~/.ssh/[private key]',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'WSL - Could not resolve host name'},\n",
       "  {'text': 'Change TO Socket\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused'},\n",
       "  {'text': 'probably some installation error, check out sy',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI --help error'},\n",
       "  {'text': 'In this section of the course, the 5432 port of pgsql is mapped to your computers 5432 port. Which means you can access the postgres database via pgcli directly from your computer.\\nSo No, you dont need to run it inside another container. Your local system will do.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - INKhould we run pgcli inside another docker container?'},\n",
       "  {'text': 'FATAL:  password authentication failed for user \"root\"\\nobservations: Below in bold do not forget the folder that was created ny_taxi_postgres_data\\nThis happens if you have a local Postgres installation in your computer. To mitigate this, use a different port, like 5431, when creating the docker container, as in: -p 5431: 5432\\nThen, we need to use this port when connecting to pgcli, as shown below:\\npgcli -h localhost -p 5431 -u root -d ny_taxi\\nThis will connect you to your postgres docker container, which is mapped to your hosts 5431 port (though you might choose any port of your liking as long as it is not occupied).\\nFor a more visual and detailed explanation, feel free to check the video 1.4.2 - Port Mapping and Networks in Docker\\nIf you want to debug: the following can help (on a MacOS)\\nTo find out if something is blocking your port (on a MacOS):\\nYou can use the lsof command to find out which application is using a specific port on your local machine. `lsof -i :5432`wi\\nOr list the running postgres services on your local machine with launchctl\\nTo unload the running service on your local machine (on a MacOS):\\nunload the launch agent for the PostgreSQL service, which will stop the service and free up the port  \\n`launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nthis one to start it again\\n`launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist`\\nChanging port from 5432:5432 to 5431:5432 helped me to avoid this error.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)'},\n",
       "  {'text': 'I get this error\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nTraceback (most recent call last):\\nFile \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\\nsys.exit(cli())\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\\nreturn self.main(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/sitYe-packages/click/core.py\", line\\n1053, in main\\nrv = self.invoke(ctx)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\\nreturn ctx.invoke(self.callback, **ctx.params)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\\nreturn __callback(*args, **kwargs)\\nFile \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\\nos.makedirs(config_dir)\\nFile \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\\nmkdir(name, mode)PermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'\\nMake sure you install pgcli without sudo.\\nThe recommended approach is to use conda/anaconda to make sure your system python is not affected.\\nIf conda install gets stuck at \"Solving environment\" try these alternatives: https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\"},\n",
       "  {'text': 'ImportError: no pq wrapper available.\\nAttempts made:\\n- couldn\\'t import \\\\dt\\nopg \\'c\\' implementation: No module named \\'psycopg_c\\'\\n- couldn\\'t import psycopg \\'binary\\' implementation: No module named \\'psycopg_binary\\'\\n- couldn\\'t import psycopg \\'python\\' implementation: libpq library not found\\nSolution:\\nFirst, make sure your Python is set to 3.9, at least.\\nAnd the reason for that is we have had cases of \\'psycopg2-binary\\' failing to install because of an old version of Python (3.7.3). \\n\\n0. You can check your current python version with: \\n$ python -V(the V must be capital)\\n1. Based on the previous output, if you\\'ve got a 3.9, skip to Step #2\\n   Otherwispye better off with a new environment with 3.9\\n$ conda create name de-zoomcamp python=3.9\\n$ conda activate de-zoomcamp\\n2. Next, you should be able to install the lib for postgres like this:\\n```\\n$ e\\n$ pip install psycopg2_binary\\n```\\n3. Finally, make sure you\\'re also installing pgcli, but use conda for that:\\n```\\n$ pgcli -h localhost -U root -d ny_taxisudo\\n```\\nThere, you should be good to go now!\\nAnother solution:\\nRun this\\npip install \"psycopg[binary,pool]\"',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - no pq wrapper available.'},\n",
       "  {'text': 'If your Bash prompt is stuck on the password command for postgres\\nUse winpty:\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\nAlternatively, try using Windows terminal or terminal in VS code.\\nEditPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nThe error above was faced continually despite inputting the correct password\\nSolution\\nOption 1: Stop the PostgreSQL service on Windows\\nOption 2 (using WSL): Completely uninstall Protgres 12 from Windows and install postgresql-client on WSL (sudo apt install postgresql-client-common postgresql-client libpq-dev)\\nOption 3: Change the port of the docker container\\nNEW SOLUTION: 27/01/2024\\nPGCLI -connection failed: FATAL:  password authentication failed for user \"root\"\\nIf youve got the error above, its probably because you were just like me, closed the connection to the Postgres:13 image in the previous step of the tutorial, which is\\n\\ndocker run -it \\\\\\n-e POSTGRES_USER=root \\\\\\n-e POSTGRES_PASSWORD=root \\\\\\n-e POSTGRES_DB=ny_taxi \\\\\\n-v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSo keep the database connected and you will be able to implement all the next steps of the tutorial.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI -  stuck on password prompt'},\n",
       "  {'text': 'Problem: If you have already installed pgcli but bash doesn\\'t recognize pgcli\\nOn Git bash: bash: pgcli: command not found\\nOn Windows Terminal: pgcli: The term \\'pgcli\\' is not recognized\\nSolution: Try adding a Python path C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to Windows PATH\\nFor details:\\nGet the location: pip list -v\\nCopy C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n3. Replace site-packages with Scripts: C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\nIt can also be that you have Python installed elsewhere.\\nFor me it was under c:\\\\python310\\\\lib\\\\site-packages\\nSo I had to add c:\\\\python310\\\\lib\\\\Scripts to PATH, as shown below.\\nPut the above path in \"Path\" (or \"PATH\") in System Variables\\nReference: https://stackoverflow.com/a/68233660',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - pgcli: command not found'},\n",
       "  {'text': 'In case running pgcli  locally causes issues or you do not want to install it locally you can use it running in a Docker container instead.\\nBelow the usage with values used in the videos of the course for:\\nnetwork name (docker network)\\npostgres related variables for pgcli\\nHostname\\nUsername\\nPort\\nDatabase name\\n$ docker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n175dd47cda07:/# pgcli -h pg-database -U root -p 5432 -d ny_taxi\\nPassword for root:\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: http://pgcli.com\\nroot@pg-database:ny_taxi> \\\\dt\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - running in a Docker container'},\n",
       "  {'text': 'PULocationID will not be recognized but PULocationID will be. This is because unquoted \"Localidentifiers are case insensitive. See docs.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - case sensitive use Quotations around columns with capital letters'},\n",
       "  {'text': 'When using the command `\\\\d <database name>` you get the error column `c.relhasoids does not exist`.\\nResolution:\\nUninstall pgcli\\nReinstall pgclidatabase \"ny_taxi\" does not exist\\nRestart pc',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'PGCLI - error column c.relhasoids does not exist'},\n",
       "  {'text': \"This happens while uploading data via the connection in jupyter notebook\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\nThe port 5432 was taken by another postgres. We are not connecting to the port in docker, but to the port on our machine. Substitute 5431 or whatever port you mapped to for port 5432.\\nAlso if this error is still persistent , kindly check if you have a service in windows running postgres , Stopping that service will resolve the issue\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\"'},\n",
       "  {'text': 'Can happen when connecting via pgcli\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\nOr while uploading data via the connection in jupyter notebook\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')\\nThis can happen when Postgres is already installed on your computer. Changing the port can resolve that (e.g. from 5432 to 5431).\\nTo check whether there even is a root user with the ability to login:\\nTry: docker exec -it <your_container_name> /bin/bash\\nAnd then run\\n???\\nAlso, you could change port from 5432:5432 to 5431:5432\\nOther solution that worked:\\nChanging `POSTGRES_USER=juroot` to `PGUSER=postgres`\\nBased on this: postgres with docker compose gives FATAL: role \"root\" does not exist error - Stack Overflow\\nAlso `docker compose down`, removing folder that had postgres volume, running `docker compose up` again.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist'},\n",
       "  {'text': '~\\\\anaconda3\\\\lib\\\\site-packages\\\\psycopg2\\\\__init__.py in connect(dsn, connection_factory, cursor_factory, **kwargs)\\n120\\n121     dsn = _ext.make_dsn(dsn, **kwargs)\\n--> 122     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\\n123     if cursor_factory is not None:\\n124         conn.cursor_factory = cursor_factory\\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\\nMake sure postgres is running. You can check that by running `docker ps`\\nSolution: If you have postgres software installed on your computer before now, build your instance on a different port like 8080 instead of 5432',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  dodatabase \"ny_taxi\" does not exist'},\n",
       "  {'text': \"Issue:\\ne\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the  ModuleNotFoundError: No module named 'psycopg2'  error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\"},\n",
       "  {'text': 'In the join queries, if we mention the column name directly or enclosed in single quotes itll throw an error says column does not exist.\\nSolution: But if we enclose the column names in double quotes then it will work',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)'},\n",
       "  {'text': 'pgAdmin has a new version. Create server dialog may not appear. Try using register-> server instead.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'pgAdmin - Create server dialog does not appear'},\n",
       "  {'text': 'Using GitHub Codespaces in the browser resulted in a blank screen after the login to pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\\nCSRFError: 400 Bad Request: The referrer does not match the host.\\nSolution #1:\\nAs recommended in the following issue  https://github.com/pgadmin-org/pgadmin4/issues/5432 setting the following environment variable solved it.\\nPGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\"\\nModified docker run command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-p \"8080:80\" \\\\\\n--name pgadmin \\\\\\n--network=pg-network \\\\\\ndpage/pgadmin4:8.2\\nSolution #2:\\nUsing the local installed VSCode to display GitHub Codespaces.\\nWhen using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one) this issue did not occur.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'pgAdmin - Blank/white screen after login (browser)'},\n",
       "  {'text': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when I trying to run the PgAdmin container via docker run or docker compose command, I am failed to access the pgAdmin address via my browser. I have switched to another browser, but still can not access the pgAdmin address. So I modified a little bit the configuration from the previous DE Zoomcamp repository like below and can access the pgAdmin address:\\nSolution #1:\\nModified docker run command\\ndocker run --rm -it \\\\\\n-e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n-e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n-e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n-e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n-e PGADMIN_LISTEN_PORT=5050 \\\\\\n-p 5050:5050 \\\\\\n--network=de-zoomcamp-network \\\\\\n--name pgadmin-container \\\\\\n--link postgres-container \\\\\\n-t dpage/pgadmin4\\nSolution #2:\\nModified docker-compose.yaml configuration (via docker compose up command)\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin-conntainer\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\n- PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n- PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n- PGADMIN_LISTEN_PORT=5050\\nvolumes:\\n- \"./pgadmin_data:/var/lib/pgadmin/data\"\\nports:\\n- \"5050:5050\"\\nnetworks:\\n- de-zoomcamp-network\\ndepends_on:\\n- postgres-conntainer\\nPython - ModuleNotFoundError: No module named \\'pysqlite2\\'\\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. ModuleNotFoundError: No module named \\'pysqlite2\\'\\nThe issue seems to arise from the missing of sqlite3.dll in path \".\\\\Anaconda\\\\Dlls\\\\\".\\nI solved it by simply copying that .dll file from \\\\Anaconda3\\\\Library\\\\bin and put it under the path mentioned above. (if you are using anaconda)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'pgAdmin - Can not access/open the PgAdmin address via browser'},\n",
       "  {'text': 'If you follow the video 1.2.2 - Ingesting NY Taxi Data to Postgres and you execute all the same\\nsteps as Alexey does, you will ingest all the data (~1.3 million rows) into the table yellow_taxi_data as expected.\\nHowever, if you try to run the whole script in the Jupyter notebook for a second time from top to bottom, you will be missing the first chunk of 100000 records. This is because there is a call to the iterator before the while loop that puts the data in the table. The while loop therefore starts by ingesting the second chunk, not the first.\\nSolution: remove the cell df=next(df_iter) that appears higher up in the notebook than the while loop. The first time w(df_iter) is called should be within the while loop.\\nNote: As this notebook is just used as a way to test the code, it was not intended to be run top to bottom, and the logic is tidied up in a later step when it is instead inserted into a .py file for the pipeline',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Python - Ingestion with Jupyter notebook - missing 100000 records'},\n",
       "  {'text': '{t_end - t_start} seconds\")\\nimport pandas as pd\\ndf = pd.read_csv(\\'path/to/file.csv.gz\\', /app/ingest_data.py:1: DeprecationWarning:)\\nIf you prefer to keep the uncompressed csv (easier preview in vscode and similar), gzip files can be unzipped using gunzip (but not unzip). On a Ubuntu local or virtual machine, you may need to apt-get install gunzip first.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Python - Iteration csv without error'},\n",
       "  {'text': \"Pandas can interpret string column values as datetime directly when reading the CSV file using pd.read_csv using the parameter parse_dates, which for example can contain a list of column names or column indices. Then the conversion afterwards is not required anymore.\\npandas.read_csv  pandas 2.1.4 documentation (pydata.org)\\nExample from week 1\\nimport pandas as pd\\ndf = pd.read_csv(\\n'yellow_tripdata_2021-01.csv',\\nnrows=100,\\nparse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\\ndf.info()\\nwhich will output\\n<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 100 entries, 0 to 99\\nData columns (total 18 columns):\\n#   Column                 Non-Null Count  Dtype\\n---  ------                 --------------  -----\\n0   VendorID               100 non-null    int64\\n1   tpep_pickup_datetime   100 non-null    datetime64[ns]\\n2   tpep_dropoff_datetime  100 non-null    datetime64[ns]\\n3   passenger_count        100 non-null    int64\\n4   trip_distance          100 non-null    float64\\n5   RatecodeID             100 non-null    int64\\n6   store_and_fwd_flag     100 non-null    object\\n7   PULocationID           100 non-null    int64\\n8   DOLocationID           100 non-null    int64\\n9   payment_type           100 non-null    int64\\n10  fare_amount            100 non-null    float64\\n11  extra                  100 non-null    float64\\n12  mta_tax                100 non-null    float64\\n13  tip_amount             100 non-null    float64\\n14  tolls_amount           100 non-null    float64\\n15  improvement_surcharge  100 non-null    float64\\n16  total_amount           100 non-null    float64\\n17  congestion_surcharge   100 non-null    float64\\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\nmemory usage: 14.2+ KB\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'iPython - Pandas parsing dates with read_csv'},\n",
       "  {'text': 'os.system(f\"curl -LO {url} -o {csv_name}\")',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Python - Python cant ingest data from the github link provided using curl'},\n",
       "  {'text': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\ndf = pd.read_csv(\\'file.csv.gz\\'\\n, compression=\\'gzip\\'\\n, low_memory=False\\n)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Python - Pandas can read *.csv.gzip'},\n",
       "  {'text': \"Contrary to pandas read_csv method theres no such easy way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\\nimport pyarrow.parquet as pq\\noutput_name = https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\\nparquet_file = pq.ParquetFile(output_name)\\nparquet_size = parquet_file.metadata.num_rows\\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\\ntable_name=yellow_taxi_schema\\n# Clear table if exists\\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\\n# default (and max) batch size\\nindex = 65536\\nfor i in parquet_file.iter_batches(use_threads=True):\\nt_start = time()\\nprint(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\\ni.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\\nindex += 65536\\nt_end = time()\\nprint(f'\\\\t- it took %.1f seconds' % (t_end - t_start))\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Python - How to iterate through and ingest parquet file'},\n",
       "  {'text': 'Error raised during the jupyter notebooks cell execution:\\nfrom sqlalchemy import create_engine.\\nSolution: Version of Python module typing_extensions >= 4.6.0. Can be updated by Conda or pip.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\"},\n",
       "  {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\"},\n",
       "  {'text': \"Error raised during the jupyter notebooks cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module psycopg2. Can be installed by Conda or pip.\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\"},\n",
       "  {'text': 'Unable to add Google Cloud SDK PATH to Windows\\nWindows error: The installer is unable to automatically update your system PATH. Please add  C:\\\\tools\\\\google-cloud-sdk\\\\bin\\nif you are constantly getting this feedback. Might be that you needed to add Gitbash to your Windows path:\\nOne way of doing that is to use conda: If you are not already using it\\nDownload the Anaconda Navigator\\nMake sure to check the box (add conda to the path when installing navigator: although not recommended do it anyway)\\nYou might also need to install git bash if you are not already using it(or you might need to uninstall it to reinstall it properly)\\nMake sure to check the following boxes while you install Gitbash\\nAdd a GitBash to Windows Terminal\\nUse Git and optional Unix tools from the command prompt\\nNow open up git bash and type conda init bash This should modify your bash profile\\nAdditionally, you might want to use Gitbash as your default terminal.\\nOpen your Windows terminal and go to settings, on the default profile change Windows power shell to git bash',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP - Unable to add Google Cloud SDK PATH to Windows'},\n",
       "  {'text': 'It asked me to create a project. This should be done from the cloud console. So maybe we dont need this FAQ.\\nWARNING: Project creation failed: HttpError accessing <https://cloudresourcemanager.googleapis.com/v1/projects?alt=json>: response: <{\\'vtpep_pickup_datetimeary\\': \\'Origin, X-Origin, Referer\\', \\'content-type\\': \\'application/json; charset=UTF-8\\', \\'content-encoding\\': \\'gzip\\', \\'date\\': \\'Mon, 24 Jan 2022 19:29:12 GMT\\', \\'server\\': \\'ESF\\', \\'cache-control\\': \\'private\\', \\'x-xss-protection\\': \\'0\\', \\'x-frame-options\\': \\'SAMEORIGIN\\', \\'x-content-type-options\\': \\'nosniff\\', \\'server-timing\\': \\'gfet4t7; dur=189\\', \\'alt-svc\\': \\'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\\', \\'transfer-encoding\\': \\'chunked\\', \\'status\\': 409}>, content <{\\n\"error\": {\\n\"code\": 409,\\n\"message\": \"Requested entity alreadytpep_pickup_datetime exists\",\\n\"status\": \"ALREADY_EXISTS\"\\n}\\n}\\nFrom Stackoverflow: https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1\\nProject IDs are unique across all projects. That means if any user ever had a project with that ID, you cannot use it. testproject is pretty common, so it\\'s not surprising it\\'s already taken.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP - Project creation failed: HttpError accessing  Requested entity alreadytpep_pickup_datetime exists'},\n",
       "  {'text': 'If you receive the error: Error 403: The project to be billed is associated with an absent billing account., accountDisabled It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\\nThe value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\\nAshish Agrawal\\nAnother possibility is that you have not linked your billing account to your current project',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP - The project to be billed is associated with an absent billing account'},\n",
       "  {'text': 'GCP Account Suspension Inquiry\\nIf Google refuses your credit/debit card, try another - Ive got an issue with Kaspi (Kazakhstan) but it worked with TBC (Georgia).\\nUnfortunately, theres small hope that support will help.\\nIt seems that Pyypl web-card should work too.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP - OR-CBAT-15 ERROR Google cloud free trial account'},\n",
       "  {'text': 'The ny-rides.json is your private file in Google Cloud Platform (GCP). \\n\\nAnd heres the way to find it:\\nGCP -> Select project with your  instance -> IAM & Admin -> Service Accounts Keys tab -> add key, JSON as key type, then click create\\nNote: Once you go into Service Accounts Keys tab, click the email, then you can see the KEYS tab where you can add key as a JSON as its key type',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP - Where can I find the ny-rides.json file?'},\n",
       "  {'text': 'In this lecture, Alexey deleted his instance in Google Cloud. Do I have to do it?\\nNope. Do not delete your instance in Google Cloud platform. Otherwise, you have to do this twice for the week 1 readings.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP - Do I need to delete my instance in Google Cloud?'},\n",
       "  {'text': 'System Resource Usage:\\ntop or htop: Shows real-time information about system resource usage, including CPU, memory, and processes.\\nfree -h: Displays information about system memory usage and availability.\\ndf -h: Shows disk space usage of file systems.\\ndu -h <directory>: Displays disk usage of a specific directory.\\nRunning Processes:\\nps aux: Lists all running processes along with detailed information.\\nNetwork:\\nifconfig or ip addr show: Shows network interface configuration.\\nnetstat -tuln: Displays active network connections and listening ports.\\nHardware Information:\\nlscpu: Displays CPU information.\\nlsblk: Lists block devices (disks and partitions).\\nlshw: Lists hardware configuration.\\nUser and Permissions:\\nwho: Shows who is logged on and their activities.\\nw: Displays information about currently logged-in users and their processes.\\nPackage Management:\\napt list --installed: Lists installed packages (for Ubuntu and Debian-based systems)',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Commands to inspect the health of your VM:'},\n",
       "  {'text': 'if youve got the error\\n Error: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at https://console.cloud.google.com/billing. The default table expiration time must be less than 60 days, billingNotEnabled\\nbut youve set your billing account indeed, then try to disable billing for the project and enable it again. It worked for ME!',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Billing account has not been enabled for this project. But youve done it indeed!'},\n",
       "  {'text': 'for windows if you having trouble install SDK try follow these steps on the link, if you getting this error:\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\nWARNING:\\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\nFor me:\\nI reinstalled the sdk using unzip file install.bat,\\nafter successfully checking gcloud version,\\nrun gcloud init to set up project before\\nyou run gcloud auth application-default login\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md\\nGCP VM - I cannot get my Virtual Machine to start because GCP has no resources.\\nClick on your VM\\nCreate an image of your VM\\nOn the page of the image, tell GCP to create a new VM instance via the image\\nOn the settings page, change the location',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP - Windows Google Cloud SDK install issue:gcp'},\n",
       "  {'text': 'The reason this video about the GCP VM exists is that many students had problems configuring their env. You can use your own env if it works for you.\\nAnd the advantage of using your own environment is that if you are working in a Github repo where you can commit, you will be able to commit the changes that you do. In the VM the repo is cloned via HTTPS so it is not possible to directly commit, even if you are the owner of the repo.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP VM - Is it necessary to use a GCP VM? When is it useful?'},\n",
       "  {'text': \"I am trying to create a directory but it won't let me do it\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n$ mkdir .ssh\\nmkdir: cannot create directory .ssh: Permission denied\\nYou should do it in your home directory. Should be your home (~)\\nLocal. But it seems you're trying to do it in the root folder (/). Should be your home (~)\\nLink to Video 1.4.1\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP VM - mkdir: cannot create directory .ssh: Permission denied'},\n",
       "  {'text': \"Failed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\nYou need to change the owner of the files you are trying to edit via VS Code. You can run the following command to change the ownership.\\nssh\\nsudo chown -R <user> <path to your directory>\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP VM - Error while saving the file in VM via VS Code'},\n",
       "  {'text': 'Question: I connected to my VM perfectly fine last week (ssh) but when I tried again this week, the connection request keeps timing out.\\nAnswer: Start your VM. Once the VM is running, copy its External IP and paste that into your config file within the ~/.ssh folder.\\ncd ~/.ssh\\ncode config  this opens the config file in VSCode',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': '. GCP VM - VM connection request timeout'},\n",
       "  {'text': '(reference: https://serverfault.com/questions/953290/google-compute-engine-ssh-connect-to-host-ip-port-22-operation-timed-out)Go to edit your VM.\\nGo to section Automation\\nAdd Startup script\\n```\\n#!/bin/bash\\nsudo ufw allow ssh\\n```\\nStop and Start VM.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP VM -  connect to host port 22 no route to host'},\n",
       "  {'text': 'You can easily forward the ports of pgAdmin, postgres and Jupyter Notebook using the built-in tools in Ubuntu and without any additional client:\\nFirst, in the VM machine, launch docker-compose up -d and jupyter notebook in the correct folder.\\nFrom the local machine, execute: ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\\nExecute the same command but with ports 8080 and 8888.\\nNow you can access pgAdmin on local machine in browser typing localhost:8080\\nFor Jupyter Notebook, type localhost:8888 in the browser of your local machine. If you have problems with the credentials, it is possible that you have to copy the link with the access token provided in the logs of the terminal of the VM machine when you launched the jupyter notebook command.\\nTo forward both pgAdmin and postgres use, ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP VM - Port forwarding from GCP without using VS Code'},\n",
       "  {'text': 'If you are using MS VS Code and running gcloud in WSL2, when you first try to login to gcp via the gcloud cli gcloud auth application-default login, you will see a message like this, and nothing will happen\\nAnd there might be a prompt to ask if you want to open it via browser, if you click on it, it will open up a page with error message\\nSolution : you should instead hover on the long link, and ctrl + click the long link\\n\\nClick configure Trusted Domains here\\n\\nPopup will appear, pick first or second entry\\nNext time you gcloud auth, the login page should popup via default browser without issues',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'GCP gcloud + MS VS Code - gcloud auth hangs'},\n",
       "  {'text': 'It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Error: Failed to query available provider packages  Could not retrieve the list of available versions for provider hashicorp/google: could not query  provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts,  please try again later'},\n",
       "  {'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout'},\n",
       "  {'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Install for WSL'},\n",
       "  {'text': 'https://github.com/hashicorp/terraform/issues/14513',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Error acquiring the state lock'},\n",
       "  {'text': 'When running\\nterraform apply\\non wsl2 I\\'ve got this error:\\n Error: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\\n Response: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\\nIT happens because there may be time desync on your machine which affects computing JWT\\nTo fix this, run the command\\nsudo hwclock -s\\nwhich fixes your system time.\\nReference',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.'},\n",
       "  {'text': ' Error: googleapi: Error 403: Access denied., forbidden\\nYour $GOOGLE_APPLICATION_CREDENTIALS might not be pointing to the correct file \\nrun = export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\nAnd then = gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Error 403 : Access denied'},\n",
       "  {'text': \"One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?'},\n",
       "  {'text': 'Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?'},\n",
       "  {'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g'},\n",
       "  {'text': 'The error:\\nError: googleapi: Error 403: Access denied., forbidden\\n\\nand\\n Error: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\nFor this solution make sure to run:\\necho $GOOGLE_APPLICATION_CREDENTIALS\\necho $?\\nSolution:\\nYou have to set again the GOOGLE_APPLICATION_CREDENTIALS as Alexey did in the environment set-up video in week1:\\nexport GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes'},\n",
       "  {'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access storage.buckets.create'},\n",
       "  {'text': 'provider \"google\" {\\nproject     = var.projectId\\ncredentials = file(\"${var.gcpkey}\")\\n#region      = var.region\\nzone = var.zone\\n}',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.'},\n",
       "  {'text': 'For the HW1 I encountered this issue. The solution is\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria Zone\\';\\nI think columns which start with uppercase need to go between Column. I ran into a lot of issues like this and   made it work out.\\nAddition to the above point, for me, there is no Astoria Zone, only Astoria is existing in the dataset.\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria;',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\"},\n",
       "  {'text': 'It is inconvenient to use quotation marks all the time, so it is better to put the data to the database all in lowercase, so in Pandas after\\ndf = pd.read_csv(taxi+_zone_lookup.csv)\\nAdd the row:\\ndf.columns = df.columns.str.lower()',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\"},\n",
       "  {'text': 'Solution (for mac users): os.system(f\"curl {url} --output {csv_name}\")',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'CURL - curl: (6) Could not resolve host: output.csv'},\n",
       "  {'text': 'To resolve this, ensure that your config file is in C/User/Username/.ssh/config',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known'},\n",
       "  {'text': 'If you use Anaconda (recommended for the course), it comes with pip, so the issues is probably that the anacondas Python is not on the PATH.\\nAdding it to the PATH is different for each operation system.\\nFor Linux and MacOS:\\nOpen a terminal.\\nFind the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/path/to/anaconda3/bin:$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\\nOn Windows, python and pip are in different locations (python is in the anaconda root, and pip is in Scripts). With GitBash:\\nLocate your Anaconda installation. The default path is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\nDetermine the correct path format for Git Bash. Paths in Git Bash follow the Unix-style, so convert the Windows path to a Unix-style path. For example, `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\\nAdd Anaconda to your PATH with the command: `export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"`.\\nTo make this change permanent, add the command to your `.bashrc` file in your home directory.\\nRefresh your environment with the command: `source ~/.bashrc`.\\nFor Windows (without Git Bash):\\nRight-click on \\'This PC\\' or \\'My Computer\\' and select \\'Properties\\'.\\nClick on \\'Advanced system settings\\'.\\nIn the System Properties window, click on \\'Environment Variables\\'.\\nIn the Environment Variables window, select the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\nIn the Edit Environment Variable window, click \\'New\\' and add the path to your Anaconda installation (typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` and C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\nClick \\'OK\\' in all windows to apply the changes.\\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.',\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': \"'pip' is not recognized as an internal or external command, operable program or batch file.\"},\n",
       "  {'text': \"Resolution: You need to stop the services which is using the port.\\nRun the following:\\n```\\nsudo kill -9 `sudo lsof -t -i:<port>`\\n```\\n<port> being 8080 in this case. This will free up the port for use.\\n~ Abhijit Chakraborty\\nError: error response from daemon: cannot stop container: 1afaf8f7d52277318b71eef8f7a7f238c777045e769dd832426219d6c4b8dfb4: permission denied\\nResolution: In my case, I had to stop docker and restart the service to get it running properly\\nUse the following command:\\n```\\nsudo systemctl restart docker.socket docker.service\\n```\\n~ Abhijit Chakraborty\\nError: cannot import module psycopg2\\nResolution: Run the following command in linux:\\n```\\nsudo apt-get install libpq-dev\\npip install psycopg2\\n```\\n~ Abhijit Chakraborty\\nError: docker build Error checking context: 'can't stat '<path-to-file>'\\nResolution: This happens due to insufficient permission for docker to access a certain file within the directory which hosts the Dockerfile.\\n1. You can create a .dockerignore file and add the directory/file which you want Dockerfile to ignore while build.\\n2. If the above does not work, then put the dockerfile and corresponding script, `\\t1.py` in our case to a subfolder. and run `docker build ...`\\nfrom inside the new folder.\\n~ Abhijit Chakraborty\",\n",
       "   'section': 'Module 1: Docker and Terraform',\n",
       "   'question': 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use'},\n",
       "  {'text': 'To get a pip-friendly requirements.txt file file from Anaconda use\\nconda install pip then `pip list format=freeze > requirements.txt`.\\n`conda list -d > requirements.txt` will not work and `pip freeze > requirements.txt` may give odd pathing.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Anaconda to PIP'},\n",
       "  {'text': 'Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Where are the FAQ questions from the previous cohorts for the orchestration module?'},\n",
       "  {'text': 'Issue : Docker containers exit instantly with code 132, upon docker compose up\\nMage documentation has it listing the cause as \"older architecture\" .\\nThis might be a hardware issue, so unless you have another computer, you can\\'t solve it without purchasing a new one, so the next best solution is a VM.\\nThis is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), its really inconclusive at this time.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Docker - 2.2.2 Configure Mage'},\n",
       "  {'text': 'This issue was occurring with Windows WSL 2\\nFor me this was because WSL 2 was not dedicating enough cpu cores to Docker.The load seems to take up at least one cpu core so I recommend dedicating at least two.\\nOpen Bash and run the following code:\\n$ cd ~\\n$ ls -la\\nLook for the .wsl config file:\\n-rw-r--r-- 1 ~1049089       31 Jan 25 12:54  .wslconfig\\nUsing a text editing tool of your choice edit or create your .wslconfig file:\\n$ nano .wslconfig\\nPaste the following into the new file/ edit the existing file in this format and save:\\n*** Note - for memory this is the RAM on your machine you can dedicate to Docker, your situation may be different than mine ***\\n[wsl2]\\nprocessors=<Number of Processors - at least 2!> example: 4\\nmemory=<memory> example:4GB\\nExample:\\nOnce you do that run:\\n$ wsl --shutdown\\nThis shuts down WSL\\nThen Restart Docker Desktop - You should now be able to load the .csv.gz file without the error into a pandas dataframe',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; Kernel Running out of memory:'},\n",
       "  {'text': 'The issue and solution on the link:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1706817366764269?thread_ts=1706815324.993529&cid=C01FABYF2RG',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': '2.2.3 Configuring Postgres'},\n",
       "  {'text': 'Check that the POSTGRES_PORT variable in the io_config.yml  file is set to port 5432, which is the default postgres port. The POSTGRES_PORT variable is the mage container port, not the host port. Hence, theres no need to set the POSTGRES_PORT to 5431 just because you already have a conflicting postgres installation in your host machine.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'MAGE - 2.2.3 OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5431 failed: Connection refused'},\n",
       "  {'text': 'You forgot to select dev profile in the dropdown menu next to where you select PostgreSQL in the connection drop down.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'MAGE - 2.2.4 executing SELECT 1; results in KeyError'},\n",
       "  {'text': 'If you are getting this error. Update your mage io_config.yaml file, and specify a timeout value set to 600 like this.\\nMake sure to save your changes.\\nMAGE - 2.2.4 Testing BigQuery connection using SQL 404 error:\\nNotFound: 404 Not found: Dataset ny-rides-diegogutierrez:None was not found in location northamerica-northeast1\\nIf you get this error even with all roles/permissions given to the service account check if you have ticked the box where it says Use raw SQL, just like the image below.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': \"MAGE -2.2.4 ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))\"},\n",
       "  {'text': 'Solution: https://stackoverflow.com/questions/48056381/google-client-invalid-jwt-token-must-be-a-short-lived-token',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': \"Problem: RefreshError: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.'})\"},\n",
       "  {'text': \"Origin of Solution (Mage Slack-Channel): https://mageai.slack.com/archives/C03HTTWFEKE/p1706543947795599\\nProblem: This error can often be seen after solving the error mentioned in 2.2.4. The error can be found in Mage version 0.9.61 and is a side-effect of the update of the code for data-loader blocks.\\nNote: Mage 0.9.62 has been released, as of Feb 5 2024. Please recheck. Solution below may be obsolete\\nSolution: Using a fixed version of the docker container\\nPull updated docker image from docker-hub\\nmageai/mageaidocker pull:alpha\\nUpdate docker-compose.yaml\\nversion: '3'\\nservices:\\nmagic:\\nimage: mageai/mageai:alpha  <--- instead of latest-tag\\ndocker-compose up\\nThe original Error is still present, but the SQL-query will return the desired result:\\n--------------------------------------------------------------------------------------\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Mage - 2.2.4 IndexError: list index out of range'},\n",
       "  {'text': 'Add\\nif not path.parent.is_dir():\\npath.parent.mkdir(parents=True)\\npath = Path(path).as_posix()\\nsee:\\nhttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1675774214591809?thread_ts=1675768839.028879&cid=C01FABYF2RG',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': '2.2.6 OSError: Cannot save file into a non-existent directory: \\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")'},\n",
       "  {'text': 'The video DE Zoomcamp 2.2.7 is missing  the actual deployment of Mage using Terraform to GCP. The steps for the deployment were not covered in the video.\\nI successfully deployed it and wanted to share some key points:\\nIn variables.tf, set the project_id default value to your GCP project ID.\\nEnable the Cloud Filestore API:\\nVisit the Google Cloud Console.to\\nNavigate to \"APIs & Services\" > \"Library.\"\\nSearch for \"Cloud Filestore API.\"\\nClick on the API and enable it.\\nTo perform the deployment:\\nterraform init\\nterraform apply\\nPlease note that during the terraform apply step, Terraform will prompt you to enter the PostgreSQL password. After that, it will ask for confirmation to proceed with the deployment. Review the changes, type \\'yes\\' when prompted, and press Enter.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'GCP - 2.2.7d Deploying Mage to GCP'},\n",
       "  {'text': 'If you want to rune multiple docker containers from different directories. Then make sure to change the port mappings in the docker-compose.yml file.\\nports:\\n- 8088:6789\\nThe 8088 port in above case is hostport, where mage will run on your local machine. You can customize this as long as the port is available. If you are running on VM, make sure to forward the port too. You need to keep the container port to 6789 as this is the port where mage is running.\\nGCP - 2.2.7d Deploying Mage to Google Cloud\\nWhile terraforming all the resources inside a VM created in GCS the following error is shown.\\nError log:\\nmodule.lb-http.google_compute_backend_service.default[\"default\"]: Creating...\\n\\n Error: Error creating GlobalAddress: googleapi: Error 403: Request had insufficient authentication scopes.\\n Details:\\n [\\n   {\\n     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n     \"domain\": \"googleapis.com\",\\n     \"metadatas\": {\\n       \"method\": \"compute.beta.GlobalAddressesService.Insert\",\\n       \"service\": \"compute.googleapis.com\"\\n     },\\n     \"reason\": \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\\n   }\\n ]\\n\\n More details:\\n Reason: insufficientPermissions, Message: Insufficient Permission\\nThis error might happen when you are using a VM inside GCS. To use the Google APIs from a GCP virtual machine you need to add the cloud platform scope (\"https://www.googleapis.com/auth/cloud-platform\") to your VM when it is created.\\nSince ours is already created you can just stop it and change the permissions. You can do it in the console, just go to \"EDIT\", g99o all the way down until you find \"Cloud API access scopes\". There you can \"Allow full access to all Cloud APIs\". I did this and all went smoothly generating all the resources needed. Hope it helps if you encounter this same error.\\nResources: https://stackoverflow.com/questions/35928534/403-request-had-insufficient-authentication-scopes-during-gcloud-container-clu',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Ruuning Multiple Mage instances in Docker from different directories'},\n",
       "  {'text': 'If you are on the free trial account on GCP you will face this issue when trying to deploy the infrastructures with terraform. This service is not available for this kind of account.\\nThe solution I found was to delete the load_balancer.tf file and to comment or delete the rows that differentiate it on the main.tf file. After this just do terraform destroy to delete any infrastructure created on the fail attempts and re-run the terraform apply.\\nCode on main.tf to comment/delete:\\nLine 166, 167, 168',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'GCP - 2.2.7d Load Balancer Problem (Security Policies quota)'},\n",
       "  {'text': \"If you get the following error\\nYou have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\\nYou can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\\nDeploying MAGE to GCP  with Terraform via the VM (2.2.7)\\nFYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so dont interrupt the process or think its taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\\n`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the app.name you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\\nWhy are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\\nI checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as mage-data-prep- it has been costing 5.01 of my free credits each day  I now have 151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'GCP - 2.2.7d Part 2 - Getting error when you run terraform apply'},\n",
       "  {'text': '```\\n Error: Error creating Connector: googleapi: Error 403: Permission \\'vpcaccess.connectors.create\\' denied on resource \\'//vpcaccess.googleapis.com/projects/<ommit>/locations/us-west1\\' (or it may not exist).\\n Details:\\n [\\n   {\\n     \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n     \"domain\": \"vpcaccess.googleapis.com\",\\n     \"metadata\": {\\n       \"permission\": \"vpcaccess.connectors.create\",\\n       \"resource\": \"projects/<ommit>/locations/us-west1\"\\n     },\\n     \"reason\": \"IAM_PERMISSION_DENIED\"\\n   }\\n ]\\n\\n   with google_vpc_access_connector.connector,\\n   on fs.tf line 19, in resource \"google_vpc_access_connector\" \"connector\":\\n   19: resource \"google_vpc_access_connector\" \"connector\" {\\n\\n```\\nSolution: Add Serverless VPC Access Admin to Service Account.\\nLine 148',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': \"Question: Permission 'vpcaccess.connectors.create'\"},\n",
       "  {'text': 'Git wont push an empty folder to GitHub, so if you put a file in that folder and then push, then you should be good to go.\\nOr - in your code- make the folder if it doesnt exist using Pathlib as shown here: https://stackoverflow.com/a/273227/4590385.\\nFor some reason, when using github storage, the relative path for writing locally no longer works. Try using two separate paths, one full path for the local write, and the original relative path for GCS bucket upload.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': \"File Path: Cannot save file into a non-existent directory: 'data/green'\"},\n",
       "  {'text': 'The green dataset contains lpep_pickup_datetime while the yellow contains tpep_pickup_datetime. Modify the script(s) depending on  the dataset as required.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'No column name lpep_pickup_datetime / tpep_pickup_datetime'},\n",
       "  {'text': 'pd.read_csv\\ndf_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\\nThe data needs to be appended to the parquet file using the fastparquet engine\\ndf.to_parquet(path, compression=\"gzip\", engine=\\'fastparquet\\', append=True)',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Process to download the VSC using Pandas is killed right away'},\n",
       "  {'text': 'denied: requested access to the resource is denied\\nThis can happen when you\\nHaven\\'t logged in properly to Docker Desktop (use docker login -u \"myusername\")\\nHave used the wrong username when pushing to docker images. Use the same one as your username and as the one you build on\\ndocker image build -t <myusername>/<imagename>:<tag>\\ndocker image push <myusername>/<imagename>:<tag>',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Push to docker image failure'},\n",
       "  {'text': \"16:21:35.607 | INFO    | Flow run 'singing-malkoha' - Executing 'write_bq-b366772c-0' immediately...\\nKilled\\nSolution:  You probably are running out of memory on your VM and need to add more.  For example, if you have 8 gigs of RAM on your VM, you may want to expand that to 16 gigs.\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Flow script fails with killed message:'},\n",
       "  {'text': 'After playing around with prefect for a while this can happen.\\nSsh to your VM and run sudo du -h --block-size=G | sort -n -r | head -n 30 to see which directory needs the most space.\\nMost likely it will be /.prefect/storage, where your cached flows are stored. You can delete older flows from there. You also have to delete the corresponding flow in the UI, otherwise it will throw you an error, when you try to run your next flow.\\nSSL Certificate Verify: (I got it when trying to run flows on MAC): urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]\\npip install certifi\\n/Applications/Python\\\\ {ver}/Install\\\\ Certificates.command\\nor\\nrunning the Install Certificate.command inside of the python{ver} folder',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'GCP VM: Disk Space is full'},\n",
       "  {'text': 'It means your container consumed all available RAM allocated to it. It can happen in particular when working on Question#3 in the homework as the dataset is relatively large and containers eat a lot of memory in general.\\nI would recommend restarting your computer and only starting the necessary processes to run the container. If that doesnt work, allocate more resources to docker. If also that doesnt work because your workstation is a potato, you can use an online compute environment service like GitPod, which is free under under 50 hours / month of use.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Docker: container crashed with status code 137.'},\n",
       "  {'text': 'In Q3 there was a task to run the etl script from web to GCS. The problem was, it wasnt really an ETL straight from web to GCS, but it was actually a web to local storage to local memory to GCS over network ETL. Yellow data is about 100 MB each per month compressed and ~700 MB after uncompressed on memory\\nThis leads to a problem where i either got a network type error because my not so good 3rd world internet or i got my WSL2 crashed/hanged because out of memory error and/or 100% resource usage hang.\\nSolution:\\nif you have a lot of time at hand, try compressing it to parquet and writing it to GCS with the timeout argument set to a really high number (the default os 60 seconds)\\nthe yellow taxi data for feb 2019 is about 100MB as parquet file\\ngcp_cloud_storage_bucket_block.upload_from_path(\\nfrom_path=f\"{path}\",\\nto_path=path,\\ntimeout=600\\n)',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Timeout due to slow upload internet'},\n",
       "  {'text': 'This error occurs when you try to re-run the export block, of the transformed green_taxi data to PostgreSQL.\\nWhat youll need to do is to drop the table using SQL in Mage (screenshot below).\\nYou should be able to re-run the block successfully after dropping the table.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'UndefinedColumn: column \"ratecode_id\", \"rate_code_id\" vendor_id, pu_location_id, do_location_id of relation \"green_taxi\" does not exist - Export transformed green_taxi data to PostgreSQL'},\n",
       "  {'text': 'SettingWithCopyWarning:\\nA value is trying to be set on a copy of a slice from a DataFrame.\\nUse the data.loc[] = value syntax instead of df[] = value to ensure that the new column is being assigned to the original dataframe instead of a copy of a dataframe or a series.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Homework - Q3 SettingWithCopyWarning Error:'},\n",
       "  {'text': 'CSV Files are very big in nyc data, so we instead of using Pandas/Python kernel , we can try Pyspark Kernel\\nDocumentation of Mage for using pyspark kernel: https://docs.mage.ai/integrations/spark-pyspark\\n?',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Since I was using slow laptop, and we have so big csv files, I used pyspark kernel in mage instead of python, How to do it?'},\n",
       "  {'text': 'So we will first delete the connection between blocks then we can remove the connection.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'I got an error when I was deleting  BLOCK IN A PIPELINE'},\n",
       "  {'text': 'While Editing the Pipeline Name It throws permission denied error.\\n(Work around)In that case proceed with the work and save later on revisit it will let you edit.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Mage UI wont let you edit the Pipeline name?'},\n",
       "  {'text': 'Solution n1 if you want to download everything :\\n```\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nfrom pyarrow.fs import GcsFileSystem\\n\\n@data_loader\\ndef load_data(*args, **kwargs):\\n    bucket_name = YOUR_BUCKET_NAME_HERE\\'\\n    blob_prefix = \\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\n    root_path = f\"{bucket_name}/{blob_prefix}\"\\npa_table = pq.read_table(\\n        source=root_path,\\n        filesystem=GcsFileSystem(),        \\n    )\\n\\n    return pa_table.to_pandas()\\nSolution n2 if you want to download only some dates :\\n@data_loader\\ndef load_data(*args, **kwargs):\\ngcs = pa.fs.GcsFileSystem()\\nbucket_name = \\'YOUR_BUCKET_NAME_HERE\\'\\nblob_prefix = \\'\\'PATH / TO / WHERE / THE / PARTITIONS / ARE\\'\\'\\nroot_path = f\"{bucket_name}/{blob_prefix}\"\\npa_dataset = pq.ParquetDataset(\\npath_or_paths=root_path,\\nfilesystem=gcs,\\nfilters=[(\\'lpep_pickup_date\\', \\'>=\\', \\'2020-10-01\\'), (\\'lpep_pickup_date\\', \\'<=\\', \\'2020-10-31\\')]\\n)\\nreturn pa_dataset.read().to_pandas()\\n# More information about the pq.Parquet.Dataset : Encapsulates details of reading a complete Parquet dataset possibly consisting of multiple files and partitions in subdirectories. Documentation here :\\nhttps://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html#pyarrow.parquet.ParquetDataset\\nERROR: UndefinedColumn: column \"vendor_id\" of relation \"green_taxi\" does not exist\\nTwo possible solutions both of them work in the same way.\\nOpen up a Data Loader connect using SQL - RUN the command \\n`DROP TABLE mage.green_taxi`\\nElse, Open up a Data Extractor of SQL  - increase the rows to above the number of rows in the dataframe (you can find that in the bottom of the transformer block) change the Write Policy to `Replace` and run the SELECT statement',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?'},\n",
       "  {'text': \"All mage files are in your /home/src/folder where you saved your credentials.json so you should be able to access them locally. You will see a folder for Pipelines,  'data loaders', 'data transformers' & 'data exporters' - inside these will be the .py or .sql files for the blocks you created in your pipeline.\\nRight click & download the pipeline itself to your local machine (which gives you metadata, pycache and other files)\\nAs above, download each .py/.sql file that corresponds to each block you created for the pipeline. You'll find these under 'data loaders', 'data transformers' 'data exporters'\\nMove the downloaded files to your GitHub repo folder & commit your changes.\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:'},\n",
       "  {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun git remote remove origin to de-couple the Mage repo,\\nrun rm -rf .git to delete local git files,\\nrun git add . to add the current folder as changes to stage, commit and push.',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?'},\n",
       "  {'text': \"When try to add three assertions:\\nvendor_id is one of the existing values in the column (currently)\\npassenger_count is greater than 0\\ntrip_distance is greater than 0\\nto test_output, I got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). Below is my code:\\ndata_filter = (data['passenger_count'] > 0) and (data['trip_distance'] > 0)\\nAfter looking for solutions at Stackoverflow, I found great discussion about it. So I changed my code into:\\ndata_filter = (data['passenger_count'] > 0) & (data['trip_distance'] > 0)\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()'},\n",
       "  {'text': 'This happened when I just booted up my PC, continuing from the progress I was doing from yesterday.\\nAfter cd-ing into your directory, and running docker compose up , the web interface for the Mage shows, but the files that I had yesterday was gone.\\nIf your files are gone, go ahead and close the web interface, and properly shutting down the mage docker compose by doing Ctrl + C once. Try running it again. This worked for me more than once (yes the issue persisted with my PC twice)\\nAlso, you should check if youre in the correct repository before doing docker compose up . This was discussed in the Slack #course-data-engineering channel',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Mage AI Files are Gone/disappearing'},\n",
       "  {'text': 'The above errors due to  at the trailing side and it need to be modified with  quotes at both ends\\nKrishna Anand',\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Mage - Errors in io.config.yaml file'},\n",
       "  {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket using pyarrow suggesting Mage doesnt have the necessary permissions to access the specified GCP credentials .json file.\\nArrowException: Unknown error: google::cloud::Status(UNKNOWN: Permanent error GetBucketMetadata: Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: Cannot open credentials file /home/src/...\\nSolution: Inside the Mage app:\\nCreate a credentials folder (e.g. gcp-creds) within the magic-zoomcamp folder\\nIn the credentials folder create a .json key file (e.g. mage-gcp-creds.json)\\nCopy/paste GCP service account credentials into the .json key file and save\\nUpdate code to point to this file. E.g.\\nenviron['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/magic-zoomcamp/gcp-creds/mage-gcp-creds.json'\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Mage - ArrowException Cannot open credentials file'},\n",
       "  {'text': \"Oserror: google::cloud::status(unavailable: retry policy exhausted getbucketmetadata: could not create a OAuth2 access token to authenticate the request. the request was not sent, as such an access token is required to complete the request successfully. learn more about google cloud authentication at https://cloud.google.com/docs/authentication. the underlying error message was: performwork() - curl error [6]=couldn't resolve host name)\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Mage - OSError'},\n",
       "  {'text': \"Problem: The following error occurs when attempting to export data from Mage to a GCS bucket. Assigned service account doesnt have the necessary permissions access Google Cloud Storage Bucket\\nPermissionError: [Errno 13] google::cloud::Status(PERMISSION_DENIED: Permanent error GetBucketMetadata:... .iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist). error_info={reason=forbidden, domain=global, metadata={http_status_code=403}}). Detail: [errno 13] Permission denied\\nSolution: Add Cloud Storage Admin role to the service account:\\nGo to project in Google Cloud Console>IAM & Admin>IAM\\nClick Edit principal (pencil symbol) to the right of the service account you are using\\nClick + ADD ANOTHER ROLE\\nSelect Cloud Storage>Storage Admin\\nClick Save\",\n",
       "   'section': 'Module 2: Workflow Orchestration',\n",
       "   'question': 'Mage - PermissionError service account does not have storage.buckets.get access to the Google Cloud Storage bucket'},\n",
       "  {'text': '1. Make sure your pyspark script is ready to be send to Dataproc cluster\\n2. Create a Dataproc Cluster in GCP Console\\n3. Make sure to edit the service account and add new role - Dataproc Editor\\n4. Copy the python script ./notebooks/pyspark_script.py and place it under GCS bucket path\\n5. Make sure gcloud cli is installed either in Mage manually or  via your Dockerfile and docker-compose files. This is needed to let Mage access google Dataproc and the script it needs to execute. Refer - Installing the latest gcloud CLI\\n6. Use the Bigquery/Dataproc script mentioned here - https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/code/cloud.md . Use Mage to trigger the query',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'Trigger Dataproc from Mage'},\n",
       "  {'text': 'A:\\n1 solution) Add -Y flag, so that apt-get automatically agrees to install additional packages\\n2) Use python ZipFile package, which is included in all modern python distributions',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets'},\n",
       "  {'text': 'Make sure to use Nullable dataTypes, such as Int64 when appliable.',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCS Bucket - error when writing data from web to GCS:'},\n",
       "  {'text': 'Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\\nWhen dealing for example with the FHV Datasets from 2019, however (see image below), one can see that the files for \\'2019-05\\', and 2019-06, have the columns \"PUlocationID\" and \"DOlocationID\" as Integers, while for the period of \\'2019-01\\' through \\'2019-04\\', the same column is defined as FLOAT.\\nSo while importing these files as parquet to BigQuery, the first one will be used to define the schema of the table, while all files following that will be used to append data on the existing table. Which means, they must all follow the very same schema of the file that created the table.\\nSo, in order to prevent errors like that, make sure to enforce the data types for the columns on the DataFrame before you serialize/upload them to BigQuery. Like this:\\npd.read_csv(\"path_or_url\").astype({\\n\\t\"col1_name\": \"datatype\",\\t\\n\\t\"col2_name\": \"datatype\",\\t\\n\\t...\\t\\t\\t\\t\\t\\n\\t\"colN_name\": \"datatype\" \\t\\n})',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': \"GCS Bucket - Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\"},\n",
       "  {'text': \"If you receive the error gzip.BadGzipFile: Not a gzipped file (b'\\\\n\\\\n'), this is because you have specified the wrong URL to the FHV dataset. Make sure to use https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\nEmphasising the /releases/download part of the URL.\",\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCS Bucket - Fix Error when importing FHV data to GCS'},\n",
       "  {'text': 'Krishna Anand',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCS Bucket - Load Data From URL list in to GCP Bucket'},\n",
       "  {'text': 'Check the Schema\\nYou might have a wrong formatting\\nTry to upload the CSV.GZ files without formatting or going through pandas via wget\\nSee this Slack conversation for helpful tips',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?'},\n",
       "  {'text': 'Run the following command to check if BigQuery Command Line Tool is installed or not: gcloud components list\\nYou can also use bq.cmd instead of bq to make it work.',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - bq: command not found'},\n",
       "  {'text': 'Use big queries carefully,\\nI created by bigquery dataset on an account where my free trial was exhausted, and got a bill of $80.\\nUse big query in free credits and destroy all the datasets after creation.\\nCheck your Billing daily! Especially if youve spinned up a VM.',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Caution in using bigquery:no'},\n",
       "  {'text': 'Be careful when you create your resources on GCP, all of them have to share the same Region in order to allow load data from GCS Bucket to BigQuery. If you forgot it when you created them, you can create a new dataset on BigQuery using the same Region which you used on your GCS Bucket.\\nThis means that your GCS Bucket and the BigQuery dataset are placed in different regions. You have to create a new dataset inside BigQuery in the same region with your GCS bucket and store the data in the newly created dataset.',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):'},\n",
       "  {'text': \"Make sure to create the BigQuery dataset in the very same location that you've created the GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then BigQuery dataset must be created in the same region (us-central1, in this example)\",\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>'},\n",
       "  {'text': 'By the way, this isnt a problem/solution, but a useful hint:\\nPlease, remember to save your progress in BigQuery SQL Editor.\\nI was almost finishing the homework, when my Chrome Tab froze and I had to reload it. Then I lost my entire SQL script.\\nSave your script from time to time. Just click on the button at the top bar. Your saved file will be available on the left panel.\\nAlternatively, you can copy paste your queries into an .sql file in your preferred editor (Notepad++, VS Code, etc.). Using the .sql extension will provide convenient color formatting.',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Remember to save your queries'},\n",
       "  {'text': 'Ans :  While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Can I use BigQuery for real-time analytics in this project?'},\n",
       "  {'text': \"could not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\\nThis error is caused by invalid data in the timestamp column. A way to identify the problem is to define the schema from the external table using string datatype. This enables the queries to work at which point we can filter out the invalid rows from the import to the materialised table and insert the fields with the timestamp data type.\",\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage'},\n",
       "  {'text': 'Background:\\n`pd.read_parquet`\\n`pd.to_datetime`\\n`pq.write_to_dataset`\\nReference:\\nhttps://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible\\nhttps://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format\\nhttps://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1\\nSolution:\\nAdd `use_deprecated_int96_timestamps=True` to `pq.write_to_dataset` function, like below\\npq.write_to_dataset(\\ntable,\\nroot_path=root_path,\\nfilesystem=gcs,\\nuse_deprecated_int96_timestamps=True\\n# Write timestamps to INT96 Parquet format\\n)',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)'},\n",
       "  {'text': 'Solution:\\nIf youre using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nimport os\\nif \\'data_exporter\\' not in globals():\\nfrom mage_ai.data_preparation.decorators import data_exporter\\n# Replace with the location of your service account key JSON file.\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/personal-gcp.json\\'\\nbucket_name = \"<YOUR_BUCKET_NAME>\"\\nobject_key = \\'nyc_taxi_data_2022.parquet\\'\\nwhere = f\\'{bucket_name}/{object_key}\\'\\n@data_exporter\\ndef export_data(data, *args, **kwargs):\\ntable = pa.Table.from_pandas(data, preserve_index=False)\\ngcs = pa.fs.GcsFileSystem()\\npq.write_table(\\ntable,\\nwhere,\\n# Convert integer columns in Epoch milliseconds\\n# to Timestamp columns in microseconds (\\'us\\') so\\n# they can be loaded into BigQuery with the right\\n# data type\\ncoerce_timestamps=\\'us\\',\\nfilesystem=gcs\\n)\\nSolution 2:\\nIf youre using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won\\'t be converted to timestamp when loaded by BigQuery later on.\\nschema = pa.schema([\\n(\\'vendor_id\\', pa.int64()),\\n(\\'lpep_pickup_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'lpep_dropoff_datetime\\', pa.timestamp(\\'ns\\')),\\n(\\'store_and_fwd_flag\\', pa.string()),\\n(\\'ratecode_id\\', pa.int64()),\\n(\\'pu_location_id\\', pa.int64()),\\n(\\'do_location_id\\', pa.int64()),\\n(\\'passenger_count\\', pa.int64()),\\n(\\'trip_distance\\', pa.float64()),\\n(\\'fare_amount\\', pa.float64()),\\n(\\'extra\\', pa.float64()),\\n(\\'mta_tax\\', pa.float64()),\\n(\\'tip_amount\\', pa.float64()),\\n(\\'tolls_amount\\', pa.float64()),\\n(\\'improvement_surcharge\\', pa.float64()),\\n(\\'total_amount\\', pa.float64()),\\n(\\'payment_type\\', pa.int64()),\\n(\\'trip_type\\', pa.int64()),\\n(\\'congestion_surcharge\\', pa.float64()),\\n(\\'lpep_pickup_month\\', pa.int64())\\n])\\ntable = pa.Table.from_pandas(data, schema=schema)',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery'},\n",
       "  {'text': 'Reference:\\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage\\nSolution:\\nfrom google.cloud import bigquery\\n# Set table_id to the ID of the table to create\\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\\n# Construct a BigQuery client object\\nclient = bigquery.Client()\\n# Set the external source format of your table\\nexternal_source_format = \"PARQUET\"\\n# Set the source_uris to point to your data in Google Cloud\\nsource_uris = [ f\\'gs://{bucket_name}/{object_key}/*\\']\\n# Create ExternalConfig object with external source format\\nexternal_config = bigquery.ExternalConfig(external_source_format)\\n# Set source_uris that point to your data in Google Cloud\\nexternal_config.source_uris = source_uris\\nexternal_config.autodetect = True\\ntable = bigquery.Table(table_id)\\n# Set the external data configuration of the table\\ntable.external_data_configuration = external_config\\ntable = client.create_table(table)  # Make an API request.\\nprint(f\\'Created table with external source: {table_id}\\')\\nprint(f\\'Format: {table.external_data_configuration.source_format}\\')',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Create External Table using Python'},\n",
       "  {'text': 'Reference:\\nhttps://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser\\nSolution:\\nCombine with Create External Table using Python, use it before client.create_table function.\\ndef tableExists(tableID, client):\\n\"\"\"\\nCheck if a table already exists using the tableID.\\nreturn : (Boolean)\\n\"\"\"\\ntry:\\ntable = client.get_table(tableID)\\nreturn True\\nexcept Exception as e: # NotFound:\\nreturn False',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Check BigQuery Table Exist And Delete'},\n",
       "  {'text': 'To avoid this error you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\\n$ bq load  --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Error: Missing close double quote (\") character'},\n",
       "  {'text': 'Solution: This problem arises if your gcs and bigquery storage is in different regions.\\nOne potential way to solve it:\\nGo to your google cloud bucket and check the region in field named Location\\nNow in bigquery, click on three dot icon near your project name and select create dataset.\\nIn region filed choose the same regions as you saw in your google cloud bucket',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US'},\n",
       "  {'text': 'There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\\nUse below Cloud Function python script to load files directly to BigQuery. Use your project id, dataset id & table id as defined by you.\\nimport tempfile\\nimport requests\\nimport logging\\nfrom google.cloud import bigquery\\ndef hello_world(request):\\n# table_id = <project_id.dataset_id.table_id>\\ntable_id = \\'de-zoomcap-project.dezoomcamp.fhv-2019\\'\\n# Create a new BigQuery client\\nclient = bigquery.Client()\\nfor month in range(4, 13):\\n# Define the schema for the data in the CSV.gz files\\nurl = \\'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz\\'.format(month)\\n# Download the CSV.gz file from Github\\nresponse = requests.get(url)\\n# Create new table if loading first month data else append\\nwrite_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\\n# Defining LoadJobConfig with schema of table to prevent it from changing with every table\\njob_config = bigquery.LoadJobConfig(\\nschema=[\\nbigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\\nbigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\\nbigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\nbigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\nbigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\\n],\\nskip_leading_rows=1,\\nwrite_disposition=write_disposition_string,\\nautodetect=True,\\nsource_format=\"CSV\",\\n)\\n# Load the data into BigQuery\\n# Create a temporary file to prevent the exception- AttributeError: \\'bytes\\' object has no attribute \\'tell\\'\"\\nwith tempfile.NamedTemporaryFile() as f:\\nf.write(response.content)\\nf.seek(0)\\njob = client.load_table_from_file(\\nf,\\ntable_id,\\nlocation=\"US\",\\njob_config=job_config,\\n)\\njob.result()\\nlogging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\\nreturn \\'Data loaded into table {}.\\'.format(table_id)',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:'},\n",
       "  {'text': 'You need to uncheck cache preferences in query settings',\n",
       "   'section': 'Module 3: Data Warehousing',\n",
       "   'question': 'GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))'},\n",
       "  {'text': 'Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\\nSolution:\\nFix the data type issue in data pipeline\\nBefore injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\\nSomething like:\\ndf[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\\ndf[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\\nNOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'GCP BQ - How to handle type error from big query and parquet data?'},\n",
       "  {'text': 'Problem occurs when misplacing content after fro``m clause in BigQuery SQLs.\\nCheck to remove any extra apaces or any other symbols, keep in lowercases, digits and dashes only',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project'},\n",
       "  {'text': 'No. Based on the documentation for Bigquery, it does not support more than 1 column to be partitioned.\\n[source]',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'GCP BQ - Does BigQuery support multiple columns partition?'},\n",
       "  {'text': 'Error Message:\\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\\nSolution:\\nConvert the column to datetime first.\\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'GCP BQ - DATE() Error in BigQuery'},\n",
       "  {'text': 'Native tables are tables where the data is stored in BigQuery.  External tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\\nResources:\\nhttps://cloud.google.com/bigquery/docs/external-tables\\nhttps://cloud.google.com/bigquery/docs/tables-intro',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'GCP BQ - Native tables vs External tables in BigQuery?'},\n",
       "  {'text': 'Issue: Tried running command to export ML model from BQ to GCS from Week 3\\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\\nIt is failing on following error:\\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\\nI verified the BQ data set and gcs bucket are in the same region- us-west1. Not sure how it gets location US. I couldnt find the solution yet.\\nSolution:  Please enter correct project_id and gcs_bucket folder address. My gcs_bucket folder address is\\ngs://dtc_data_lake_optimum-airfoil-376815/tip_model',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS'},\n",
       "  {'text': \"To solve this error mention the location = US when creating the dim_zones table\\n{{ config(\\nmaterialized='table',\\nlocation='US'\\n) }}\\nJust Update this part to solve the issue and run the dim_zones again and then run the fact_trips\",\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql'},\n",
       "  {'text': 'Solution: proceed with setting up serving_dir on your computer as in the extract_model.md file. Then instead of\\ndocker pull tensorflow/serving\\nuse\\ndocker pull emacski/tensorflow-serving\\nThen\\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\\nThen run the curl command as written, and you should get a prediction.',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).'},\n",
       "  {'text': 'Try deleting data youve saved to your VM locally during ETLs\\nKill processes related to deleted files\\nDownload ncdu and look for large files (pay particular attention to files related to Prefect)\\nIf you delete any files related to Prefect, eliminate caching from your flow code',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'VMs - What do I do if my VM runs out of space?'},\n",
       "  {'text': \"Ans: What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files (but nothing like cleaning the data and putting it in parquet format)\",\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': \"Homework - What does it mean Stop with loading the files into a bucket.' Stop with loading the files into a bucket?\"},\n",
       "  {'text': 'If for whatever reason you try to read parquets directly from nyc.govs cloudfront into pandas, you might run into this error:\\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\\nCause:\\nthere is one errant data record where the dropOff_datetime was set to year 3019 instead of 2019.\\npandas uses timestamp[ns] (as noted above), and int64 only allows a ~580 year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`\\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of pd.Timestamp.Max\\nFix:\\nUse pyarrow to read it:\\nimport pyarrow.parquet as pq df = pq.read_table(\\'fhv_tripdata_2019-02.parquet\\').to_pandas(safe=False)\\nHowever this results in weird timestamps for the offending record\\nRead the datetime columns separately using pq.read_table\\n\\ntable = pq.read_table(taxi.parquet)\\ndatetimes = [list of datetime column names]\\ndf_dts = pd.DataFrame()\\nfor col in datetimes:\\ndf_dts[col] = pd.to_datetime(table .column(col), errors=\\'coerce\\')\\n\\nThe `errors=coerce` parameter will convert the out of bounds timestamps into either the max or the min\\nUse parquet.compute.filter to remove the offending rows\\n\\nimport pyarrow.compute as pc\\ntable = pq.read_table(\"taxi.parquet\")\\ndf = table.filter(\\npc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\\n).to_pandas()',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error'},\n",
       "  {'text': 'Answer: The 2022 NYC taxi data parquet files are available for each month separately. Therefore, you need to add all 12 files to your GCS bucket and then refer to them using the URIs option when creating an external table in BigQuery. You can use the wildcard \"*\" to refer to all 12 files using a single string.',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Question: for homework 3 , we need all 12 parquet files for green taxi 2022 right ?'},\n",
       "  {'text': 'This can help avoid schema issues in the homework. \\nDownload files locally and use the upload files button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Homework - Uploading files to GCS via GUI'},\n",
       "  {'text': 'Ans: Take a careful look at the format of the dates in the question.',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Homework - Qn 5: The partitioned/clustered table isnt giving me the prediction I expected'},\n",
       "  {'text': 'Many people arent getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?'},\n",
       "  {'text': 'UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\nSolution:\\nStep 1: When reading the data from the web into the pandas dataframe mention the encoding as follows:\\npd.read_csv(dataset_url, low_memory=False, encoding=\\'latin1\\')\\nStep 2: When writing the dataframe from the local system to GCS as a csv mention the encoding as follows:\\ndf.to_csv(path_on_gsc, compression=\"gzip\", encoding=\\'utf-8\\')\\nAlternative: use pd.read_parquet(url)',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Python - invalid start byte Error Message'},\n",
       "  {'text': 'A generator is a function in python that returns an iterator using the yield keyword.\\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Python - Generators in python'},\n",
       "  {'text': 'The read_parquet function supports a list of files as an argument. The list of files will be merged into a single result table.',\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Python - Easiest way to read multiple files at the same time?'},\n",
       "  {'text': \"Incorrect:\\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer) or\\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\\nCorrect:\\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\",\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': \"Python - These won't work. You need to make sure you use Int64:\"},\n",
       "  {'text': \"ValueError: Path /Users/kt/.prefect/storage/44ccce0813ed4f24ab2d3783de7a9c3a does not exist.\\nRemove ```cache_key_fn=task_input_hash ``` as its in argument in your function & run your flow again.\\nNote: catche key is beneficial if you happen to run the code multiple times, it won't repeat the process which you have finished running in the previous run.  That means, if you have this ```cache_key``` in your initial run, this might cause the error.\",\n",
       "   'section': \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\",\n",
       "   'question': 'Prefect - Error on Running Prefect Flow to Load data to GCS'},\n",
       "  {'text': '@task\\ndef download_file(url: str, file_path: str):\\nresponse = requests.get(url)\\nopen(file_path, \"wb\").write(response.content)\\nreturn file_path\\n@flow\\ndef extract_from_web() -> None:\\nfile_path = download_file(url=f\\'{url-filename}.csv.gz\\',file_path=f\\'{filename}.csv.gz\\')',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Prefect - Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).'},\n",
       "  {'text': 'Update the seed column types in the dbt_project.yaml file\\nfor using double : float\\nfor using int : numeric\\nDBT Cloud production error: prod dataset not available in location EU\\nProblem: I am trying to deploy my DBT  models to production, using DBT Cloud. The data should live in BigQuery.  The dataset location is EU.  However, when I am running the model in production, a prod dataset is being create in BigQuery with a location US and the dbt invoke build is failing giving me \"ERROR 404: porject.dataset:prod not available in location EU\". I tried different ways to fix this. I am not sure if there is a more simple solution then creating my project or buckets in location US. Hope anyone can help here.\\nNote: Everything is working fine in development mode, the issue is just happening when scheduling and running job in production\\nSolution: I created the prod dataset manually in BQ and specified EU, then I ran the job.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'If you are getting not found in location us error.'},\n",
       "  {'text': 'Error: This project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\\nThe error itself tells us how to solve this issue, the guide is here. And from videos @1:42 and also slack chat',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Setup - No development environment'},\n",
       "  {'text': \"Runtime Error\\ndbt was unable to connect to the specified database.\\nThe database returned the following error:\\n>Database Error\\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\\nCheck your database credentials and try again. For more information, visit:\\nhttps://docs.getdbt.com/docs/configure-your-profile\\nSteps to resolve error in Google Cloud:\\n1. Navigate to IAM & Admin and select IAM\\n2. Click Grant Access if your newly created dbt service account isn't listed\\n3. In New principals field, add your service account\\n4. Select a Role and search for BigQuery Job User to add\\n5. Go back to dbt cloud project setup and Test your connection\\n6. Note: Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Setup - Connecting dbt Cloud with BigQuery Error'},\n",
       "  {'text': 'error: This dbt Cloud run was cancelled because a valid dbt project was not found. Please check that the repository contains a proper dbt_project.yml config file. If your dbt project is located in a subdirectory of the connected repository, be sure to specify its location on the Project settings page in dbt Cloud',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Dbt build error'},\n",
       "  {'text': \"Error: Failed to clone repository.\\ngit clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/\\nCloning into '/usr/src/develop/...\\nWarning: Permanently added 'github.com,140.82.114.4' (ECDSA) to the list of known hosts.\\ngit@github.com: Permission denied (publickey).\\nfatal: Could not read from remote repository.\\nIssue: You dont have permissions to write to DataTalksClub/data-engineering-zoomcamp.git\\nSolution 1: Clone the repository and use this forked repo, which contains your github username. Then, proceed to specify the path, as in:\\n[your github username]/data-engineering-zoomcamp.git\\nSolution 2: create a fresh repo for dbt-lessons. Wed need to do branching and PRs in this lesson, so it might be a good idea to also not mess up your whole other repo. Then you dont have to create a subfolder for the dbt project files\\nSolution 3: Use https link\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Setup - Failed to clone repository.'},\n",
       "  {'text': \"Solution:\\nCheck if youre on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\\nSo If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\\nNote from another user: Im in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.'},\n",
       "  {'text': 'Issue: If the DBT cloud IDE loading indefinitely then giving you this error\\nSolution: check the dbt_cloud_setup.md  file and make a SSH Key and use gitclone to import repo into dbt project, copy and paste deploy key back in your repo setting.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Setup - Your IDE session was unable to start. Please contact support.'},\n",
       "  {'text': 'Issue: If you dont define the column format while converting from csv to parquet Python will choose based on the first rows.\\nSolution: Defined the schema while running web_to_gcp.py pipeline.\\nSebastian adapted the script:\\nhttps://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py\\nNeed a quick change to make the file work with gz files, added the following lines (and dont forget to delete the file at the end of each iteration of the loop to avoid any problem of disk space)\\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\\nopen(file_name_gz, \\'wb\\').write(r.content)\\nos.system(f\"gzip -d {file_name_gz}\")\\nos.system(f\"rm {file_name_init}.*\")\\nSame ERROR - When running dbt run for fact_trips.sql, the task failed with error:\\nParquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64\\n\\nCtrl+Alt+ZCtrl+\\n\\nReason: Parquet files have their own schema. Some parquet files for green data have records with decimals in ehail_fee column.\\nThere are some possible fixes:\\nDrop ehail_feel column since it is not really used. For instance when creating a partitioned table from the external table in BigQuery\\nSELECT * EXCEPT (ehail_fee) FROM\\nModify stg_green_tripdata.sql model using this line cast(0 as numeric) as ehail_fee.\\nModify Airflow dag to make the conversion and avoid the error.\\npv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types = {\\'ehail_fee\\': \\'float64\\'}))\\nSame type of ERROR - parquet files with different data types - Fix it with pandas\\nHere is another possibility that could be interesting:\\nYou can specify the dtypes when importing the file from csv to a dataframe with pandas\\npd.from_csv(..., dtype=type_dict)\\nOne obstacle is that the regular int64 pandas use (I think this is from the numpy library) does not accept null values (NaN, not a number). But you can use the pandas Int64 instead, notice capital I. The type_dict is a python dictionary mapping the column names to the dtypes.\\nSources:\\nhttps://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\\nNullable integer data type  pandas 1.5.3 documentation',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'DBT - I am having problems with columns datatype while running DBT/BigQuery'},\n",
       "  {'text': 'If the provided URL isnt working for you (https://nyc-tlc.s3.amazonaws.com/trip+data/):\\nWe can use the GitHub CLI to easily download the needed trip data from https://github.com/DataTalksClub/nyc-tlc-data, and manually upload to a GCS bucket.\\nInstructions on how to download the CLI here: https://github.com/cli/cli\\nCommands to use:\\ngh auth login\\ngh release list -R DataTalksClub/nyc-tlc-data\\ngh release download yellow -R DataTalksClub/nyc-tlc-data\\ngh release download green -R DataTalksClub/nyc-tlc-data\\netc.\\nNow you can upload the files to a GCS bucket using the GUI.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket'},\n",
       "  {'text': \"R: This conversion is needed for the question 3 of homework, in order to process files for fhv data. The error is:\\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 7 columns, got 1: B02765\\nCause: Some random line breaks in this particular file.\\nFixed by opening a bash in the container executing the dag and manually running the following command that deletes all \\\\n not preceded by \\\\r.\\nperl -i -pe 's/(?<!\\\\r)\\\\n/\\\\1/g' fhv_tripdata_2020-01.csv\\nAfter that, clear the failed task in Airflow to force re-execution.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Ingestion - Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow'},\n",
       "  {'text': 'I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main  DataTalksClub/data-engineering-bootcamp (github.com)\\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\\nThen I found another hack shared in the slack which was suggested by Victoria.\\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\\nPlease watch until the end as there is few schema changes required to be done',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Hack to load yellow and green trip data for 2019 and 2020'},\n",
       "  {'text': 'gs\\\\storage_link\\\\*.parquet need to be added in destination folder',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Move many files (more than one) from Google cloud storage bucket to Big query'},\n",
       "  {'text': 'One common cause experienced is lack of space after running prefect several times. When running prefect, check the folder .prefect/storage and delete the logs now and then to avoid the problem.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'GCP VM - All of sudden ssh stopped working for my VM after my last restart'},\n",
       "  {'text': 'You can try to do this steps:',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)'},\n",
       "  {'text': 'R: Go to BigQuery, and check the location of BOTH\\nThe source dataset (trips_data_all), and\\nThe schema youre trying to write to (name should be \\tdbt_<first initial><last name> (if you didnt change the default settings at the end when setting up your project))\\nLikely, your source data will be in your region, but the write location will be a multi-regional location (US in this example). Delete these datasets, and recreate them with your specified region and the correct naming format.\\nAlternatively, instead of removing datasets, you can specify the single-region location you are using. E.g. instead of location: US, specify the region, so location: US-east1. See this Github comment for more detail. Additionally please see this post of Sandy\\nIn DBT cloud you can actually specify the location using the following steps:\\nGPo to your profile page (top right drop-down --> profile)\\nThen go to under Credentials --> Analytics (you may have customised this name)\\nClick on Bigquery >\\nHit Edit\\nUpdate your location, you may need to re-upload your service account JSON to re-fetch your private key, and save. (NOTE: be sure to exactly copy the region BigQuery specifies your dataset is in.)',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': '404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1'},\n",
       "  {'text': 'Error: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\\nFix:\\nReplace dbt_utils.surrogate_key  with dbt_utils.generate_surrogate_key in stg_green_tripdata.sql\\nWhen executing dbt run after fact_trips.sql has been created, the task failed with error:\\nR: Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\\n1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\\n2. Add the related roles to the service account in use in GCS.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated'},\n",
       "  {'text': 'You need to create packages.yml file in main project directory and add packages meta data:\\npackages:\\n- package: dbt-labs/dbt_utils\\nversion: 0.8.0\\nAfter creating file run:\\nAnd hit enter.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'When You are getting error dbt_utils not found'},\n",
       "  {'text': \"Ensure you properly format your yml file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the --vars '{'is_test_run': 'false'}')  and click on any stages logs to expand and read errors messages or warnings.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.'},\n",
       "  {'text': \"Make sure you use:\\ndbt run --var is_test_run: false or\\ndbt build --var is_test_run: false\\n(watch out for formatted text from this document: re-type the single quotes). If that does not work, use --vars '{'is_test_run': 'false'}' with each phrase separately quoted.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Build - Why do my Fact_trips only contain a few days of data?'},\n",
       "  {'text': 'Check if you specified if_exists argument correctly when writing data from GCS to BigQuery. When I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data I had specified if_exists=\"replace\" while I was experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020 make sure to set if_exists=\"append\"\\nif_exists=\"replace\" will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted)\\nif_exists=\"append\" will append the new monthly data -> you end up with data from all months',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Build - Why do my fact_trips only contain one month of data?'},\n",
       "  {'text': \"R: After the second SELECT, change this line:\\ndate_trunc('month', pickup_datetime) as revenue_month,\\nTo this line:\\ndate_trunc(pickup_datetime, month) as revenue_month,\\nMake sure that month isnt surrounded by quotes!\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.'},\n",
       "  {'text': 'For this instead:\\n{{ dbt_utils.generate_surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     ,\\n     field_z\\n]) }}',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Replace: \\n{{ dbt_utils.surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     ,\\n     field_z     \\n]) }}'},\n",
       "  {'text': 'Remove the dataset from BigQuery which was created by dbt and run dbt run again so that it will recreate the dataset in BigQuery with the correct location',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'I changed location in dbt, but dbt run still gives me an error'},\n",
       "  {'text': 'Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\nDBT - Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\nAnswer: when you create the CI/CD job, under Compare Changes against an environment (Deferral) make sure that you select  No; do not defer to another environment - otherwise dbt wont merge your dev models into production models; it will create a new environment called dbt_cloud_pr_number of pull request',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.'},\n",
       "  {'text': \"Vic created three different datasets in the videos.. dbt_<name> was used for development and you used a production dataset for the production environment. What was the use for the staging dataset?\\nR: Staging, as the name suggests, is like an intermediate between the raw datasets and the fact and dim tables, which are the finished product, so to speak. You'll notice that the datasets in staging are materialised as views and not tables.\\nVic didn't use it for the project, you just need to create production and dbt_name + trips_data_all that you had already.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Why do we need the Staging dataset?'},\n",
       "  {'text': 'Try removing the network: host line in docker-compose.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'DBT Docs Served but Not Accessible via Browser'},\n",
       "  {'text': 'Go to Account settings >> Project >> Analytics >> Click on your connection >> go all the way down to Location and type in the GCP location just as displayed in GCP (e.g. europe-west6). You might need to reupload your GCP key.\\nDelete your dataset in GBQ\\nRebuild project: dbt build\\nNewly built dataset should be in the correct location',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6'},\n",
       "  {'text': 'Create a new branch to edit. More on this can be found here in the dbt docs.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Dbt+git - Main branch is read-only'},\n",
       "  {'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the main branch.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\"},\n",
       "  {'text': \"Error:\\nTriggered by pull requests\\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\\nSolution: Contrary to the guide on DTC repo, dont use the Git Clone option. Use the Github one instead. Step-by-step guide to UN-LINK Git Clone and RE-LINK with Github in the next entry below\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat'},\n",
       "  {'text': 'If youre trying to configure CI with Github and on the jobs options you cant see Run on Pull Requests? on triggers, you have to reconnect with Github using native connection instead clone by SSH. Follow these steps:\\nOn Profile Settings > Linked Accounts connect your Github account with dbt project allowing the permissions asked. More info at https://docs.getdbt.com/docs/collaborate/git/connect-gith\\nDisconnect your current Githubs configuration from Account Settings > Projects (analytics) > Github connection. At the bottom left appears the button Disconnect, press it.\\nOnce we have confirmed the change, we can configure it again. This time, choose Github and it will appear in all repositories which you have allowed to work with dbt. Select your repository and its ready.\\nGo to the Deploy > job configurations page and go down until Triggers and now you can see the option Run on Pull Requests:',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github'},\n",
       "  {'text': \"If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may have encountered an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image. Don't worry - a quick fix for this is to simply save your schema.yml file. Once you've done this, you should be able to view your Lineage graph without any further issues.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)\"},\n",
       "  {'text': '> in macro test_accepted_values (tests/generic/builtin.sql)\\n> called by test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)\\nRemember that you have to add to dbt_project.yml the vars:\\nvars:\\npayment_type_values: [1, 2, 3, 4, 5, 6]',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"'NoneType' object is not iterable\"},\n",
       "  {'text': \"You will face this issue if you copied and pasted the exact macro directly from data-engineering-zoomcamp repo.\\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\\nWhat youd have to do is to change the data type of the numbers (1, 2, 3 etc.) to text by inserting , as the initial payment_type data type should be string (Note: I extracted and loaded the green trips data using Google BQ Marketplace)\\n{#\\nThis macro returns the description of the payment_type\\n#}\\n{% macro get_payment_type_description(payment_type) -%}\\ncase {{ payment_type }}\\nwhen '1' then 'Credit card'\\nwhen '2' then 'Cash'\\nwhen '3' then 'No charge'\\nwhen '4' then 'Dispute'\\nwhen '5' then 'Unknown'\\nwhen '6' then 'Voided trip'\\nend\\n{%- endmacro %}\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'dbt macro errors with get_payment_type_description(payment_type)'},\n",
       "  {'text': 'The dbt error  log contains a link to BigQuery. When you follow it you will see your query and the problematic line will be highlighted.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Troubleshooting in dbt:'},\n",
       "  {'text': 'It is a default behaviour of dbt to append custom schema to initial schema. To override this behaviour simply create a macro named generate_schema_name.sql:\\n{% macro generate_schema_name(custom_schema_name, node) -%}\\n{%- set default_schema = target.schema -%}\\n{%- if custom_schema_name is none -%}\\n{{ default_schema }}\\n{%- else -%}\\n{{ custom_schema_name | trim }}\\n{%- endif -%}\\n{%- endmacro %}\\nNow you can override default custom schema in dbt_project.yml:',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Why changing the target schema to marts actually creates a schema named dbt_marts instead?'},\n",
       "  {'text': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'How to set subdirectory of the github repository as the dbt project root'},\n",
       "  {'text': \"Remember that you should modify accordingly your .sql models, to read from existing table names in BigQuery/postgres db\\nExample: select * from {{ source('staging',<your table name in the database>') }}\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\"},\n",
       "  {'text': 'Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your seeds folder if the seed file is inside it.\\nAnother thing to check is your .gitignore file. Make sure that the .csv extension is not included.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)\"},\n",
       "  {'text': '1. Go to your dbt cloud service account\\n1. Adding the  [Storage Object Admin,Storage Admin] role in addition tco BigQuery Admin.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'When executing dbt run after using fhv_tripdata as an external table: you get Access Denied: BigQuery BigQuery: Permission denied'},\n",
       "  {'text': 'Problem: when injecting data to bigquery, you may face the type error. This is because pandas by default will parse integer columns with missing value as float type.\\nSolution:\\nOne way to solve this problem is to specify/ cast data type Int64 during the data transformation stage.\\nHowever, you may be lazy to type all the int columns. If that is the case, you can simply use convert_dtypes to infer the data type\\n# Make pandas to infer correct data type (as pandas parse int with missing as float)\\ndf.fillna(-999999, inplace=True)\\ndf = df.convert_dtypes()\\ndf = df.replace(-999999, None)',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'How to automatically infer the column data type (pandas missing value issues)?'},\n",
       "  {'text': 'Seed files loaded from directory with name seed, thats why you should rename dir with name data to seed',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'When loading github repo raise exception that taxi_zone_lookup not found'},\n",
       "  {'text': 'Check the .gitignore file and make sure you dont have *.csv in it\\n\\nDbt error 404 was not found in location\\nMy specific error:\\nRuntime Error in rpc request (from remote system.sql) 404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6 Location: europe-west6 Job ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\nMake sure all of your datasets have the correct region and not a generalised region:\\nEurope-west6 as opposed to EU\\n\\nMatch this in dbt settings:\\ndbt -> projects -> optional settings -> manually set location to match',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'taxi_zone_lookup not found'},\n",
       "  {'text': \"The easiest way to avoid these errors is by ingesting the relevant data in a .csv.gz file type. Then, do:\\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\\nOPTIONS (\\nformat = 'CSV',\\nuris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n);\\nAs an example. You should no longer have any data type issues for week 4.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Data type errors when ingesting with parquet files'},\n",
       "  {'text': 'This is due to the way the deduplication is done in the two staging files.\\nSolution: add order by in the partition by part of both staging files. Keep adding columns to order by until the number of rows in the fact_trips table is consistent when re-running the fact_trips model.\\nExplanation (a bit convoluted, feel free to clarify, correct etc.)\\nWe partition by vendor id and pickup_datetime and choose the first row (rn=1) from all these partitions. These partitions are not ordered, so every time we run this, the first row might be a different one. Since the first row is different between runs, it might or might not contain an unknown borough. Then, in the fact_trips model we will discard a different number of rows when we discard all values with an unknown borough.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Inconsistent number of rows when re-running fact_trips model'},\n",
       "  {'text': 'If you encounter data type error on trip_type column, it may due to some nan values that isnt null in bigquery.\\nSolution: try casting it to FLOAT datatype instead of NUMERIC',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Data Type Error when running fact table'},\n",
       "  {'text': \"This error could result if you are using some select * query without mentioning the name of table for ex:\\nwith dim_zones as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\nwhere borough != 'Unknown'\\n),\\nfhv as (\\nselect * from `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n)\\nselect * from fhv\\ninner join dim_zones as pickup_zone\\non fhv.PUlocationID = pickup_zone.locationid\\ninner join dim_zones as dropoff_zone\\non fhv.DOlocationID = dropoff_zone.locationid\\n);\\nTo resolve just replace use : select fhv.* from fhv\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'CREATE TABLE has columns with duplicate name locationid.'},\n",
       "  {'text': 'Some ehail fees are null and casting them to integer gives Bad int64 value: 0.0 error,\\nSolution:\\nUsing safe_cast returns NULL instead of throwing an error. So use safe_cast from dbt_utils function in the jinja code for casting into integer as follows:\\n{{ dbt_utils.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"integer\"))}} as ehail_fee,\\nCan also just use safe_cast(ehail_fee as integer) without relying on dbt_utils.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Bad int64 value: 0.0 error'},\n",
       "  {'text': \"You might encounter this when building the fact_trips.sql model. The issue may be with the payment_type_description field.\\nUsing safe_cast as above, would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer.\\ncast(replace({{ payment_type }},'.0','') as integer)\\nBad int64 value: 1.0 error (again)\\n\\nI found that there are more columns causing the bad INT64: ratecodeid and trip_type on Green_tripdata table.\\nYou can use the queries below to address them:\\nCAST(\\nREGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', '') AS INT64\\n) AS ratecodeid,\\nCAST(\\nCASE\\nWHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\\\.\\\\d+') THEN NULL\\nELSE CAST(trip_type AS INT64)\\nEND AS INT64\\n) AS trip_type,\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Bad int64 value: 2.0/1.0 error'},\n",
       "  {'text': 'The two solution above dont work for me - I used the line below in `stg_green_trips.sql` to replace the original ehail_fee line:\\n`{{ dbt.safe_cast(\\'ehail_fee\\',  api.Column.translate_type(\"numeric\"))}} as ehail_fee,`',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'DBT - Error on building fact_trips.sql: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")'},\n",
       "  {'text': \"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\\nIt should be:\\ndbt run --var 'is_test_run: false'\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'The - vars argument must be a YAML dictionary, but was of type str'},\n",
       "  {'text': \"You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.'\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Not able to change Environment Type as it is greyed out and inaccessible'},\n",
       "  {'text': 'Database Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\nIn my case, I was set up in a different branch, so always check the branch you are working on. Change the 04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml file in the\\nsources:\\n- name: staging\\ndatabase: your_database_name\\nIf this error will continue when running dbt job, As for changing the branch for your job, you can use the Custom Branch settings in your dbt Cloud environment. This allows you to run your job on a different branch than the default one (usually main). To do this, you need to:\\nGo to an environment and select Settings to edit it\\nSelect Only run on a custom branch in General settings\\nEnter the name of your custom branch (e.g. HW)\\nClick Save\\nCould not parse the dbt project. please check that the repository contains a valid dbt project\\nRunning the Environment on the master branch causes this error, you must activate Only run on a custom branch checkbox and specify the branch you are  working when Environment is setup.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Access Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.'},\n",
       "  {'text': 'Change to main branch, make a pull request from the development branch.\\nNote: this will take you to github.\\nApprove the merging and rerun you job, it would work as planned now',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Made change to your modelling files and commit the your development branch, but Job still runs on old file?'},\n",
       "  {'text': 'Before you can develop some data model on dbt, you should create development environment and set some parameter on it. After the model being developed, we should also create deployment environment to create and run some jobs.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Setup - Ive set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?'},\n",
       "  {'text': 'Error Message:\\nInvestigate Sentry error: ProtocolError \"Invalid input ConnectionInputs.SEND_HEADERS in state ConnectionState.CLOSED\"\\nSolution:\\nreference\\nRun it again because it happens sometimes. Or wait a few minutes, it will continue.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError'},\n",
       "  {'text': \"My taxi data was loaded into gcs with etl_web_to_gcs.py script that converts csv data into parquet. Then I placed raw data trips into external tables and when I executed dbt run I got an error message: Parquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE. It is because several columns in files have different formats of data.\\nWhen I added df[col] = df[col].astype('Int64') transformation to the columns: passenger_count, payment_type, RatecodeID, VendorID, trip_type it went ok. Several people also faced this error and more about it you can read on the slack channel.\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'BigQuery returns an error when i try to run dbt run:'},\n",
       "  {'text': 'Use the syntax below instead if the code in the tutorial is not working.\\ndbt run --select stg_green_tripdata --vars \\'{\"is_test_run\": false}\\'',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:\"},\n",
       "  {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\"},\n",
       "  {'text': \"If you have problems editing dbt_project.yml when using Docker after docker-compose run dbt-bq-dtc init, to change profile taxi_rides_ny to 'bq-dbt-workshop, just run:\\nsudo chown -R username path\\nDBT - Internal Error: Profile should not be None if loading is completed\\nWhen  running dbt debug, change the directory to the newly created subdirectory (e.g: the newly created `taxi_rides_ny` directory, which contains the dbt project).\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': '\\u200b\\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)'},\n",
       "  {'text': 'When running a query on BigQuery sometimes could appear a this table is not on the specified location error.\\nFor this problem there is not a straightforward solution, you need to dig a little, but the problem could be one of these:\\nCheck the locations of your bucket, datasets and tables. Make sure they are all on the same one.\\nChange the query settings to the location you are in: on the query window select more -> query settings -> select the location\\nCheck if all the paths you are using in your query to your tables are correct: you can click on the table -> details -> and copy the path.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Google Cloud BigQuery Location Problems'},\n",
       "  {'text': 'This happens because we have moved the dbt project to another directory on our repo.\\nOr might be that youre on a different branch than is expected to be merged from / to.\\nSolution:\\nGo to the projects window on dbt cloud -> settings -> edit -> and add directory (the extra path to the dbt project)\\nFor example:\\n/week5/taxi_rides_ny\\nMake sure your file explorer path and this Project settings path matches and theres no files waiting to be committed to github if youre running the job to deploy to PROD.\\nAnd that you had setup the PROD environment to check in the main branch, or whichever you specified.\\nIn the picture below, I had set it to ella2024 to be checked as production-ready by the freshness check mark at the PROD environment settings. So each time I merge a branch from something else into ella2024 and then trigger the PR, the CI check job would kick-in. But we still do need to Merge and close the PR manually, I believe, that part is not automated.\\nYou set up the PROD custom branch (if not default main) in the Environment setup screen.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found.'},\n",
       "  {'text': 'When you are creating the pull request and running the CI, dbt is creating a new schema on BIgQuery. By default that new schema will be created on US location, if you have your dataset, schemas and tables on EU that will generate an error and the pull request will not be accepted. To change that location to EU on the connection to BigQuery from dbt we need to add the location EU on the connection optional settings:\\nDbt -> project -> settings -> connection BIgQuery -> OPtional Settings -> Location -> EU',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'DBT Deploy + CI - Location Problems on BigQuery'},\n",
       "  {'text': 'When running trying to run the dbt project on prod there is some things you need to do and check on your own:\\nFirst Make the pull request and Merge the branch into the main.\\nMake sure you have the latest version, if you made changes to the repo in another place.\\nCheck if the dbt_project.yml file is accessible to the project, if not check this solution (Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.).\\nCheck if the name you gave to the dataset on BigQuery is the same you put on the dataset spot on the production environment created on dbt cloud.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'DBT Deploy - Error When trying to run the dbt project on Prod'},\n",
       "  {'text': 'In the step in this video (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying dataset not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\\nSolution:\\nTurns out I forgot to specify Location to be `EU` when adding connection details.\\nDevelop -> Configure Cloud CLI -> Projects -> taxi_rides_ny -> (connection) Bigquery -> Edit -> Location (Optional) -> type `EU` -> Save',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'DBT - Error: 404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU after building from stg_green_tripdata.sql'},\n",
       "  {'text': 'Issue: If youre having problems loading the FHV_20?? data from the github repo into GCS and then into BQ (input file not of type parquet), you need to do two things. First, append the URL Template link with ?raw=true like so:\\nURL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\\\\\'%Y-%m\\\\\\') }}.parquet?raw=true\"\\nSecond, update make sure the URL_PREFIX is set to the following value:\\n\\nURL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\nIt is critical that you use this link with the keyword blob. If your link has tree here, replace it. Everything else can stay the same, including the curl -sSLf command. ',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Homework - Ingesting FHV_20?? data'},\n",
       "  {'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Homework - Ingesting NYC TLC Data'},\n",
       "  {'text': 'If you have to securely put your credentials for a project and, probably, push it to a git repository then the best option is to use an environment variable\\nFor example for web_to_gcs.py or git_csv_to_gcs.py we have to set these variables:\\nGOOGLE_APPLICATION_CREDENTIALS\\nGCP_GCS_BUCKET\\nThe easises option to do it  is to use .env  (dotenv).\\nInstall it and add a few lines of code that inject these variables for your project\\npip install python-dotenv\\nfrom dotenv import load_dotenv\\nimport os\\n# Load environment variables from .env file\\nload_dotenv()\\n# Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\ncredentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\nBUCKET = os.environ.get(\"GCP_GCS_BUCKET\")',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'How to set environment variable easily for any credentials'},\n",
       "  {'text': \"If you uploaded manually the fvh 2019 csv files, you may face errors regarding date types. Try to create an the external table in bigquery but define the pickup_datetime and dropoff_datetime to be strings\\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\\ndispatching_base_num STRING,\\npickup_datetime STRING,\\ndropoff_datetime STRING,\\nPUlocationID STRING,\\nDOlocationID STRING,\\nSR_Flag STRING,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'csv',\\nuris = ['gs://bucket/*.csv']\\n);\\nThen when creating the fhv core model in dbt, use TIMESTAMP(CAST(()) to ensure it first parses as a string and then convert it to timestamp.\\nwith fhv_tripdata as (\\nselect * from {{ ref('stg_fhv_tripdata') }}\\n),\\ndim_zones as (\\nselect * from {{ ref('dim_zones') }}\\nwhere borough != 'Unknown'\\n)\\nselect fhv_tripdata.dispatching_base_num,\\nTIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\\nTIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\"},\n",
       "  {'text': \"If you uploaded manually the fvh 2019 parquet files manually after downloading from https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet you may face errors regarding date types while loading the data in a landing table (say fhv_tripdata). Try to create an the external table with the schema defines as following and load each month in a loop.\\n-----Correct load with schema defination----will not throw error----------------------\\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\\ndispatching_base_num STRING,\\npickup_datetime TIMESTAMP,\\ndropoff_datetime TIMESTAMP,\\nPUlocationID FLOAT64,\\nDOlocationID FLOAT64,\\nSR_Flag FLOAT64,\\nAffiliated_base_number STRING\\n)\\nOPTIONS (\\nformat = 'PARQUET',\\nuris = ['gs://project id/fhv_2019_8.parquet']\\n);\\nCan Also USE  uris = ['gs://project id/fhv_2019_*.parquet'] (THIS WILL remove the need for the loop and can be done for all month in single RUN )\\n THANKYOU FOR THIS \",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldnt parse datetime column as timestamp,couldnt handle NULL values in PULocationID,DOLocationID'},\n",
       "  {'text': 'When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\\nInstead, navigate to https://lookerstudio.google.com/navigation/reporting which will take you to the free version.',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'Google Looker Studio - you have used up your 30-day trial'},\n",
       "  {'text': 'Ans: Dbt provides a mechanism called \"ref\" to manage dependencies between models. By referencing other models using the \"ref\" keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.\\nLoading FHV Data goes into slumber using Mage?\\nTry loading the data using jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\\nLoad the data into a pandas dataframe using the urls, make necessary transformations, upload the gcp bucket / alternatively download the parquet/csv files locally and then upload to GCP manually.\\nRegion Mismatch in DBT and BigQuery\\nIf you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as US by default and hence it is much easier to set your dbt profile location as US while transforming the tables and views. \\nYou can change the location as follows:',\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'How does dbt handle dependencies between models?'},\n",
       "  {'text': \"Use the PostgreSQL COPY FROM feature that is compatible with csv files\\nCOPY table_name [ ( column_name [, ...] ) ]\\nFROM { 'filename' | PROGRAM 'command' | STDIN }\\n[ [ WITH ] ( option [, ...] ) ]\\n[ WHERE condition ]\",\n",
       "   'section': 'Module 4: analytics engineering with dbt',\n",
       "   'question': 'What is the fastest way to upload taxi data to dbt-postgres?'},\n",
       "  {'text': 'Update the line:\\nWith:',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \"PROFILE_NAME\", target: \\'dev\\', invalid: \\'5432\\'is not of type \\'integer\\''},\n",
       "  {'text': 'Install SDKMAN:\\ncurl -s \"https://get.sdkman.io\" | bash\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nUsing SDKMAN, install Java 11 and Spark 3.3.2:\\nsdk install java 11.0.22-tem\\nsdk install spark 3.3.2\\nOpen a new terminal or run the following in the same shell:\\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\\nVerify the locations and versions of Java and Spark that were installed:\\necho $JAVA_HOME\\njava -version\\necho $SPARK_HOME\\nspark-submit --version',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)'},\n",
       "  {'text': 'If youre seriously struggling to set things up \"locally\" (here locally meaning non/partly-managed environment like own laptop, a VM or Codespaces) you can use the following guide to use Spark in Google Colab:\\nhttps://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304\\nStarter notebook:\\nhttps://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb\\nIts advisable to spend some time setting things up locally rather than jumping right into this solution.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'PySpark - Setting Spark up in Google Colab'},\n",
       "  {'text': 'If after installing Java (either jdk or openjdk), Hadoop and Spark, and setting the corresponding environment variables you find the following error when spark-shell is run at CMD:\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed\\nmodule @0x3c947bc5\\nSolution: Java 17 or 19 is not supported by Spark. Spark 3.x: requires Java 8/11/16. Install Java 11 from the website provided in the windows.md setup file.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Spark-shell: unable to load native-hadoop library for platform - Windows'},\n",
       "  {'text': 'I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\\nSolution:\\npip install findspark on the command line inside proper environment\\nAdd to the top of the script\\nimport findspark\\nfindspark.init()',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.'},\n",
       "  {'text': 'This is because Python 3.11 has some inconsistencies with such an old version of Spark. The solution is a downgrade in the Python version. Python 3.9 using a conda environment takes care of it. Or install newer PySpark >= 3.5.1 works for me (Ella) [source].',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)'},\n",
       "  {'text': 'If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\\nInstall OpenJDK 11,\\non MacOS: $ brew install java11\\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\nto ~/.bashrc or ~/zshrc\\nActivate working environment (by pipenv / poetry / conda)\\nRun $ pip install pyspark\\nWork with exercises as normal\\nAll default commands of spark will be also available at shell session under activated enviroment.\\nHope this can help!\\nP.s. you wont need findspark to firstly initialize.\\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\nIf you\\'re getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You\\'re most likely using incompatible versions of the JDK or Python with Spark.\\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\\n\\n$ sdk install java 17.0.10-librca\\n$ sdk install spark 3.5.0\\n$ sdk install hadoop 3.3.5\\nAs PySpark 3.5.0 supports Python 3.8+ make sure you\\'re setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\\n\\n\\n$ conda create -n ENV_NAME python=3.11\\n$ conda activate ENV_NAME\\n$ pip install pyspark==3.5.0\\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet(\\'zones\\')(On window)\\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\\nHere my\\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)'},\n",
       "  {'text': 'After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder \\\\\\n.master(\"local[*]\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\ndf = spark.read \\\\\\n.option(\"header\", \"true\") \\\\\\n.csv(\\'taxi+_zone_lookup.csv\\')\\ndf.show()\\nit gives the error:\\nRuntimeError: Java gateway process exited before sending its port number\\nThe solution (for me) was:\\npip install findspark on the command line and then\\nAdd\\nimport findspark\\nfindspark.init()\\nto the top of the script.\\nAnother possible solution is:\\nCheck that pyspark is pointing to the correct location.\\nRun pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\\nIf it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\\nTo add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\\nOnce everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'lsRuntimeError: Java gateway process exited before sending its port number'},\n",
       "  {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Module Not Found Error in Jupyter Notebook .'},\n",
       "  {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of py4j of the spark youre using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\"},\n",
       "  {'text': 'If below does not work, then download the latest available py4j version with\\nconda install -c conda-forge py4j\\nTake care of the latest version number in the website to replace appropriately.\\nNow add\\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\nexport PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\nin your  .bashrc file.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\"},\n",
       "  {'text': 'Even after we have exported our paths correctly you may find that  even though Jupyter is installed you might not have Jupyter Noteboopgak for one reason or another. Full instructions are found here (for my walkthrough) or here (where I got the original instructions from) but are included below. These instructions include setting up a virtual environment (handy if you are on your own machine doing this and not a VM):\\nFull steps:\\nUpdate and upgrade packages:\\nsudo apt update && sudo apt -y upgrade\\nInstall Python:\\nsudo apt install python3-pip python3-dev\\nInstall Python virtualenv:\\nsudo -H pip3 install --upgrade pip\\nsudo -H pip3 install virtualenv\\nCreate a Python Virtual Environment:\\nmkdir notebook\\ncd notebook\\nvirtualenv jupyterenv\\nsource jupyterenv/bin/activate\\nInstall Jupyter Notebook:\\npip install jupyter\\nRun Jupyter Notebook:\\njupyter notebook',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Exception: Jupyter command `jupyter-notebook` not found.'},\n",
       "  {'text': 'Code executed:\\ndf = spark.read.parquet(pq_path)\\n some operations on df \\ndf.write.parquet(pq_path, mode=\"overwrite\")\\njava.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\\nThe problem is that Sparks performs lazy transformations, so the actual action that trigger the job is df.write, which does delete the parquet files that is trying to read (mode=overwrite)\\nSolution: Write to a different directorydf\\ndf.write.parquet(pq_path_temp, mode=\"overwrite\")',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Error java.io.FileNotFoundException'},\n",
       "  {'text': 'You need to create the Hadoop /bin directory manually and add the downloaded files in there, since the shell script provided for Windows installation just puts them in /c/tools/hadoop-3.2.0/ .',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)'},\n",
       "  {'text': 'Actually Spark SQL is one independent type of SQL - Spark SQL.\\nThe several SQL providers are very similar:\\nSELECT [attributes]\\nFROM [table]\\nWHERE [filter]\\nGROUP BY [grouping attributes]\\nHAVING [filtering the groups]\\nORDER BY [attribute to order]\\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\nON [attributes table joining table2] (...)\\nWhat differs the most between several SQL providers are built-in functions.\\nFor Built-in Spark SQL function check this link: https://spark.apache.org/docs/latest/api/sql/index.html\\nExtra information on SPARK SQL :\\nhttps://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?'},\n",
       "  {'text': \"Solution: I had two notebooks running, and the one I wanted to look at had opened a port on localhost:4041.\\nIf a port is in use, then Spark uses the next available port number. It can be even 4044. Clean up after yourself when a port does not work or a container does not run.\\nYou can run spark.sparkContext.uiWebUrl\\nand result will be some like\\n'http://172.19.10.61:4041'\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'The spark viewer on localhost:4040 was not showing the current run'},\n",
       "  {'text': 'Solution: replace Java Developer Kit 11 with Java Developer Kit 8.\\nJava - RuntimeError: Java gateway process exited before sending its port number\\nShows java_home is not set on the notebook log\\nhttps://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/\\nhttps://twitter.com/drkrishnaanand/status/1765423415878463839',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)'},\n",
       "  {'text': 'I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1\\nI also added the google_credentials.json and .p12 to auth with gcs. These files are downloadable from GCP Service account.\\nTo create the SparkSession:\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n.appName(\\'spark-read-from-bigquery\\') \\\\\\n.config(\\'BigQueryProjectId\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\\'BigQueryDatasetLocation\\',\\'de_final_data\\') \\\\\\n.config(\\'parentProject\\',\\'razor-project-xxxxxxx) \\\\\\n.config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n.config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n.config(\"spark.driver.memory\", \"4g\") \\\\\\n.config(\"spark.executor.memory\", \"2g\") \\\\\\n.config(\"spark.memory.offHeap.enabled\",True) \\\\\\n.config(\"spark.memory.offHeap.size\",\"5g\") \\\\\\n.config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n.config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n.config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n.config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n.getOrCreate()',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries'},\n",
       "  {'text': 'While creating a SparkSession using the config spark.jars.packages as com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\\nspark = SparkSession.builder.master(\\'local\\').appName(\\'bq\\').config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\").getOrCreate()\\nautomatically downloads the required dependency jars and configures the connector, removing the need to manage this dependency. More details available here',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Spark BigQuery connector Automatic configuration'},\n",
       "  {'text': 'Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\\nTheres a few extra steps to go into reading from GCS with PySpark\\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\\n3.) In your Python script, there are a few extra classes youll have to import:\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.conf import SparkConf\\nfrom pyspark.context import SparkContext\\n4.) You must set up your configurations before building your SparkSession. Heres my code snippet:\\nconf = SparkConf() \\\\\\n.setMaster(\\'local[*]\\') \\\\\\n.setAppName(\\'test\\') \\\\\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\\\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc = SparkContext(conf=conf)\\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\\n5.) Once you run that, build your SparkSession with the new parameters wed just instantiated in the previous step:\\nspark = SparkSession.builder \\\\\\n.config(conf=sc.getConf()) \\\\\\n.getOrCreate()\\n6.) Finally, youre able to read your files straight from GCS!\\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Spark Cloud Storage connector'},\n",
       "  {'text': 'from pyarrow.parquet import ParquetFile\\npf = ParquetFile(\\'fhvhv_tripdata_2021-01.parquet\\')\\n#pyarrow builds tables, not dataframes\\ntbl_small = next(pf.iter_batches(batch_size = 1000))\\n#this function converts the table to a dataframe of manageable size\\ndf = tbl_small.to_pandas()\\nAlternatively without PyArrow:\\ndf = spark.read.parquet(\\'fhvhv_tripdata_2021-01.parquet\\')\\ndf1 = df.sort(\\'DOLocationID\\').limit(1000)\\npdf = df1.select(\"*\").toPandas()\\ngcsu',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'How can I read a small number of rows from the parquet file directly?'},\n",
       "  {'text': 'Probably youll encounter this if you followed the video 5.3.1 - First Look at Spark/PySpark and used the parquet file from the TLC website (csv was used in the video).\\nWhen defining the schema, the PULocation and DOLocationID are defined as IntegerType. This will cause an error because the Parquet file is INT64 and youll get an error like:\\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\\nChange the schema definition from IntegerType to LongType and it should work',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'DataType error when creating Spark DataFrame with a specified schema?'},\n",
       "  {'text': 'df_finalx=df_finalw.select([col(x).alias(x.replace(\" \",\"\")) for x in df_finalw.columns])\\nKrishna Anand',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Remove white spaces from column names in Pyspark'},\n",
       "  {'text': 'This error comes up on the Spark video 5.3.1 - First Look at Spark/PySpark,\\nbecause as at the creation of the video, 2021 data was the most recent which utilised csv files but as at now its parquet.\\nSo when you run the command spark.createDataFrame(df1_pandas).show(),\\nYou get the Attribute error. This is caused by the pandas version 2.0.0 which seems incompatible with Spark 3.3.2, so to fix it you have to downgrade pandas to 1.5.3 using the command pip install -U pandas==1.5.3\\nAnother option is adding the following after importing pandas, if one does not want to downgrade pandas version (source) :\\npd.DataFrame.iteritems = pd.DataFrame.items\\nNote that this problem is solved with Spark versions from 3.4.1',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\"},\n",
       "  {'text': 'Another alternative is to install pandas 2.0.1 (it worked well as at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\"},\n",
       "  {'text': 'Open a CMD terminal in administrator mode\\ncd %SPARK_HOME%\\nStart a master node: bin\\\\spark-class org.apache.spark.deploy.master.Master\\nStart a worker node: bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\nbin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\nspark://<master_ip>:<port>: copy the address from the previous command, in my case it was spark://localhost:7077\\nUse --host <IP_ADDR> if you want to run the worker on a different machine. For now leave it empty.\\nNow you can access Spark UI through localhost:8080\\nHomework for Module 5:\\nDo not refer to the homework file located under /05-batch/code/. The correct file is located under\\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2024/05-batch/homework.md',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Spark Standalone Mode on Windows'},\n",
       "  {'text': 'You can either type the export command every time you run a new session, add it to the .bashrc/ which you can find in /home or run this command at the beginning of your homebook:\\nimport findspark\\nfindspark.init()',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Export PYTHONPATH command in linux is temporary'},\n",
       "  {'text': 'I solved this issue: unzip the file with:\\nf\\nbefore creating head.csv',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Compressed file ended before the end-of-stream marker was reached'},\n",
       "  {'text': 'In the code along from Video 5.3.3 Alexey downloads the CSV files from the NYT website and gzips them in their bash script. If we now (2023) follow along but download the data from the GH course Repo, it will already be zippes as csv.gz files. Therefore we zip it again if we follow the code from the video exactly. This then leads to gibberish outcome when we then try to cat the contents or count the lines with zcat, because the file is zipped twitch and zcat only unzips it once.\\nsolution: do not gzip the files downloaded from the course repo. Just wget them and save them as they are as csv.gz files. Then the zcat command and the showSchema command will also work\\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\necho \"downloading ${URL} to ${LOCAL_PATH}\"\\nmkdir -p ${LOCAL_PREFIX}\\nwget ${URL} -O ${LOCAL_PATH}\\necho \"compressing ${LOCAL_PATH}\"\\n# gzip ${LOCAL_PATH} <- uncomment this line',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Compression Error: zcat output is gibberish, seems like still compressed'},\n",
       "  {'text': 'Occurred while running : spark.createDataFrame(df_pandas).show()\\nThis error is usually due to the python version, since spark till date of 2 march 2023 doesnt support python 3.11, try creating a new env with python version 3.8 and then run this command.\\nOn the virtual machine, you can create a conda environment (here called myenv) with python 3.10 installed:\\nconda create -n myenv python=3.10 anaconda\\nThen you must run conda activate myenv to run python 3.10. Otherwise youll still be running version 3.11. You can deactivate by typing conda deactivate.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'PicklingError: Could not serialise object: IndexError: tuple index out of range.'},\n",
       "  {'text': 'Make sure you have your credentials of your GCP in your VM under the location defined in the script.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?'},\n",
       "  {'text': 'To run spark in docker setup\\n1. Build bitnami spark docker\\na. clone bitnami repo using command\\ngit clone https://github.com/bitnami/containers.git\\n(tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)\\nb. edit file `bitnami/spark/3.3/debian-11/Dockerfile` and update java and spark version as following\\n\"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\\\\\n\"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\\\\\nreference: https://github.com/bitnami/containers/issues/13409\\nc. build docker image by navigating to above directory and running docker build command\\nnavigate cd bitnami/spark/3.3/debian-11/\\nbuild command docker build -t spark:3.3-java-17 .\\n2. run docker compose\\nusing following file\\n```yaml docker-compose.yml\\nversion: \\'2\\'\\nservices:\\nspark:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=master\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8080:8080\\'\\n- \\'7077:7077\\'\\nspark-worker:\\nimage: spark:3.3-java-17\\nenvironment:\\n- SPARK_MODE=worker\\n- SPARK_MASTER_URL=spark://spark:7077\\n- SPARK_WORKER_MEMORY=1G\\n- SPARK_WORKER_CORES=1\\n- SPARK_RPC_AUTHENTICATION_ENABLED=no\\n- SPARK_RPC_ENCRYPTION_ENABLED=no\\n- SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n- SPARK_SSL_ENABLED=no\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8081:8081\\'\\nspark-nb:\\nimage: jupyter/pyspark-notebook:java-17.0.5\\nenvironment:\\n- SPARK_MASTER_URL=spark://spark:7077\\nvolumes:\\n- \"./:/home/jovyan/work:rw\"\\nports:\\n- \\'8888:8888\\'\\n- \\'4040:4040\\'\\n```\\nrun command to deploy docker compose\\ndocker-compose up\\nAccess jupyter notebook using link logged in docker compose logs\\nSpark master url is spark://spark:7077',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Spark docker-compose setup'},\n",
       "  {'text': 'To do this\\npip install gcsfs,\\nThereafter copy the uri path to the file and use \\ndf = pandas.read_csc(gs://path)',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'How do you read data stored in gcs on pandas with your local computer?'},\n",
       "  {'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you dont have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'TypeError when using spark.createDataFrame function on a pandas df'},\n",
       "  {'text': 'Default executor memory is 1gb. This error appeared when working with the homework dataset.\\nError: MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\\nScaling row group sizes to 95.00% for 8 writers\\nSolution:\\nIncrease the memory of the executor when creating the Spark session like this:\\nRemember to restart the Jupyter session (ie. close the Spark session) or the config wont take effect.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory'},\n",
       "  {'text': 'Change the working directory to the spark directory:\\nif you have setup up your SPARK_HOME variable, use the following;\\ncd %SPARK_HOME%\\nif not, use the following;\\ncd <path to spark installation>\\nCreating a Local Spark Cluster\\nTo start Spark Master:\\nbin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\nStarting up a cluster:\\nbin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'How to spark standalone cluster is run on windows OS'},\n",
       "  {'text': 'I added PYTHONPATH, JAVA_HOME and SPARK_HOME to ~/.bashrc, import pyspark worked ok in iPython in terminal, but couldnt be found in .ipynb opened in VS Code\\nAfter adding new lines to ~/.bashrc, need to restart the shell to activate the new lines, do either\\nsource ~/.bashrc\\nexec bash\\nInstead of configuring paths in ~/.bashrc, I created .env file in the root of my workspace:',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code'},\n",
       "  {'text': 'I dont use visual studio, so I did it the old fashioned way: ssh -L 8888:localhost:8888 <my user>@<VM IP> (replace user and IP with the ones used by the GCP VM, e.g. : ssh -L 8888:localhost:8888 myuser@34.140.188.1',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'How to port forward outside VS Code'},\n",
       "  {'text': 'If you are doing wc -l fhvhv_tripdata_2021-01.csv.gz  with the gzip file as the file argument, you will get a different result, obviously! Since the file is compressed.\\nUnzip the file and then do wc -l fhvhv_tripdata_2021-01.csv to get the right results.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'wc -l is giving a different result then shown in the video'},\n",
       "  {'text': 'when trying to:\\nURL=\"spark://$HOSTNAME:7077\"\\nspark-submit \\\\\\n--master=\"{$URL}\" \\\\\\n06_spark_sql.py \\\\\\n--input_green=data/pq/green/2021/*/ \\\\\\n--input_yellow=data/pq/yellow/2021/*/ \\\\\\n--output=data/report-2021\\nand you get errors like the following (SUMMARIZED):\\nWARN Utils: Your hostname, <HOSTNAME> resolves to a loopback address..\\nWARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Setting default log level to \"WARN\".\\nException in thread \"main\" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local at \\nTry replacing --master=\"{$URL}\"\\nwith --master=$URL (edited)\\nExtra edit for spark version 3.4.2 - if encountering:\\n`Error: Unrecognized option: --master=`\\n Replace `--master=\"{$URL}\"` with  `--master \"${URL}\"`',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': '`spark-submit` errors'},\n",
       "  {'text': 'If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\nFor Windows, create a new User Variable HADOOP_HOME that points to your Hadoop directory. Then add %HADOOP_HOME%\\\\bin to the PATH variable.\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z'},\n",
       "  {'text': \"Change the hadoop version to 3.0.1.Replace all the files in the local hadoop bin folder with the files in this repo:  winutils/hadoop-3.0.1/bin at master  cdarlint/winutils (github.com)\\nIf this does not work try to change other versions found in this repository.\\nFor more information please see this link: This version of %1 is not compatible with the version of Windows you're running  Issue #20  cdarlint/winutils (github.com)\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Java.io.IOException. Cannot run program C:\\\\hadoop\\\\bin\\\\winutils.exe. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.'},\n",
       "  {'text': 'Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=my_cluster \\\\\\n--region=us-central1 \\\\\\n--project=my-dtc-project-1010101 \\\\\\ngs://my-dtc-bucket-id/code/06_spark_sql.py\\n-- \\\\\\n',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.'},\n",
       "  {'text': 'Go to %SPARK_HOME%\\\\bin\\nRun spark-class org.apache.spark.deploy.master.Master to run the master. This will give you a URL of the form spark://ip:port\\nRun spark-class org.apache.spark.deploy.worker.Worker spark://ip:port to run the worker. Make sure you use the URL you obtained in step 2.\\nCreate a new Jupyter notebook:\\nspark = SparkSession.builder \\\\\\n.master(\"spark://{ip}:7077\") \\\\\\n.appName(\\'test\\') \\\\\\n.getOrCreate()\\nCheck on Spark UI the master, worker and app.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Run Local Cluster Spark in Windows 10 with CMD'},\n",
       "  {'text': 'This occurs because you are not logged in gcloud auth login and maybe the project id is not settled. Then type in a terminal:\\ngcloud auth login\\nThis will open a tab in the browser, accept the terms, after that close the tab if you want. Then set the project is like:\\ngcloud config set project <YOUR PROJECT_ID>\\nThen you can run the command to upload the pq dir to a GCS Bucket:\\ngsutil -m cp -r pq/ <YOUR URI from gsutil>/pq',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\"},\n",
       "  {'text': \"When submit a job, it might throw an error about Java in log panel within Dataproc. I changed the Versioning Control when I created a cluster, so it means that I delete the cluster and created a new one, and instead of choosing Debian-Hadoop-Spark, I switch to Ubuntu 20.02-Hadoop3.3-Spark3.3 for Versioning Control feature, the main reason to choose this is because I have the same Ubuntu version in mi laptop, I tried to find documentation to sustent this but unfortunately I couldn't nevertheless it works for me.\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'py4j.protocol.Py4JJavaError  GCP'},\n",
       "  {'text': \"Use both repartition and coalesce, like so:\\ndf = df.repartition(6)\\ndf = df.coalesce(6)\\ndf.write.parquet('fhv/2019/10', mode='overwrite')\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead'},\n",
       "  {'text': \"Possible solution - Try to forward the port using ssh cli instead of vs code.\\nRun > ssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>\\nssh hostname is the name you specified in the ~/.ssh/config file\\nIn case of Jupyter Notebook run\\nssh -L 8888:localhost:8888 gcp-vm\\nfrom your local machines cli.\\nNOTE: If you logout from the session, the connection would break. Also while creating the spark session notice the block's log because sometimes it fails to run at 4040 and then switches to 4041.\\n~Abhijit Chakrborty: If you are having trouble accessing localhost ports from GCP VM consider adding the forwarding instructions to .ssh/config file as following:\\n```\\nHost <hostname>\\nHostname <external-gcp-ip>\\nUser xxxx\\nIdentityFile yyyy\\nLocalForward 8888 localhost:8888\\nLocalForward 8080 localhost:8080\\nLocalForward 5432 localhost:5432\\nLocalForward 4040 localhost:4040\\n```\\nThis should automatically forward all ports and will enable accessing localhost ports.\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?'},\n",
       "  {'text': '~ Abhijit Chakraborty\\n`sdk list java`  to check for available java sdk versions.\\n`sdk install java 11.0.22-amzn`  as  java-11.0.22-amzn was available for my codespace.\\nclick on Y if prompted to change the default java version.\\nCheck for java version using `java -version `.\\nIf working fine great, else `sdk default java 11.0.22-amzn` or whatever version you have installed.',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Installing Java 11 on codespaces'},\n",
       "  {'text': 'Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\\nSolution: As mentioned here, sometimes there might not be enough resources in the given region to allocate the request. Usually, gets freed up in a bit and one can create a cluster.  abhirup ghosh\\nSolution 2:  Changing the type of boot-disk from PD-Balanced to PD-Standard, in terraform, helped solve the problem.- Sundara Kumar Padmanabhan',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\"},\n",
       "  {'text': \"Pyspark converts the difference of two TimestampType values to Python's native datetime.timedelta object. The timedelta object only stores the duration in terms of days, seconds, and microseconds. Each of the three units of time must be manually converted into hours in order to express the total duration between the two timestamps using only hours.\\nAnother way for achieving this is using the datediff (sql function). It receives this parameters\\nUpper Date: the closest date you have. For example dropoff_datetime\\nLower Date: the farthest date you have.  For example pickup_datetime\\nAnd the result is returned in terms of days, so you could multiply the result for 24 in order to get the hours.\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Homework - how to convert the time difference of two timestamps to hours'},\n",
       "  {'text': 'This version combination worked for me:\\nPySpark = 3.3.2\\nPandas = 1.5.3\\n\\nIf it still has an error,',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'PicklingError: Could not serialize object: IndexError: tuple index out of range'},\n",
       "  {'text': \"Run this before SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.'},\n",
       "  {'text': \"import os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\nDataproc Pricing: https://cloud.google.com/dataproc/pricing#on_gke_pricing\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.'},\n",
       "  {'text': 'Ans: No, you can submit a job to DataProc from your local computer by installing gsutil (https://cloud.google.com/storage/docs/gsutil_install) and configuring it. Then, you can execute the following command from your local computer.\\ngcloud dataproc jobs submit pyspark \\\\\\n--cluster=de-zoomcamp-cluster \\\\\\n--region=europe-west6 \\\\\\ngs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\\\\n-- \\\\\\n--input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\\\\n--input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\\\\n--output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020 (edited)',\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?'},\n",
       "  {'text': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn't work. What can I do?\\nError\\nInsufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\\nRequest ID: 17942272465025572271\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\nMaster Node:\\nMachine type: n2-standard-2\\nPrimary disk size: 85 GB\\nWorker Node:\\nNumber of worker nodes: 2\\nMachine type: n2-standard-2\\nPrimary disk size: 80 GB\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\",\n",
       "   'section': 'Module 5: pyspark',\n",
       "   'question': 'In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error'},\n",
       "  {'text': 'The MacOS setup instruction (https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java) for setting the JAVA_HOME environment variable is for Intel-based Macs which have a default install location at /usr/local/. If you have an Apple Silicon mac, you will have to set JAVA_HOME to /opt/homebrew/, specifically in your .bashrc or .zshrc:\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\nConfirm that your path was correctly set by running the command: which java\\nYou should expect to see the output:\\n/opt/homebrew/opt/openjdk/bin/java\\nReference: https://docs.brew.sh/Installation',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon'},\n",
       "  {'text': 'Check Docker Compose File:\\nEnsure that your docker-compose.yaml file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\nOn Mac OSX 12.2.1 (Monterey) I could not start the kafka control center. I opened Docker Desktop and saw docker images still running from week 4, which I did not see when I typed docker ps. I deleted them in docker desktop and then had no problem starting up the kafka environment.',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Could not start docker image control-center from the docker-compose.yaml file.'},\n",
       "  {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Module kafka not found when trying to run producer.py'},\n",
       "  {'text': 'ImportError: DLL load failed while importing cimpl: The specified module could not be found\\nVerify Python Version:\\nMake sure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n... you may have to load librdkafka-5d2e2910.dll in the code. Add this before importing avro:\\nfrom ctypes import CDLL\\nCDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\\nIt seems that the error may occur depending on the OS and python version installed.\\nALTERNATIVE:\\nImportError: DLL load failed while importing cimpl\\nSOLUTION: $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1 in Powershell.\\nYou need to set this DLL manually in Conda Env.\\nSource: https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Error importing cimpl dll when running avro examples'},\n",
       "  {'text': \"SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\",\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': \"ModuleNotFoundError: No module named 'avro'\"},\n",
       "  {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka APIs while eliminating Kafka complexity.',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Error while running python3 stream.py worker'},\n",
       "  {'text': 'Got this error because the docker container memory was exhausted. The dta file was upto 800MB but my docker container does not have enough memory to handle that.\\nSolution was to load the file in chunks with Pandas, then create multiple parquet files for each dat file I was processing. This worked smoothly and the issue was resolved.',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Negsignal:SIGKILL while converting dta files to parquet format'},\n",
       "  {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing'},\n",
       "  {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Kafka- python videos have low audio and hard to follow up'},\n",
       "  {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable'},\n",
       "  {'text': 'Ankush said we can focus on horizontal scaling option.\\nthink of scaling in terms of scaling from consumer end. Or consuming message via horizontal scaling',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Kafka homwork Q3, there are options that support scaling concept more than the others:'},\n",
       "  {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images arent readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"},\n",
       "  {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Python Kafka: ./build.sh: Permission denied Error'},\n",
       "  {'text': 'Restarting all services worked for me:\\ndocker-compose down\\ndocker-compose up',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Python Kafka: KafkaTimeoutError: Failed to update metadata after 60.0 secs. when running stream-example/producer.py'},\n",
       "  {'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark version\\nspark-submit version\\nAdd your version to SPARK_VERSION in build.sh',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.'},\n",
       "  {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails'},\n",
       "  {'text': 'Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.'},\n",
       "  {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran gradle shadowjar, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build'},\n",
       "  {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py'},\n",
       "  {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal'},\n",
       "  {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent'},\n",
       "  {'text': 'Situation: in VS Code, usually there will be a triangle icon next to each test. I couldnt see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n Explorer (first icon on the left navigation bar)\\n JAVA PROJECTS (bottom collapsable)\\n  icon next in the rightmost position to JAVA PROJECTS\\n  clean Workspace\\n Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Java Kafka: Tests are not picked up in VSCode'},\n",
       "  {'text': 'In Confluent Cloud:\\nEnvironment  default (or whatever you named your environment as)  The right navigation bar   Stream Governance API   The URL under Endpoint\\nAnd create credentials from Credentials section below it',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'Confluent Kafka: Where can I find schema registry URL?'},\n",
       "  {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "   'section': 'Module 6: streaming with kafka',\n",
       "   'question': 'How do I check compatibility of local and container Spark versions?'},\n",
       "  {'text': 'According to https://github.com/dpkp/kafka-python/\\nDUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING\\nUse pip install kafka-python-ng instead',\n",
       "   'section': 'Project',\n",
       "   'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?'},\n",
       "  {'text': 'Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.',\n",
       "   'section': 'Project',\n",
       "   'question': 'How is my capstone project going to be evaluated?'},\n",
       "  {'text': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects. There are simply TWO chances to pass the course. You can use the Second Attempt if you a) fail the first attempt b) do not have the time due to other engagements such as holiday or sickness etc. to enter your project into the first attempt.',\n",
       "   'section': 'Project',\n",
       "   'question': 'Project 1 & Project 2'},\n",
       "  {'text': 'See a list of datasets here: https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_7_project/datasets.md',\n",
       "   'section': 'Project',\n",
       "   'question': 'Does anyone know nice and relatively large datasets?'},\n",
       "  {'text': 'You need to redefine the python environment variable to that of your user account',\n",
       "   'section': 'Project',\n",
       "   'question': 'How to run python as start up script?'},\n",
       "  {'text': 'Initiate a Spark Session\\nspark = (SparkSession\\n.builder\\n.appName(app_name)\\n.master(master=master)\\n.getOrCreate())\\nspark.streams.resetTerminated()\\nquery1 = spark\\n.readStream\\n\\n\\n.load()\\nquery2 = spark\\n.readStream\\n\\n\\n.load()\\nquery3 = spark\\n.readStream\\n\\n\\n.load()\\nquery1.start()\\nquery2.start()\\nquery3.start()\\nspark.streams.awaitAnyTermination() #waits for any one of the query to receive kill signal or error failure. This is asynchronous\\n# On the contrary query3.start().awaitTermination() is a blocking ex call. Works well when we are reading only from one topic.',\n",
       "   'section': 'Project',\n",
       "   'question': 'Spark Streaming - How do I read from multiple topics in the same Spark Session'},\n",
       "  {'text': 'Transformed data can be moved in to azure blob storage and then it can be moved in to azure SQL DB, instead of moving directly from databricks to Azure SQL DB.',\n",
       "   'section': 'Project',\n",
       "   'question': 'Data Transformation from Databricks to Azure SQL DB'},\n",
       "  {'text': 'The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\\nDetailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\\nSource code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py',\n",
       "   'section': 'Project',\n",
       "   'question': 'Orchestrating dbt with Airflow'},\n",
       "  {'text': 'https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html\\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html\\nGive the following roles to you service account:\\nDataProc Administrator\\nService Account User (explanation here)\\nUse DataprocSubmitPySparkJobOperator, DataprocDeleteClusterOperator and  DataprocCreateClusterOperator.\\nWhen using  DataprocSubmitPySparkJobOperator, do not forget to add:\\ndataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\nBecause DataProc does not already have the BigQuery Connector.',\n",
       "   'section': 'Project',\n",
       "   'question': 'Orchestrating DataProc with Airflow'},\n",
       "  {'text': 'You can trigger your dbt job in Mage pipeline. For this get your dbt cloud api key under settings/Api tokens/personal tokens. Add it safely to  your .env\\nFor example\\ndbt_api_trigger=dbt_**\\nNavigate to job page and find api trigger  link\\nThen create a custom mage Python block with a simple http request like here\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\ndotenv_path = Path(\\'/home/src/.env\\')\\nload_dotenv(dotenv_path=dotenv_path)\\ndbt_api_trigger= os.getenv(dbt_api_trigger)\\nurl = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\nheaders = {\\n        \"Authorization\": f\"Token {dbt_api_trigger}\",\\n        \"Content-Type\": \"application/json\" }\\nbody = {\\n        \"cause\": \"Triggered via API\"\\n    }\\n    response = requests.post(url, headers=headers, json=body)\\nvoila! You triggered dbt job form your mage pipeline.',\n",
       "   'section': 'Project',\n",
       "   'question': 'Orchestrating dbt cloud with Mage'},\n",
       "  {'text': \"The slack thread : thttps://datatalks-club.slack.com/archives/C01FABYF2RG/p1677678161866999\\nThe question is that sometimes even if you take plenty of effort to document every single step, and we can't even sure if the person doing the peer review will be able to follow-up, so how this criteria will be evaluated?\\nAlex clarifies: Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions and so on - then it's already great\",\n",
       "   'section': 'Project',\n",
       "   'question': 'Project evaluation - Reproducibility'},\n",
       "  {'text': 'The key valut in Azure cloud is used to store credentials or passwords or secrets of different tech stack used in Azure. For example if u do not want to expose the password in SQL database, then we can save the password under a given name and use them in other Azure stack.',\n",
       "   'section': 'Project',\n",
       "   'question': 'Key Vault in Azure cloud stack'},\n",
       "  {'text': 'You can get the version of py4j from inside docker using this command\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \"ls /opt/spark/python/lib\"',\n",
       "   'section': 'Project',\n",
       "   'question': \"Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\"},\n",
       "  {'text': 'Either use conda or pip for managing venv, using both of them together will cause incompatibility.\\nIf youre using conda, install psycopg2 using the conda-forge channel, which may handle the architecture compatibility automatically\\nconda install -c conda-forge psycopg2\\nIf pip, do the normal install\\npip install psycopg2',\n",
       "   'section': 'Project',\n",
       "   'question': 'psycopg2 complains of incompatible environment e.g x86 instead of amd'},\n",
       "  {'text': 'This is not a FAQ but more of an advice if you want to set up dbt locally, I did it in the following way:\\nI had the postgres instance from week 2 (year 2024) up (the docker-compose)\\nmkdir dbt\\nvi dbt/profiles.yml\\nAnd here I attached this content (only the required fields) and replaced them with the proper values (for instance mine where in the .env file of the folder of week 2 docker stuff)\\ncd dbt && git clone https://github.com/dbt-labs/dbt-starter-project\\nmkdir project && cd project && mv dbt-starter-project/* .\\nMake sure that you align the profile name in profiles.yml with the dbt_project.yml file\\nAdd this line anywhere on the dbt_project.yml file:\\nconfig-version: 2\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres ls\\nIf you have trouble run\\ndocker run --network=mage-zoomcamp_default --mount type=bind,source=/<your-path>/dbt/project,target=/usr/app --mount type=bind,source=/<your-path>/profiles.yml,target=/root/.dbt/profiles.yml ghcr.io/dbt-labs/dbt-postgres debug',\n",
       "   'section': 'Project',\n",
       "   'question': 'Setting up dbt locally with Docker and Postgres'},\n",
       "  {'text': 'The following line should be included in pyspark configuration\\n# Example initialization of SparkSession variable\\nspark = (SparkSession.builder\\n.master(...)\\n.appName(...)\\n# Add the following configuration\\n.config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n)',\n",
       "   'section': 'Project',\n",
       "   'question': 'How to connect Pyspark with BigQuery?'},\n",
       "  {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "   'section': 'Course Management Form for Homeworks',\n",
       "   'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key'},\n",
       "  {'text': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname, or your real name, if you prefer. Your entry on the Leaderboard is the one highlighted in teal(?) / light green (?).\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.\\nQuestion: Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?\\nAnswer: Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.',\n",
       "   'section': 'Workshop 1 - dlthub',\n",
       "   'question': 'Edit Course Profile.'},\n",
       "  {'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If youre doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "   'section': 'Workshop 1 - dlthub',\n",
       "   'question': 'How do I install the necessary dependencies to run the code?'},\n",
       "  {'text': 'If you are running Jupyter Notebook on a fresh new Codespace or in local machine with a new Virtual Environment, you will need this package to run the starter Jupyter Notebook offered by the teacher. Execute this:\\npip install jupyter',\n",
       "   'section': 'Workshop 1 - dlthub',\n",
       "   'question': 'Other packages needed but not listed'},\n",
       "  {'text': 'Alternatively, you can switch to in-file storage with:',\n",
       "   'section': 'Workshop 1 - dlthub',\n",
       "   'question': 'How can I use DuckDB In-Memory database with dlt ?'},\n",
       "  {'text': 'After loading, you should have a total of 8 records, and ID 3 should have age 33\\nQuestion: Calculate the sum of ages of all the people loaded as described above\\nThe sum of all eight records\\' respective ages is too big to be in the choices. You need to first filter out the people whose occupation is equal to None in order to get an answer that is close to or present in the given choices. \\n----------------------------------------------------------------------------------------\\nFIXED = use a raw string and keep the file:/// at the start of your file path\\nI\\'m having an issue with the dlt workshop notebook. The \\'Load to Parquet file\\' section specifically. No matter what I change the file path to, it\\'s still saving the dlt files directly to my C drive.\\n# Set the bucket_url. We can also use a local folder\\nos.environ[\\'DESTINATION__FILESYSTEM__BUCKET_URL\\'] = r\\'file:///content/.dlt/my_folder\\'\\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n# Define your pipeline\\npipeline = dlt.pipeline(\\npipeline_name=\\'my_pipeline\\',\\ndestination=\\'filesystem\\',\\ndataset_name=\\'mydata\\'\\n)\\n# Run the pipeline with the generator we created earlier.\\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\\nprint(load_info)\\n# Get a list of all Parquet files in the specified folder\\nparquet_files = glob.glob(\\'/content/.dlt/my_folder/mydata/users/*.parquet\\')\\n# show parquet files\\nfor file in parquet_files:\\nprint(file)',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Homework - dlt Exercise 3 - Merge a generator concerns'},\n",
       "  {'text': 'Check the contents of the repository with ls - the command.sh file should be in the root folder\\nIf it is not, verify that you had cloned the correct repository - https://github.com/risingwavelabs/risingwave-data-talks-workshop-2024-03-04',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'command.sh Error - source: no such file or directory: command.sh'},\n",
       "  {'text': \"psql is a command line tool that is installed alongside PostgreSQL DB, but since we've always been running PostgreSQL in a container, you've only got `pgcli`, which lacks the feature to run a sql script into the DB. Besides, having a command line for each database flavor you'll have to deal with as a Data Professional is far from ideal.\\nSo, instead, you can use usql. Check the docs for details on how to install for your OS. On macOS, it supports `homebrew`, and on Windows, it supports scoop.\\nSo, to run the taxi_trips.sql script with usql:\",\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'psql - command not found: psql (alternative install)'},\n",
       "  {'text': 'If you encounter this error and are certain that you have docker compose installed, but typically run it as docker compose without the hyphen, then consider editing command.sh file by removing the hyphen from docker-compose. Example:\\nstart-cluster() {\\ndocker compose -f docker/docker-compose.yml up -d\\n}',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Setup - source command.sh - error: docker-compose not found'},\n",
       "  {'text': 'ERROR: The Compose file \\'./docker/docker-compose.yml\\' is invalid because:\\nInvalid top-level property \"x-image\". Valid top-level sections for this Compose file are: version, services, networks, volumes, secrets, configs, and extensions starting with \"x-\".\\nYou might be seeing this error because you\\'re using the wrong Compose file version. Either specify a supported version (e.g \"2.2\" or \"3.3\") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.\\nFor more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/\\nIf you encounter the above error and have docker-compose installed, try updating your version of docker-compose. At the time of reporting this issue (March 17 2024), Ubuntu does not seem to support a docker-compose version high enough to run the required docker images. If you have this error and are on a Ubuntu machine, consider starting a VM with a Debian machine or look for an alternative way to download docker-compose at the latest version on your machine.',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Setup - start-cluster error: Invalid top-level property x-image'},\n",
       "  {'text': 'Ans: [source] Yes, it is so that we can observe the changes as were working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?'},\n",
       "  {'text': 'Ans: No, it is not.',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]'},\n",
       "  {'text': 'Ans: about 7GB free for all the containers to be provisioned and then the psql still needs to run and ingest the taxi data, so maybe 10gb in total?',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Setup - Qn: How much free disk space should we have? [source]'},\n",
       "  {'text': 'Replace psycopg2==2.9.9 with psycopg2-binary in the requirements.txt file [source] [another]\\nWhen you open another terminal to run the psql, remember to do the source command.sh step for each terminal session\\n---------------------------------------------------------------------------------------------',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Psycopg2 - issues when running stream-kafka script'},\n",
       "  {'text': \"If youre using an Anaconda installation:\\nCd home/\\nConda install gcc\\nSource back to your RisingWave Venv - source .venv/bin/activate\\nPip install psycopg2-binary\\nPip install -r requirements.txt\\nFor some reason this worked - the Conda base doesnt have the GCC installed - (GNU Compiler Collection) a compiler system that supports various programming languages. Without this the it fails to install pyproject.toml-based projects\\nIt's possible that in your specific environment, the gcc installation was required at the system level rather than within the virtual environment. This can happen if the build process for psycopg2 tries to access system-level dependencies during installation.\\nInstalling gcc in your main Python installation (Conda) would make it available system-wide, allowing any Python environment to access it when necessary for building packages.\\ngcc stands for GNU Compiler Collection. It is a compiler system developed by the GNU Project that supports various programming languages, including C, C++, Objective-C, and Fortran.\\nGCC is widely used for compiling source code written in these languages into executable programs or libraries. It's a key tool in the software development process, particularly in the compilation stage where source code is translated into machine code that can be executed by a computer's processor.\\nIn addition to compiling source code, GCC also provides various optimization options, debugging support, and extensive documentation, making it a powerful and versatile tool for developers across different platforms and architectures.\\n-----------------------------------------------------------------------------------\",\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`'},\n",
       "  {'text': \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\nUse the git bash terminal in windows.\\nActivate python venv from git bash: source .venv/Scripts/activate\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\nNow from git bash, run the seed-kafka cmd. It should work now.\\nAdditional Notes:\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\nThe equivalent of source commands.sh  in Powershell is . .\\\\commands.sh from the workshop directory.\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\n--------------------------------------------------------------------------------------\",\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.'},\n",
       "  {'text': 'In case the script gets stuck on\\n%3|1709652240.100|FAIL|rdkafka#producer-2| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Connect to ipv4#127.0.0.1:9092 failed: Connection refused (after 0ms in state CONNECT)gre\\nafter trying to load the trip data, check the logs of the message_queue container in docker. If it keeps restarting with Could not initialize seastar: std::runtime_error (insufficient physical memory: needed 4294967296 available 4067422208)  as the last message, then go to the docker-compose file in the docker folder of the project and change the memory command for the message_queue service for some lower value.\\nSolution: lower the memory allocation of the service message_queue in your docker-compose file from 4GB. If you have the insufficient physical memory error message (try 3GB)\\nIssue: Running psql -f risingwave-sql/table/trip_data.sql after starting services with default values using docker-compose up gives the error  psql:risingwave-sql/table/trip_data.sql:61: ERROR:  syntax error at or near \".\" LINE 60:       properties.bootstrap.server=\\'message_queue:29092\\'\\nSolution: Make sure you have run source commands.sh in each terminal window',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Running stream-kafka script gets stuck on a loop with Connection Refused'},\n",
       "  {'text': 'Use seed-kafka instead of stream-kafka to get a static set of results.',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'For the homework questions is there a specific number of records that have to be processed to obtain the final answer?'},\n",
       "  {'text': 'It is best to use the order by and limit clause on the query to the materialized view instead of the materialized view creation in order to guarantee consistent results\\nHomework - The answers in the homework do not match the provided options: You must follow the following steps: 1. clean-cluster 2. docker volume prune and use seed-kafka instead of stream-kafka. Ensure that the number of records is 100K.',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Homework - Materialized view does not guarantee order by warning'},\n",
       "  {'text': 'For this workshop, and if you are following the view from Noel (2024) this requires you to install postgres to use it on your terminal. Found this steps (commands) to get it done [source]:\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo sh -c \\'echo \"deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list\\'\\nsudo apt update\\napt install postgresql postgresql-contrib\\n(comment): now lets check the service for postgresql\\nservice postgresql status\\n(comment) If down: use the next command\\nservice postgresql start\\n(comment) And your are done',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'How to install postgress on Linux like OS'},\n",
       "  {'text': 'Refer to the solution given in the first solution here:\\nhttps://stackoverflow.com/questions/24683221/xdg-open-no-method-available-even-after-installing-xdg-utils\\nInstead of w3m use any other browser of your choice.\\nIt is just trying to open the index.html file. Which you can do from your File Explorer/Finder. If youre on wsl try using explorer.exe index.html',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Unable to Open Dashboard as xdg-open doesnt open any browser'},\n",
       "  {'text': 'Example Error:\\nWhen attempting to execute a Python script named seed-kafka.py or server.py with the following shebang line specifying Python 3 as the interpreter:\\nUsers may encounter the following error in a Unix-like environment:\\nThis error indicates that there is a problem with the Python interpreter path specified in the shebang line. The presence of the \\\\r character suggests that the script was edited or created in a Windows environment, causing the interpreter path to be incorrect when executed in Unix-like environments.\\n2 Solutions:\\nEither one or the other\\nUpdate Shebang Line:\\nVerify Python Interpreter Path: Use the which python3 command to determine the path to the Python 3 interpreter available in the current environment.\\nUpdate Shebang Line: Open the script file in a text editor. Modify the shebang line to point to the correct Python interpreter path found in the previous step. Ensure that the shebang line is consistent with the Python interpreter path in the execution environment.\\nExample Shebang Line:\\nReplace /usr/bin/env python3 with the correct Python interpreter path found using which python3.\\nConvert Line Endings:\\nUse the dos2unix command-line tool to convert the line endings of the script from Windows-style to Unix-style.\\nThis removes the extraneous carriage return characters (\\\\r), resolving issues related to unexpected tokens and ensuring compatibility with Unix-like environments.\\nExample Command:',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'Resolving Python Interpreter Path Inconsistencies in Unix-like Environments'},\n",
       "  {'text': 'Ans : Windowing in streaming SQL involves defining a time-based or row-based boundary for data processing. It allows you to analyze and aggregate data over specific time intervals or based on the number of events received, providing a way to manage and organize streaming data for analysis.',\n",
       "   'section': 'Workshop 2 - RisingWave',\n",
       "   'question': 'How does windowing work in Sql?'},\n",
       "  {'text': 'Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \"pip install kafka-python\", you can resolve the issue by using \"pip install git+https://github.com/dpkp/kafka-python.git\". If you have already installed kafka-python, you need to run \"pip uninstall kafka-python\" before executing \"pip install git+https://github.com/dpkp/kafka-python.git\" to resolve the compatibility issue.\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"{{ env_var(\\'GCP_CREDENTIALS\\') }}\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\nexport DBT_PROFILES_DBT=path/to/profiles.yml\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\nOnce DIRs are set,:\\ndbt debug config-dir\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\nThen create a trigger.py as such:\\nimport os\\nimport requests\\nclass MageTrigger:\\nOPTIONS = {\\n\"<pipeline_name>\": {\\n\"trigger_id\": 10,\\n\"key\": \"f3a1a4228fc64cfd85295b668c93f3b2\"\\n}\\n}\\n@staticmethod\\ndef trigger_pipeline(pipeline_name, variables=None):\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\"trigger_id\"]\\nkey = MageTrigger.OPTIONS[pipeline_name][\"key\"]\\nendpoint = f\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\"\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\npayload = {}\\nif variables is not None:\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\nreturn response\\nMageTrigger.trigger_pipeline(\"<pipeline_name>\")\\nFinally, after the mage server is up an running, simply this command:\\npython trigger.py from mage directory in terminal.\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\nYou can use this configuration in your DBT model:\\n{\\n\"field\": \"<field name>\",\\n\"data_type\": \"<timestamp | date | datetime | int64>\",\\n\"granularity\": \"<hour | day | month | year>\"\\n# Only required if data_type is \"int64\"\\n\"range\": {\\n\"start\": <int>,\\n\"end\": <int>,\\n\"interval\": <int>\\n}\\n}\\nand for clustering\\n{{\\nconfig(\\nmaterialized = \"table\",\\ncluster_by = \"order_id\",\\n)\\n}}\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs',\n",
       "   'section': 'Triggers in Mage via CLI',\n",
       "   'question': 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?'},\n",
       "  {'text': 'Docker Commands\\n# Create a Docker Image from a base image\\nDocker run -it ubuntu bash\\n#List docker images\\nDocker images list\\n#List  Running containers\\nDocker ps -a\\n#List with full container ids\\nDocker ps -a --no-trunc\\n#Add onto existing image to create new image\\nDocker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\\n# Create a Docker Image with an entrypoint from a base image\\nDocker run -it --entry_point=bash python:3.11\\n#Attach to a stopped container\\nDocker start -ai <Container_Name>\\n#Attach to a running container\\ndocker exec -it <Container_ID> bash\\n#copying from host to container\\nDocker cp <SRC_PATH/file> <containerid>:<dest_path>\\n#copying from container to host\\nDocker cp <containerid>:<Srct_path> <Dest Path on host/file>\\n#Create an image from a docker file\\nDocker build -t <Image_Name> <Location of Dockerfile>\\n#DockerFile Options and best practices\\nhttps://devopscube.com/build-docker-image/\\n#Docker delete all images forcefully\\ndocker rmi -f $(docker images -aq)\\n#Docker delete all containers forcefully\\ndocker rm -f $(docker ps -qa)\\n#docker compose creation\\nhttps://www.composerize.com/\\nGCP Commands\\n1.     Create SSH Keys\\n2.     Added to the Settings of Compute Engine VM Instance\\n3.     SSH-ed into the VM Instance with a config similar to following\\nHost my-website.com\\nHostName my-website.com\\nUser my-user\\nIdentityFile ~/.ssh/id_rsa\\n4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\\n5.     Install Docker after\\na.     Sudo apt-get update\\nb.     Sudo apt-get docker\\n6.     To run Docker without SUDO permissions\\na.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\\n7.     Google cloud remote copy\\na.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\\nInstall GCP Cloud SDK on Docker Machine\\nhttps://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\\nsudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\\nAnaconda Commands\\n#Activate environment\\nConda Activate <environment_name>\\n#DeActivate environment\\nConda DeActivate <environment_name>\\n#Start iterm without conda environment\\nconda config --set auto_activate_base false\\n# Using Conda forge as default (Community driven packaging recipes and solutions)\\nhttps://conda-forge.org/docs/user/introduction.html\\nconda --version\\nconda update conda\\nconda config --add channels conda-forge\\nconda config --set channel_priority strict\\n#Using Libmamba as Solver\\nconda install pgcli  --solver=libmamba\\nLinux/MAC Commands\\nStarting and Stopping Services on Linux\\n  \\tsudo systemctl start postgresql\\n  \\tsudo systemctl stop postgresql\\nStarting and Stopping Services on MAC\\n      launchctl start postgresql\\n      launchctl stop postgresql\\nIdentifying processes listening to a Port across MAC/Linux\\nsudo lsof -i -P -n | grep LISTEN\\n$ sudo netstat -tulpn | grep LISTEN\\n$ sudo ss -tulpn | grep LISTEN\\n$ sudo lsof -i:22 ## see a specific port such as 22 ##\\n$ sudo nmap -sTU -O IP-address-Here\\nInstalling a package on Debian\\nsudo apt install <packagename>\\nListing all package on Debian\\nDpkg -l | grep <packagename>\\nUnInstalling a package on Debian\\nSudo apt remove <packagename>\\nSudo apt autoclean  && sudo apt autoremove\\nList all Processes on Debian/Ubuntu\\nPs -aux\\napt-get update && apt-get install procps\\napt-get install iproute2 for ss -tulpn\\n#Postgres Install\\nsudo sh -c \\'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list\\'\\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\\nsudo apt-get update\\nsudo apt-get -y install postgresql\\n#Changing Postgresql port to 5432\\n- sudo service postgresql stop - sed -e \\'s/^port.*/port = 5432/\\' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\\n- sudo chown postgres postgresql.conf\\n- sudo mv postgresql.conf /etc/postgresql/10/main\\n- sudo systemctl restart postgresql',\n",
       "   'section': 'Triggers in Mage via CLI',\n",
       "   'question': 'Basic Commands'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28fae0ad-3716-4501-9dae-3384484ce4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69644afb-5011-4503-b125-387298ba2279",
   "metadata": {},
   "source": [
    "Note that you need to have the requests library:\n",
    "- pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca1256de-38f3-4bab-b259-ccde84222934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDont forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a8062-4d43-465b-bfc0-f062a612fb74",
   "metadata": {},
   "source": [
    "## Q2. Indexing the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f11da4-f6b5-4227-b683-891d4178a0a5",
   "metadata": {},
   "source": [
    "Index the data in the same way as was shown in the course videos. Make the course field a keyword and the rest should be text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d9887-1ad4-45b8-8653-43910ecaed62",
   "metadata": {},
   "source": [
    "Which function do you use for adding your data to elastic?\n",
    "\n",
    "- insert\n",
    "- index\n",
    "- put\n",
    "- add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4df4e06f-38a1-4c21-890c-aeb9d8d02884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd148d0e-e70b-49c0-9495-ab848c65d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ac66db4-5c54-4e45-977a-9f06a3f5a726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-question'})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-question\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "835fb4f5-dbad-4d4f-acb8-320158baace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 948/948 [00:19<00:00, 48.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c62897-5d8e-4a23-9930-fe708caf74f9",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "- index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315dcf67-b9f2-4963-bb92-e26b6cd1601f",
   "metadata": {},
   "source": [
    "## Q3. Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b37ed3-e81c-4978-907a-867cbf4afca8",
   "metadata": {},
   "source": [
    "Now let's search in our index.\n",
    "\n",
    "We will execute a query \"How do I execute a command in a running docker container?\".\n",
    "\n",
    "Use only question and text fields and give question a boost of 4, and use \"type\": \"best_fields\".\n",
    "\n",
    "What's the score for the top ranking result?\n",
    "\n",
    "- 94.05\n",
    "- 84.05\n",
    "- 74.05\n",
    "- 64.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15067574-32fb-4d2b-b9a5-4b641608d55b",
   "metadata": {},
   "source": [
    "#### Look at the _score field.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "476cd510-a545-4199-8bad-386ca589b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I execute a command in a running docker container?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce6bed-e0d5-4a57-b4dc-c526c3419692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 4.0}\n",
    "\n",
    "    results = index.search(\n",
    "    query=query,\n",
    "    filter_dict={'course':'data-engineering-zoomcamp'}, # type of filtering\n",
    "    boost_dict=boost,\n",
    "    num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03550524-bd56-4362-8e6c-a20daead505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    boost = {'question': 4.0}\n",
    "\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"question^4.0\", \"text\"],  #  boost   question\n",
    "                            \"type\": \"best_fields\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"filter\": [\n",
    "                    {\n",
    "                        \"term\": {\n",
    "                            \"course\": \"data-engineering-zoomcamp\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_score'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67ef6750-f36e-40ba-abdc-7850400a8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_docs = elastic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "595cc040-8f3a-477e-9d88-9429db83dac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75.54128, 43.922554, 38.684105, 38.33403, 35.94081]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30c17f-19e9-482c-806c-757732aa1bdb",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "- 75.54128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af0694-5fac-4e82-87fd-d1118e5323c8",
   "metadata": {},
   "source": [
    "## Q4. Filtering\n",
    "Now let's only limit the questions to machine-learning-zoomcamp.\n",
    "\n",
    "Return 3 results. What's the 3rd question returned by the search engine?\n",
    "\n",
    "- How do I debug a docker container?\n",
    "- How do I copy files from a different folder into docker containers working directory?\n",
    "- How do Lambda container images work?\n",
    "- How can I annotate a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e88ddea5-02f4-40fe-ad32-aef873a07a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    boost = {'question': 4.0}\n",
    "\n",
    "    search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"question^4.0\", \"text\"],  #  boost   question\n",
    "                            \"type\": \"best_fields\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"filter\": [\n",
    "                    {\n",
    "                        \"term\": {\n",
    "                            \"course\": \"machine-learning-zoomcamp\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "\n",
    "    return response\n",
    "    \n",
    "    # result_docs = []\n",
    "    \n",
    "    # for hit in response['hits']['hits']:\n",
    "    #     result_docs.append(hit['_score'])\n",
    "    \n",
    "    # return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "420586ac-4959-4e42-a5d0-f4d8bd5f35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "respone = elastic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44756ddf-8bd6-43cf-909f-f63a5c41827b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'course-question',\n",
       " '_id': 'wPF8aZAB4spfKTEBWOWq',\n",
       " '_score': 49.938507,\n",
       " '_source': {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan',\n",
       "  'section': '5. Deploying Machine Learning Models',\n",
       "  'question': 'How do I copy files from a different folder into docker containers working directory?',\n",
       "  'course': 'machine-learning-zoomcamp'}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respone['hits']['hits'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f622ec0-20f0-4794-95c6-de2698a122d6",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "- How do I copy files from a different folder into docker containers working directory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66feda2f-5d81-4cef-aa0e-abeb7083e002",
   "metadata": {},
   "source": [
    "## Q5. Building a prompt\n",
    "Now we're ready to build a prompt to send to an LLM.\n",
    "\n",
    "Take the records returned from Elasticsearch in Q4 and use this template to build the context. Separate context entries by two linebreaks (\\n\\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61af8c-d577-484d-9734-5e028106fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef984b05-1c87-42d8-be0a-92f03381fffd",
   "metadata": {},
   "source": [
    "Now use the context you just created along with the \"How do I execute a command in a running docker container?\" question to construct a prompt using the template below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f056af0e-f8bf-4fc8-9293-0e71c67b7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c32ca-4e61-478f-8b6d-7de8edd54272",
   "metadata": {},
   "source": [
    "### What's the length of the resulting prompt? (use the len function)\n",
    "\n",
    "- 962\n",
    "- 1462\n",
    "- 1962\n",
    "- 2462"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343ae59-369d-478e-9b1f-0d0ce420618a",
   "metadata": {},
   "source": [
    "## Q6. Tokens\n",
    "When we use the OpenAI Platform, we're charged by the number of tokens we send in our prompt and receive in the response.\n",
    "\n",
    "The OpenAI python package uses tiktoken for tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e152c-7980-4cac-be83-222a9b913df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb252ba3-84cc-400e-9f9a-6c5d15955407",
   "metadata": {},
   "source": [
    "Let's calculate the number of tokens in our query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78bf19-3071-4aaa-8947-6d3eff0f4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156909ce-3bb6-4371-a2fa-9a6542737c4e",
   "metadata": {},
   "source": [
    "Use the encode function. How many tokens does our prompt have?\n",
    "\n",
    "- 122\n",
    "- 222\n",
    "- 322\n",
    "- 422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9055b14-864c-40d7-8c57-cc5c77173c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6c61c-f603-4efc-86f3-dfb70e401124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b60249-7388-4103-af79-74835482da30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
